
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  奇异值分解 SVD - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div>
	<article class="hentry" role="article">
	<header>
			  	<h1 class="entry-title">奇异值分解 SVD</h1>
				<p class="meta"><time datetime="2017-08-27T02:12:53+08:00" pubdate data-updated="true">2017/8/27</time></p>
			 </header>
		  	<div class="entry-content">
			  	<p>SVD，英文全称是Singular Values Decomposition，关于奇异值的解释，参考一下百度百科里的介绍，设\(A\)是一个m*n的矩阵，那么\(AA^T\)的q个特征值得平方根叫做\(A\)的特征值。在PCA的介绍中，我们提到了特征值分解的方式来实现提取特征，特征值分解要求必须是方阵。而奇异值分解的目的也是提取特征值，但是针对的是任意矩阵。SVD除此之外还可以用来做图片压缩，在推荐系统中的应用也是名声大噪。</p>

<h2 id="toc_0">前置知识</h2>

<p>在学习SVD之前需要一些线性代数的知识，下面做一些简单介绍。</p>

<h4 id="toc_1">正交矩阵</h4>

<p>在线形代数里，若矩阵\(A\)满足\(AA^T=I\)或\(A^TA=I\)，其中\(E\)为单位矩阵，那么可以说\(n\)阶实矩阵为正交矩阵。<br/>
\[<br/>
\begin{align*}<br/>
(AA^T)_{ij} = \sum_{k=0}^N a_{ik} a_{kj} = I_{ij}<br/>
\end{align*}<br/>
\]</p>

<p>由单位矩阵的性质可知主对角线上的元素不为0，其他元素都为0，可知\(A\)的各行两两相交，且\(||A^T||=1\)，同理\(A\)的列向量两两正交且\(||A||=1\)。</p>

<p>一个正交矩阵对应的变换叫做正交变换，这个变换有一些特点。假设我们在空间里有一个点\(X=(2,4)^T\)，那么现在我们用一个正交矩阵\(A=\left [ \begin{array}{cc}\frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} \end{array} \right ]\)来对其做正交变换，所以得到的新的坐标点的位置为\(X^{&#39;}=A^TX=(3\sqrt{2},\sqrt{2})^T\)，我们将得到的新的坐标点在新的正交基构成的坐标系中表示出来，如下图：</p>

<div align="center">
    <img src="media/15037711738928/15273990942816.jpg" width="500px" />
</div>

<p>由上图可知，坐标点的\(X\)的位置其实并没有发生改变，只是在相对应不同的坐标系位置发生了改变，实际上也就是只是将坐标系旋转了。</p>

<p>正交矩阵其实是一种特殊的每一个元素都是实数的酉矩阵，一个\(n\times n\)的复数矩阵\(U\)，满足：<br/>
\[<br/>
U^{H}U = UU^{H} = I<br/>
\]<br/>
其中\(U^{H}\)是\(U\)的共轭转置矩阵，那么\(U\)则被称为酉矩阵。</p>

<h2 id="toc_2">SVD介绍</h2>

<p>设\(X\)是一个\(m*n\)的矩阵，则存在\(m\)阶正交矩阵\(U\)和\(n\)阶正交矩阵\(V\)，在复数领域就是酉矩阵，满足：<br/>
\[<br/>
X = U\Sigma V^T = U\left [ \begin{array}{cccc}\sigma_1&amp;&amp;&amp;\\&amp;\sigma_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\sigma_r\\&amp;&amp;&amp;0\\\end{array}\right ]V<br/>
\]</p>

<p>其中\(\sigma_1&gt;\sigma_2&gt;\sigma_r\)被称为奇异值，\(U\)和\(V\)的前r列向量被称为奇异向量。一般前10%甚至1%的奇异值，将能代表超过99%的信息量。关于奇异值和奇异向量的求解就是SVD分解。</p>

<h2 id="toc_3">SVD分解</h2>

<p>在PCA中操作往往是对数据集\(X\)的协方差矩阵\((X-\overline{X})(X-\overline{X})^T\)进行提取特征值，但是特征值提取只能是方阵，那么对于非方阵提取特征值便可以通过SVD的方式。奇异值分解是一种能分解任意矩阵的方法，对于\(m\times n\)的矩阵\(X\)有：<br/>
\[<br/>
X = U\Sigma V^T<br/>
\]</p>

<p>其中\(\Sigma\)是一个\(m\times n\)的矩阵，除主对角线的元素外都为0，主对角线的元素则是奇异值，按照从大到小排列。\(U\)是一个\(m\times m\)的方阵，称为\(X\)的左奇异向量，\(V\)是一个\(n \times n\)的方阵，称为\(X\)的右奇异向量，它们都是正交矩阵，满足\(UU^T=I,VV^T=I\)。</p>

<p>现在开始讨论\(U\)和\(V\)和\(\Sigma\)的求解：<br/>
\[<br/>
X = U\Sigma V^T \Rightarrow X^T = V\Sigma^TU^T \Rightarrow XX^T = U\Sigma V^TV\Sigma^TU^T=U(\Sigma\Sigma^T)U^T<br/>
\]</p>

<p>因为\((XX^T)^T = XX^T\)，所以\(XX^T\)是一个对称向量，由对称向量的性质，\(n\)阶对称向量，必有正交矩阵\(P\)，满足\(A = P\Lambda P^T\)，所以结合上式可以认为左奇异向量\(U\)是对称矩阵\(XX^T\)的特征向量。同理：<br/>
\[<br/>
X^TX = V\Sigma^TU^TU\Sigma V^T = V(\Sigma^T\Sigma) V^T<br/>
\]</p>

<p>可知右奇异向量\(V\)可看做对称矩阵\(X^TX\)的特征向量。同时我们也能将\(\Sigma^T\Sigma\)看做对称矩阵\(XX^T\)或\(X^TX\)的特征值组成的矩阵，也就是奇异值可以通过\(X^TX\)或\(XX^T\)特征值取平方根来求得。</p>

<p>这里说奇异值可以通过\(X^TX\)或\(XX^T\)特征值取平方根来求得，那就必然存在两个必然条件：<br/>
(1) \(X^TX\)或\(XX^T\)的特征值必须是非负数<br/>
(2) \(A^TA\)和\(AA^T\)的非零特征值必须相等（为了说明清楚，换个表示）</p>

<p>证明（1）：假设矩阵的\(X^TX\)的特征值\(\sigma\)对应的特征向量是\(v\)，有正交矩阵的性质可知\(v^Tv = 1\)：<br/>
\[<br/>
X^TX v = \sigma v \Rightarrow \sigma=v^TX^TX v = ||Xv||^2<br/>
\]</p>

<p>所以得证\(\sigma\ge 0\)，同理可知\(XX^T\)的特征向量必是非负数。</p>

<p>证明（2）：首先证明向量\(A^TA\)与\(AA^T\)有相等个非零特征值：<br/>
设矩阵\(A\)是\(m\times n\)的矩阵，由矩阵秩的性质可知\(r(A)=r(A^T)\)。对于任意的向量\(x\)，如果\(Ax = 0\)则 \(A^TAx = A^T0 = 0\)，即\(Ax=0\)的解空间是\(A^Ax=0\)的解空间。如果\(A^TAx = 0\)时，两边同左乘\(x^T\)得\(x^TA^TAx = (Ax)^TAx= 0 \)，也就是\(||Ax||=0\)，所以\(Ax=0\)，即\(A^TAx=0\)的解空间是\(Ax=0\)的解空间。结合前面知：\(Ax=0\)与\(A^TAx=0\)拥有相同的解空间，所以\(r(A)=r(A^TA)\)，同理可证：\(r(A^T)=r(AA^T)\)。</p>

<p>设\(r(A) = r\)，所以\(r=r(A)=r(A^TA)\)=\(r(A^T)=r(AA^T)\)，因为\(AA^T\)是实对称向量，并由秩的定义可知\(AA^T\)化成行列式后有\(r\)个非零行，即\(AA^T\sim\left[\begin{array}\\\sigma_1&amp;&amp;&amp;\\&amp;\ddots&amp;&amp;\\&amp;&amp;\sigma_r&amp;\\&amp;&amp;&amp;0\\\end{array}\right]\)，其中\(\sigma_n\)是\(AA^T\)的特征值，\(r=AA^T\)非零特征值得个数。同理\(r=A^TA\)非零特征值得个数。</p>

<p>再证明\(A^TA\)的非零特征值也是\(AA^T\)的特征值：<br/>
设\(\sigma\)是\(A^TA\)的特征值，所以有\(A^TAv=\sigma v\)，两边同时左乘\(A\)得：\(AA^T(Av) = \sigma Av\)，这可以看成\(\sigma\)是\(AA^T\)的特征值，其中对应的特征向量是\(Av\)，所以\(A^TA\)的特征值也是\(AA^T\)的特征值。反之也可证明\(AA^T\)的特征值也是\(A^TA\)的特征值。</p>

<p>综上（1）（2）可知\(A^TA\)与\(AA^A\)特征值的区别就在零特征值的数量。</p>

<p>那么我们在求出\(X^TX\)或\(XX^T\)任意一个特征值后，可以利用左右奇异向量的关系，在求出其中一个的前提下，求出另一个，而不用再通过特征值得方式计算，假设\(\sigma_i\)为\(X\)的奇异值：<br/>
\[<br/>
X = U\Sigma V^T \Rightarrow AV = U\Sigma \Rightarrow Xv_i = \sigma_iu_i\\\Rightarrow u_i = Xv_i/\sigma_i<br/>
\]</p>

<p>因为特征值与奇异值的关系，可知\(X^TX\)的特征值为\(\sigma_i^2\)，假设\(X^TX\)对应的特征向量为\(v_i\)，则：<br/>
\[<br/>
\begin{align*}<br/>
&amp;X^TX v_i = \sigma_i^2 v_i \\<br/>
&amp;\Rightarrow v_i^T X^TXv_i = \sigma_i^2 v_i^Tv_i \\<br/>
&amp;\Rightarrow \sigma_i^2 = ||Xv_i||^2 \\<br/>
\end{align*}<br/>
\]</p>

<p>注意到：<br/>
\[<br/>
XX^Tu_i = XX^T\frac{Xv_i}{\sigma_i} = X\frac{X^TXv_i}{\sigma_i}=X\frac{\sigma_i^2 v_i}{\sigma_i} = \sigma_i^2\frac{Xv_i}{\sigma_i} = \sigma_i^2p_i<br/>
\]</p>

<p>所以\(u_i\)是\(XX^T\)的特征向量，其实这个在上面已经知道了。</p>

<p>且有：<br/>
\[<br/>
u_i^Tu_j = (\frac{Xv_i}{\sigma_i})^T\frac{Xv_j}{\sigma_j} = \frac{(Xv_i)^T}{\sigma_i}\frac{Xv_j}{\sigma_j} = \frac{v_i^TX^TXv_j}{\sigma_i\sigma_j} = \frac{v_i^T\sigma_j^2v_j}{\sigma_i\sigma_j}=\frac{\sigma_j}{\sigma_i}v_i^Tv_j<br/>
\]</p>

<p>令\(\delta_{ij}=v_i^Tv_j\)，则：<br/>
\[<br/>
u_i^Tu_j=\frac{\sigma_j}{\sigma_i}\delta_{ij}<br/>
\]</p>

<p>当特征向量\(v_i\)是一组正交单位特征向量时：<br/>
当\(i \neq j\)时\(\delta_{ij}=v_iv_j=0\)，此时\(u_i^Tu_j=\frac{\sigma_j}{\sigma_i}\delta_{ij}=0=\delta_{ij}\)；<br/>
当\(i = j\)时\(\sigma_i=\sigma_j\)，此时\(u_i^Tu_j=\frac{\sigma_j}{\sigma_i}\delta_{ij}=\delta_{ij}\)，<br/>
所以\(u_i\)也是一组正交单位特征向量。</p>

<h4 id="toc_4">SVD分解实例</h4>

<p>求解\(A = \left ( \begin{array}{cc} 1 &amp; 0\\ 1&amp;1\\ 0&amp;0\\0&amp;1\\ \end{array} \right)\)<br/>
首先计算\(AA^T=\left ( \begin{array}{cc} 1 &amp; 0\\ 1&amp;1\\ 0&amp;0\\0&amp;1\\ \end{array} \right)\cdot\left ( \begin{array}{cc}  1&amp;1&amp;0&amp;0\\ 0&amp;1&amp;0&amp;1\\ \end{array} \right) = \left(\begin{array}{cc}1 &amp; 1&amp;0&amp;0\\1&amp;2&amp;0&amp;1\\0&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;1\\\end{array}\right)\) ，然后计算\(A^TA\)的特征向量就是左奇异矩阵\(U\)。<br/>
\[<br/>
|AA^T-\lambda I| = \left | \begin{array}{cccc}1-\lambda &amp;1&amp;0&amp;0 \\ 1&amp;2-\lambda &amp;0&amp;1 \\0&amp;0&amp; -\lambda&amp;0 \\0&amp;1&amp;0&amp;1-\lambda \\\end{array}\right | = -\lambda\left | \begin{array}{cc}1-\lambda&amp;1&amp;0\\1&amp;2-\lambda&amp;1\\0&amp;1&amp;1-\lambda\end{array}\right|=0\\<br/>
\Rightarrow -\lambda[(1-\lambda)^2(2-\lambda)-2(1-\lambda)] =0<br/>
\]</p>

<p>可以求出\(\lambda_1=3\,,\lambda_2=1\,,\lambda_3=\lambda_4=0\)。<br/>
当\(\lambda_1=3\)时：<br/>
\[<br/>
(AA^T-\lambda I)\overrightarrow{\nu} = \left(\begin{array}{cccc}-2&amp;1&amp;0&amp;0\\1&amp;-1&amp;0&amp;1\\0&amp;0&amp;-3&amp;0\\0&amp;1&amp;0&amp;-2\\\end{array}\right)\left(\begin{array}{c}x_1\\x_2\\x_3\\x_4\\\end{array}\right) \\<br/>
\Rightarrow \left \{ \begin{array}\\-2x_1+x_2=0\\x_1-x_2+x_4=0\\-3x_3=0\\x_2-2x_4=0\\\end{array}\right .<br/>
\]</p>

<p>可以求得其中一个解为\(x1=1\,,x2=2\,,x_3=0\,,x4=1\)，归一化后得：<br/>
\[<br/>
\overrightarrow{\nu} = \left ( \begin{array}{c}<br/>
\frac{1}{\sqrt{6}}\\\frac{2}{\sqrt{6}}\\0\\\frac{1}{\sqrt{6}}\\\end{array}\right)<br/>
\]</p>

<p>同样的方式，可以求出特征矩阵为：<br/>
\[<br/>
V = \left ( \begin{array}{c}<br/>
\frac{1}{\sqrt{6}}&amp;\frac{\sqrt{2}}{2}&amp;\frac{1}{2}&amp;-\frac{1}{2}\\<br/>
\frac{2}{\sqrt{6}}&amp;0&amp;-\frac{1}{2}&amp;\frac{1}{2}\\<br/>
0&amp;0&amp;-\frac{1}{2}&amp;\frac{1}{2}\\<br/>
\frac{1}{\sqrt{6}}&amp;-\frac{\sqrt{2}}{2}&amp;\frac{1}{2}&amp;-\frac{1}{2}\\<br/>
\end{array}\right)<br/>
\]</p>

<p>我们知道这就是左奇异矩阵\(U\)，同理再计算\(A^TA\)的特征值\(\lambda_1=3\,,\lambda_2=1\)，代入求得右奇异矩阵\(V\)：<br/>
\[<br/>
V=\left ( \begin{array}{cc}<br/>
\frac{\sqrt{2}}{2}&amp;\frac{\sqrt{2}}{2}\\\frac{\sqrt{2}}{2}&amp;-\frac{\sqrt{2}}{2}\\<br/>
\end{array} \right )<br/>
\]</p>

<p>前面我们说过\(\Sigma\)主对角线的元素，也就是奇异值，是特征值得平方根，所以可求得：<br/>
\[<br/>
\Sigma = \left ( \begin{array}{cc}<br/>
\sqrt{3}&amp;0\\0&amp;1\\0&amp;0\\0&amp;0\\<br/>
\end{array} \right )<br/>
\]</p>

<p>所以矩阵\(A\)被分解为了3个矩阵的乘积：<br/>
\[<br/>
A = \left ( \begin{array}{c}<br/>
\frac{1}{\sqrt{6}}&amp;\frac{\sqrt{2}}{2}&amp;\frac{1}{2}&amp;-\frac{1}{2}\\<br/>
\frac{2}{\sqrt{6}}&amp;0&amp;-\frac{1}{2}&amp;\frac{1}{2}\\<br/>
0&amp;0&amp;-\frac{1}{2}&amp;\frac{1}{2}\\<br/>
\frac{1}{\sqrt{6}}&amp;-\frac{\sqrt{2}}{2}&amp;\frac{1}{2}&amp;-\frac{1}{2}\\<br/>
\end{array}\right) \cdot \left ( \begin{array}{cc}<br/>
\sqrt{3}&amp;0\\0&amp;1\\0&amp;0\\0&amp;0\\<br/>
\end{array} \right ) \cdot \left( \begin{array}{cc}<br/>
\frac{\sqrt{2}}{2}&amp;\frac{\sqrt{2}}{2}\\\frac{\sqrt{2}}{2}&amp;-\frac{\sqrt{2}}{2}\\<br/>
\end{array} \right )<br/>
\]</p>

			</div>

		
	  
		<footer>
		 <p class="meta">

			<strong>Categories:</strong>&nbsp; 
			<span class="categories">
			
			    <a class='category' href='%E9%99%8D%E7%BB%B4.html'>降维</a>&nbsp;
			 
			</span>
		    </p>
		    <p class="meta">
		      
		 </p>
	    
		<div class="sharing">
		  
          

          

		</div>

	    <p class="meta">
	    
	        <a class="basic-alignment left" href="15041093080052.html" 
	        title="Previous Post: 线性判别分析 LDA">&laquo; 线性判别分析 LDA</a>
	    
	    
	        <a class="basic-alignment right" href="15034227314732.html" 
	        title="Next Post: 主成分分析 PCA">主成分分析 PCA &raquo;</a>
	    
	    </p>
	  </footer>
	</article>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>