
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  马尔科夫链蒙特卡罗方法 Markov Chain Monte Carlo，MCMC - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div>
	<article class="hentry" role="article">
	<header>
			  	<h1 class="entry-title">马尔科夫链蒙特卡罗方法 Markov Chain Monte Carlo，MCMC</h1>
				<p class="meta"><time datetime="2018-02-25T06:19:09+08:00" pubdate data-updated="true">2018/2/25</time></p>
			 </header>
		  	<div class="entry-content">
			  	<p>对于一般的分布的采样，之前已经有一些方法可以解决，但是对于一些复杂分布的采样，却没有很好的函数实现，本文将介绍 MCMC 方法提供解决方法。</p>

<h3 id="toc_0">马尔科夫链 Markov Chain</h3>

<p>设 \(X_t\) 表示随机变量 \(X\) 在时刻 \(t\) 的取值，若该变量随时间变化的转移概率仅仅依赖于它的当前取值，与过去状态无关，所谓的“遗忘性”，即：<br/>
\[<br/>
P(X_{t+1}=s_{t+1}|X_0=s_0,X_1=s_1,...,X_t=s_t) = P(X_{t+1}=s_{t+1}|X_t=s_t)<br/>
\]</p>

<p>也就是说状态转移仅仅依赖于前一个状态，其中 \(s_0,s_1,...,s_i,...\) 表示随机变量 \(X\) 的可能状态，这个性质称为马尔科夫性质。具有马尔科夫性质的随机过程叫马尔科夫过程，其中变量称为马尔科夫变量。</p>

<p>马尔可夫链指的是在一段时间内随机变量X的取值序列 \((X_0,X_1,...,X_m)\) ，它们满足如上的马尔可夫性质。</p>

<ol>
<li>时间、状态都离散的叫做马尔可夫链。</li>
<li>时间连续、状态离散的叫做时间连续马尔可夫链。</li>
<li>时间、状态都连续的叫做马尔可夫过程。</li>
</ol>

<h4 id="toc_1">转换概率</h4>

<p>马尔可夫链是通过对应的转移概率定义的，转移概率指的是随机变量从一个时刻到下一个时刻，从状态 \(s_i\) 转移到另一个状态 \(s_j\) 的概率，即：<br/>
\[<br/>
P(i\rightarrow j):=P_{i,j} = P(X_{t+1}=s_j|X_t = s_i)<br/>
\]</p>

<p>记 \(\pi_k^{(t)}\) 表示随机变量 \(X\) 在时刻 \(t\) 的取值为 \(s_k\) 的概率，则随机变量 \(X\) 在时刻 \(t+1\) 的取值为 \(s_i\) 的概率为：<br/>
\[<br/>
\begin{align*}<br/>
\pi_i^{(t+1)} &amp;= P(X_{t+1} = s_i)\\<br/>
&amp;= \sum_{k} P(X_{t+1}=s_i|X_t = s_k)\cdot P(X_t=s_k)\\<br/>
&amp;= \sum_k P_{k,i}\cdot \pi_{k}^{(t)}<br/>
\end{align*}<br/>
\]</p>

<p>假设状态的数目为 \(n\)，则有：<br/>
\[<br/>
(\pi_1^{(t+1)},\cdots,\pi_j^{(t+1)},\cdots,\pi_n^{(t+1)}) = (\pi_1^{(t)},\cdots,\pi_i^{(t)},\cdots,\pi_1^{(t)})\left [ \begin{array}{cccccc}<br/>
P_{1,1}&amp;P_{1,2}&amp;\cdots&amp;P_{1,j}&amp;\cdots&amp;P_{1,n}\\<br/>
P_{2,1}&amp;P_{2,2}&amp;\cdots&amp;P_{2,j}&amp;\cdots&amp;P_{2,n}\\<br/>
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\cdots&amp;\vdots\\<br/>
P_{i,1}&amp;P_{i,2}&amp;\cdots&amp;P_{i,j}&amp;\cdots&amp;P_{i,n}\\<br/>
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\cdots&amp;\vdots\\<br/>
P_{n,1}&amp;P_{n,2}&amp;\cdots&amp;P_{n,j}&amp;\cdots&amp;P_{n,n}\\<br/>
\end{array} \right]<br/>
\]</p>

<p>对于任意的 \(i\) 都有 \(\sum_{j=1}^n P_{i,j} = 1\)。若设 \(\Pi=[\pi_1^{(0)},\pi_2^{(0)},\cdots,\pi_n^{(0)}]\) 是系统的初始概率分布，\(\pi_i^{(0)}\) 是系统在初始时刻处于状态 \(i\) 的概率，满足 \(\sum_{i=1}^n \pi_i^{(0)}=1\) 。</p>

<h4 id="toc_2">平稳分布</h4>

<p>对于马尔可夫链，需要注意以下的两点：</p>

<ol>
<li>周期性：即经过有限次的状态转移，又回到了自身；</li>
<li>不可约：即两个状态之间相互转移；</li>
</ol>

<p><strong>马氏定理：</strong>如果一个非周期不可约马尔可夫链的转移矩阵为 \(P\)，无论初始值 \(\pi^{(0)}\) 的取值，都有 \(\lim_{t\rightarrow \infty} \pi^{(0)} P^t\) 存在且初始值无关，记 \(\lim_{t\rightarrow \infty} \pi^{(0)} P^t = \pi^*\)，我们有：<br/>
\[<br/>
\pi^* P = \pi^*<br/>
\]</p>

<p>\(pi\) 是方程唯一非负解。其中：<br/>
\[<br/>
\pi^* =[\pi(1),\pi(2),\cdots,\pi(j),\cdots],\quad \sum_{i=0}^n \pi(i)=1<br/>
\]</p>

<p>则称 \(\pi^*\) 为马尔可夫链的平稳分布。存在稳态分布要求马尔可夫链是连通的（没有孤立点），同时不存在一个连通子图是没有对外的出边的。</p>

<p><strong>细致平稳条件：</strong>如果非周期马尔可夫链的转移矩阵 \(P\) 和分布 \(\pi\) 满足 <br/>
\[<br/>
\pi(i) P_{i,j} = \pi(j) P_{j,i},\quad \forall i,j<br/>
\]</p>

<p>则 \(\pi^*=(\pi(1),\pi(2),\cdots,\pi(j),\cdots)\) 是马尔可夫链的平稳分布。</p>

<p>这个定理是显而易见的，因为细致平稳条件的物理含义就是对于任何两个状态 \(i,j\) ，从 \(i\) 转移出去到 \(j\) 而丢失的概率质量，恰好会被从 \(j\) 转移回来到 \(i\) 的概率质量补充，所以状态 \(i\) 上的概率质量 \(\pi(i)\) 是稳定的，从而 \(\pi^*\) 是马尔可夫链的平稳分布。数学上的证明也很简单，由细致平稳条件可得：<br/>
\[<br/>
\sum_{i=1}^n \pi(i)P_{i,j}=\sum_{i=1}^n \pi(j)P_{j,i}= \pi(j)\sum_{i=1}^n P_{j,i} =\pi(j)\\<br/>
\Rightarrow \pi P=\pi<br/>
\]</p>

<p>由于 \(\pi^*\) 是方程 \(\pi P=\pi\) 的解，所以 \(\pi^*\) 是平稳分布。</p>

<p>马氏定理和细致平稳条件非常重要，是MCMC(Markov Chain Monte Carlo) 方法的基础。</p>

<h3 id="toc_3">Metropolis 采样</h3>

<p>对于给定的概率分布 \(p(x)\)，我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布，于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为 \(P\) 的马氏链，使得该马氏链的平稳分布恰好是 \(p(x)\)，那么我们从任何一个初始状态 \(x_0\) 出发沿着马氏链转移, 得到一个转移序列 \(x_0,x_1,x_2,\cdots,x_n,x_{n+1},\cdots\)， 如果马氏链在第 \(n\) 步已经收敛了，于是我们就得到了 \(\pi(x)\) 的样本 \(x_n,x_{n+1},\cdots\)。</p>

<p>这个绝妙的想法在 1953 年被 Metropolis 想到了，为了研究粒子系统的平稳性质， Metropolis 考虑了物理学中常见的波尔兹曼分布的采样问题，首次提出了基于马氏链的蒙特卡罗方法，即 Metropolis 算法，并在最早的计算机上编程实现。Metropolis 算法是首个普适的采样方法，并启发了一系列 MCMC 方法，所以人们把它视为随机模拟技术腾飞的起点。 Metropolis 的这篇论文被收录在《统计学中的重大突破》中， Metropolis 算法也被遴选为二十世纪的十个最重要的算法之一。</p>

<p>假设现在有个转移矩阵为 \(Q\) 的马氏链，其中 \(q(j|i)\)，表示马尔可夫链在第 \(t\) 代时的状态为 \(i\) ，下一时刻转移到状态为 \(j\) 的概率。此时不满足细致平稳性：<br/>
\[<br/>
\pi(i) q(j|i) \neq \pi(j) q(i|j),\quad \forall i,j<br/>
\]</p>

<p>为了满足细致平稳性，在左边乘上右边，在右边乘上左边，也就是：<br/>
\[<br/>
\pi(i) q(j|i)\pi(j) q(i|j) = \pi(j) q(i|j)\pi(i) q(j|i),\quad \forall i,j<br/>
\]</p>

<p>令 \(\alpha(j|i) = \pi(j) q(i|j)\)，则上式可以写成：<br/>
\[<br/>
\pi(i) q(j|i) \alpha(j|i) = \pi(j) q(i|j) \alpha(i|j),\quad \forall i,j<br/>
\]</p>

<p>假设 \(Q&#39;(j|i) = q_(j|i) \alpha(j|i)\) ，所以有：<br/>
\[<br/>
\begin{align}<br/>
\pi(i)\underbrace{q(j|i) \alpha(j|i)}_{Q&#39;(j|i)} = \pi(j) \underbrace{ q(i|j) \alpha(i|j)}_{Q&#39;(i|j)}\label{puaq}\\<br/>
\end{align}<br/>
\]</p>

<p>将 \(Q&#39;(j|i)\) 视为新的转移概率，这样便把原先具有转移矩阵 \(Q\) 的普通马氏链，改造为具有转移矩阵 \(Q&#39;\) 的马氏链，而 \(Q&#39;\) 是满足细致平稳条件的，平稳分布是 \(\pi^*\)，在改造过程中引入了接受概率 \(\alpha(i,j)\)，物理意义可以理解为在原来的马氏链上，从状态 \(i\) 以 \(q(j|i)\) 跳转到状态 \(j\) 的时候，我们以 \(\alpha(i|j)\) 的概率接受这个转移，于是得到新的马氏链 \(Q&#39;\) 的转移概率为 \(q(j|i) \alpha(j|i)\) 。</p>

<p>得到接受概率 \(\alpha\) 之后，就可以随机从均匀分布中得到 \(u\) 值，如果 \(u \le \alpha(j|i) = \pi(j) q(i|j)\) 则接受状态 \(i\) 转移 到 \(j\)，否则不接受转移。</p>

<p>具体的算法流程如下：</p>

<ol>
<li>初始化马氏链初始状态 \(X_0 = x_0\)</li>
<li><p>对 \(t=0,1,2,...\)，循环以下过程进行采样</p>

<ul>
<li>第 \(t\) 个时刻马氏链的状态 \(X_t = x_t\)，采样 \(y\sim q(x|x_t)\)</li>
<li>从均匀分布采样 \(u\sim U(0,1)\)</li>
<li>如果 \(u\le \alpha(y|x_t) =  \pi(y) q(x_t|y)\)，则接受转移 \(x_t\rightarrow y\)，即\(X_{t+1} = y\)</li>
<li>否则不接收转移，即 \(X_{t+1} = x_t\)</li>
</ul></li>
</ol>

<p>上面过程中 \(p(x)\)，\(q(x|y)\) 说的是离散的情形，事实上即便这两个分布是连续的，以上算法仍然是有效的，于是就得到更一般的连续概率分布 \(p(x)\) 的采样算法，而 \(q(x|y)\) 就是一个任意一个连续二元概率分布对应的条件分布。</p>

<h5 id="toc_4">Metropolis-Hastings 算法</h5>

<p>但是Metropolis算法构造出的接受概率可能会很小，这样造成算法要经过很多的迭代才能到达平稳分布。为了加快收敛效果，我们需要在 \ref{puaq} 两边同比例增加 \(\alpha(i|j)\) 和 \(\alpha(j|i)\) ,假设要将左边的 \(\alpha(i|j)\) 增加到 1 ，需要两边同乘上 \(\frac{1}{\alpha(i|j)}\) 倍，则右边接受概率变为 \(\frac{\alpha(j|i)}{\alpha(i|j)}\)：<br/>
\[<br/>
\pi(i) q(j|i) = \pi(j) q(i|j) \frac{\alpha(i|j)}{\alpha(j|i)} = \frac{\pi(i) q(j|i)}{\pi(j) q(i|j)},\quad \forall i,j<br/>
\]</p>

<p>这样接受概率就可以写成：<br/>
\[<br/>
\begin{align*}<br/>
\alpha(j|i) &amp;= 1\\<br/>
\alpha(i|j) &amp;= \frac{\pi(i) q(j|i)}{\pi(j) q(i|j)}\\<br/>
\end{align*}<br/>
\]</p>

<p>因为在 Metropolis 采样算法过程中，生成 \([0,1]\) 均匀分布随机值 \(u\) 必定不大于1，可以将 \(\alpha(i|j)\) 缩小范围：<br/>
\[<br/>
\alpha(i|j) = \frac{\pi(i) q(j|i)}{\pi(j) q(i|j)}\quad\Leftrightarrow\quad \alpha(i|j) = \min\bigg({1,\frac{\pi(i) q(j|i)}{\pi(j) q(i|j)}}\bigg)<br/>
\]</p>

<p>这样改造后接受概率对后续过程并没有影响。</p>

<p>具体的算法流程如下：</p>

<ol>
<li>初始化马氏链初始状态 \(X_0 = x_0\)</li>
<li><p>对 \(t=0,1,2,...\)，循环以下过程进行采样</p>

<ul>
<li>第 \(t\) 个时刻马氏链的状态 \(X_t = x_t\)，采样 \(y\sim q(x|x_t)\)</li>
<li>从均匀分布采样 \(u\sim U(0,1)\)</li>
<li>如果 \(u\le \alpha(y|x_t) =  \min\bigg({1,\frac{\pi(y) q(x_t|y)}{\pi(x_t) q(y|x_t)}}\bigg)\)，则接受转移 \(x_t\rightarrow y\)，即\(X_{t+1} = y\)</li>
<li>否则不接收转移，即 \(X_{t+1} = x_t\)</li>
</ul></li>
</ol>

<p>对于分布 \(\pi(x)\)，我们构造转移矩阵 \(Q&#39;\) 使其满足细致平稳条件<br/>
\[<br/>
p(x)Q&#39;(x\rightarrow y) = p(y)Q&#39;(y\rightarrow x)<br/>
\]</p>

<p>此处 \(x\) 并不要求是一维的，对于高维空间的 \(p(\mathbf x)\)，如果满足细致平稳条件：<br/>
\[<br/>
p(\mathbf x)Q&#39;(\mathbf x\rightarrow \mathbf y) = p(\mathbf y)Q&#39;(\mathbf y\rightarrow \mathbf x)<br/>
\]</p>

<p>那么以上的 Metropolis-Hastings 算法一样有效。</p>

<h3 id="toc_5">MCMC-Gibbs Sampling算法</h3>

<p>设想 \(p(x,y)\) 是 p.d.f. 或 p.m.f. ，如果直接对他们进行采样会比较困难。不过，我们假设我们可以很容易地从条件分布 \(p(x|y)\) 和 \(p(y|x)\) 中进行采样。假设有两个 \(x\) 轴坐标相同的点 \(A(x_1,y_1)\) 和 \(B(x_1,y_2)\)。<br/>
易得：<br/>
\[<br/>
p(x_1,y_1) p(y_2|x_1) = p(x_1)p(y_1|x_1)p(y_2|x_1)\\<br/>
p(x_1,y_2) p(y_1|x_1) = p(x_1)p(y_2|x_1)p(y_1|x_1)\\<br/>
\]</p>

<p>比较上述两个等式右边得：<br/>
\[<br/>
\begin{align}<br/>
p(x_1,y_1) p(y_2|x_1) = p(x_1,y_2) p(y_1|x_1)\label{pxyp}\\<br/>
\end{align}<br/>
\]</p>

<p>即<br/>
\[<br/>
\begin{align}<br/>
p(A) p(y_2|x_1) = p(B) p(y_1|x_1)\label{papy}\\<br/>
\end{align}<br/>
\]</p>

<p>同理，如果现在有个点 \(C(x_3,y_1)\)，和点 \(A\) 在同一个 \(y\) 轴上，有如下等式：<br/>
\[<br/>
\begin{align}<br/>
p(A) p(x_3,y_1) = p(C) p(x_1|y_1)\label{papx}\\<br/>
\end{align}<br/>
\]</p>

<p>令<br/>
\[<br/>
\begin{align*}<br/>
&amp;Q(A\rightarrow B) = p(y_B|x_1) &amp;\quad \text{if }x_A = x_B = x_1\\<br/>
&amp;Q(A\rightarrow C) = p(x_C|y_1) &amp;\quad \text{if }y_A = y_C = y_1\\<br/>
&amp;Q(A\rightarrow D) = 0 &amp;\quad \text{otherwise} \\<br/>
\end{align*}<br/>
\]</p>

<p>代入(\ref{papy})和(\ref{papx})得：<br/>
\[<br/>
p(A) Q(A\rightarrow B) = p(B) Q(B\rightarrow A)\\<br/>
p(A) Q(A\rightarrow C) = p(C) Q(C\rightarrow A)\\<br/>
\]</p>

<p>有了如上的转移矩阵 Q，我们很容易知道对平面上任意两点 \(X\)、\(Y\)，满足细致平稳条件<br/>
\[<br/>
\begin{align}<br/>
p(X)Q(X\rightarrow Y)=p(Y)Q(Y\rightarrow X)\label{pxqx}\\<br/>
\end{align}<br/>
\]</p>

<p>于是这个二维空间上的马氏链将收敛到平稳分布 \(p(x,y)p(x,y)\)。而这个算法就称为 Gibbs Sampling 算法。</p>

<h5 id="toc_6">算法步骤</h5>

<ol>
<li>随机初始化 \(x_0,y_0\)</li>
<li><p>对于 \(t=0,1,2,\cdots\)，循环采样</p>

<ul>
<li>从条件分布 \(p(Y|X=x_t)\) 中采样 \(y_{t+1}\) 得到点 \((x_t,y_{t+1})\)。</li>
<li>从条件分布 \(p(X|Y=y_{t+1})\) 中采样 \(x_{t+1}\) 得到点 \((x_{t+1},y_{t+1})\)。</li>
</ul></li>
</ol>

<h5 id="toc_7">Gibbs sampling 中的马氏转移链</h5>

<p>以上采样过程中，如图所示，马氏链的转移只是轮换的沿着坐标轴 \(x\) 轴和 \(y\) 轴做转移，于是得到样本 \((x_0,y_0),(x_0,y_1),(x_1,y_1),(x_1,y_2),(x_2,y_2),\cdots\) 马氏链收敛后，最终得到的样本就是 \(p(x,y)p(x,y)\) 的样本，而收敛之前的阶段称为 burn-in period。</p>

<p>Gibbs Sampling 算法大都是坐标轴轮换采样的，但是这其实是不强制要求的。最一般的情形可以是，在 \(t\) 时刻，可以在 \(x\) 轴和 \(y\) 轴之间随机的选一个坐标轴，然后按条件概率做转移，马氏链也是一样收敛的。轮换两个坐标轴只是一种方便的形式。</p>

<p>以上的过程我们很容易推广到高维的情形，对于(\ref{papx})式，如果 \(x_1\) 变为多维情形 \(\mathbf x_1\)，可以看出推导过程不变，所以细致平稳条件同样是成立的<br/>
\[<br/>
p(\mathbf x_1,y_1)p(y_2|\mathbf x_1)=p(\mathbf x_1,y_2)p(y_1|\mathbf x_1)<br/>
\]</p>

<p>此时转移矩阵 \(Q\) 由条件分布 \(p(y|\mathbf x_1)\) 定义。上式只是说明了一根坐标轴的情形，和二维情形类似，很容易验证对所有坐标轴都有类似的结论。所以 \(n\) 维空间中对于概率分布 \(p(x_1,x_2,\cdots,x_n)\) 可以如下定义转移矩阵：马氏链转移的过程中，只能沿着坐标轴做转移，每次只转移一根坐标轴，沿着 \(x_i\) 这根坐标轴做转移的时候，转移概率由条件概率 \(p(x_i|x_1,x_2,\cdots,x_{i-1},x_{i+1},\cdots,x_n)\) 定义；其他的轴转移概率都设置为 0。</p>

<p>于是我们可以把 Gibbs Smapling 算法从采样二维的 \(p(x,y)\) 推广到采样 \(n\) 维的 \(p(x_1,x_2,...,x_n)\)。</p>

<h5 id="toc_8">n 维 Gibbs Sampling 算法步骤</h5>

<ol>
<li>随机初始化 \(\{x_i:i=1,2,\cdots,n \}\)</li>
<li>对 \(t=0,1,\cdots\) 循环采样

<ul>
<li>\(x_1^{(t+1)} = p(x_1|x_2^{(t)},x_3^{(t)},\cdots,x_n^{(t)})\)</li>
<li>\(x_2^{(t+1)} = p(x_2|x_1^{(t)},x_3^{(t)},\cdots,x_n^{(t)})\)</li>
<li>\(\cdots\)</li>
<li>\(x_j^{(t+1)} = p(x_1|x_2^{(t)},x_3^{(t)},\cdots,x_{j-1}^{(t)},x_{j+1}^{(t)},\cdots,x_n^{(t)})\)</li>
<li>\(\cdots\)</li>
<li>\(x_n^{(t+1)} = p(x_n|x_2^{(t)},x_3^{(t)},\cdots,x_{n-1}^{(t)})\)</li>
</ul></li>
</ol>

<p>以上算法收敛后，得到的就是概率分布 \(p(x_1,x_2,\cdots,x_n)\) 的样本，当然这些样本并不独立，但是我们此处要求的是采样得到的样本符合给定的概率分布，并不要求独立。同样的，在以上算法中，坐标轴轮换采样不是必须的，可以在坐标轴轮换中引入随机性，这时候转移矩阵 \(Q\) 中任何两个点的转移概率中就会包含坐标轴选择的概率，而在通常的 Gibbs Sampling 算法中，坐标轴轮换是一个确定性的过程，也就是在给定时刻 \(t\)，在一根固定的坐标轴上转移的概率是 1。</p>

<h3 id="toc_9">总结</h3>

<p>无论 metropolis-hasting 算法还是 gibbs 算法，都需要一个 burn in 过程，只有在达到平衡状态时候得到的样本才能是平衡状态时候的目标分布的样本，因此，在 burn in 过程中产生的样本都需要被舍弃。如何判断一个过程是否达到了平衡状态还没有一个成熟的方法来解决，目前常见的方法是看是否状态已经平稳。</p>

<p>关于链的收敛有这样一些检验方法</p>

<ol>
<li><strong>图形方法</strong>：这是简单直观的方法。我们可以利用这样一些图形：

<ul>
<li>迹图（trace plot）：将所产生的样本对迭代次数作图，生成马氏链的一条样本路径。如果当 \(t\) 足够大时，路径表现出稳定性没有明显的周期和趋势，就可以认为是收敛了。</li>
<li>自相关图（Autocorrelation plot）：如果产生的样本序列自相关程度很高，用迹图检验的效果会比较差。一般自相关随迭代步长的增加而减小，如果没有表现出这种现象，说明链的收敛性有问题。</li>
<li>遍历均值图（ergodic mean plot）：MCMC的理论基础是马尔科夫链的遍历定理。因此可以用累积均值对迭代步骤作图，观察遍历均值是否收敛。</li>
</ul></li>
<li><strong>蒙特卡洛误差</strong></li>
<li><strong>Gelman-Rubin方法</strong></li>
</ol>

<hr/>

<p><a href="https://www.cnblogs.com/xbinworld/p/4266146.html">随机采样方法整理与讲解</a><br/>
<a href="https://www.jianshu.com/p/1511c94b2ac3">LDA漫游系列(四)-Gibbs Sampling</a><br/>
<a href="https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling/">靳志辉 LDA-math-MCMC 和 Gibbs Sampling</a><br/>
<a href="https://www.zybuluo.com/evilking/note/753058">自相关</a></p>

			</div>

		
	  
		<footer>
		 <p class="meta">

			<strong>Categories:</strong>&nbsp; 
			<span class="categories">
			
			    <a class='category' href='HMM.html'>HMM</a>&nbsp;
			 
			</span>
		    </p>
		    <p class="meta">
		      
		 </p>
	    
		<div class="sharing">
		  
          

          

		</div>

	    <p class="meta">
	    
	        <a class="basic-alignment left" href="15198236385746.html" 
	        title="Previous Post: 极坐标计算二重积分">&laquo; 极坐标计算二重积分</a>
	    
	    
	        <a class="basic-alignment right" href="15189654050126.html" 
	        title="Next Post: 蒙特卡罗方法 Monte Carlo Simulation">蒙特卡罗方法 Monte Carlo Simulation &raquo;</a>
	    
	    </p>
	  </footer>
	</article>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>