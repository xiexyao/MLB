
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  隐马尔可夫模型 Hidden Markov Model - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div>
	<article class="hentry" role="article">
	<header>
			  	<h1 class="entry-title">隐马尔可夫模型 Hidden Markov Model</h1>
				<p class="meta"><time datetime="2018-03-09T19:49:28+08:00" pubdate data-updated="true">2018/3/9</time></p>
			 </header>
		  	<div class="entry-content">
			  	<p>关于隐马尔可夫模型有个很生动的例子，假设有一个隐士，他不能够直接获取到天气的观察情况，但是他有一些水藻。民间传说告诉我们水藻的状态与天气状态有一定的概率关系——天气和水藻的状态是紧密相关的。在这个例子中我们有两组状态，观察的状态（水藻的状态）和隐藏的状态（天气的状态）。我们希望为隐士设计一种算法，在不能够直接观察天气的情况下，通过水藻和马尔科夫假设来预测天气。</p>

<p>在这种情况下，观察到的状态序列与隐藏过程有一定的概率关系。我们使用隐马尔科夫模型对这样的过程建模，这个模型包含了一个底层隐藏的随时间改变的马尔科夫过程，以及一个与隐藏状态某种程度相关的可观察到的状态集合。</p>

<p>下图是隐藏状态（天气）与观察序列的示意图，假设隐藏状态由一个简单的一阶马尔可夫过程描述，那么他们之间都相互连接。</p>

<div align="center">
    <img src="media/15205961685497/15366882957510.jpg" width="500" />
</div>

<p>隐藏状态和观察状态之间的连接表示：在给定的马尔可夫过程中，一个隐藏状态生成特定的观察状态的概率。可以很清晰知道一个隐藏状态到所有观察状态的概率之和为1，即图中：<br/>
\[<br/>
\begin{align*}<br/>
P(\text{Dry,Dryish,Damp,Soggy|Sun}) &amp;= 1\\<br/>
P(\text{Dry,Dryish,Damp,Soggy|Cloud}) &amp;= 1\\<br/>
P(\text{Dry,Dryish,Damp,Soggy|Rain}) &amp;= 1\\<br/>
\end{align*}<br/>
\]</p>

<p>除了定义了马尔科夫过程的概率关系，我们还有另一个矩阵，定义为混淆矩阵（confusion matrix），它包含了给定一个隐藏状态后得到的观察状态的概率。对于天气例子，混淆矩阵是<br/>
\[<br/>
\begin{array}{c|cccc}<br/>
&amp;\quad \text{Dry}\quad &amp;\quad \text{Dryish}\quad &amp;\quad \text{Damp}\quad &amp;\quad \text{Soggy}\quad \\\hline<br/>
\text{Sun}\quad  &amp;\quad \text{0.60}\quad &amp;\quad \text{0.20}\quad &amp;\quad \text{0.15}\quad &amp;\quad \text{0.05}\quad \\<br/>
\text{Cloud}\quad &amp;\quad \text{0.25}\quad &amp;\quad \text{0.25}\quad &amp;\quad \text{0.25}\quad &amp;\quad \text{0.25}\quad \\<br/>
\text{Rain}\quad &amp;\quad \text{0.05}\quad &amp;\quad \text{0.10}\quad &amp;\quad \text{0.35}\quad &amp;\quad \text{0.50}\quad \\<br/>
\end{array}<br/>
\]</p>

<p>注意矩阵的每一行之和是 1。</p>

<h3 id="toc_0">隐马尔可夫模型</h3>

<p>一个隐马尔可夫模型通常需要三组参数组成：</p>

<ol>
<li><p>状态转移概率：模型在各个状态间转换的概率，通常记为矩阵 \(\mathbb A = [a_{ij}]_{N\times N}\)，其中：<br/>
\[<br/>
a_{ij} = P(x_{t+1} = s_j|x_t = s_i),\quad 1\le i,j\le N<br/>
\]</p>

<p>表示在任意时刻 \(t\)，状态 \(s_i\) 转移到状态 \(s_j\) 的概率。</p></li>
<li><p>输出观察概率：模型根据当前隐藏状态获得各个观察值的概率，通常记为矩阵 \(\mathbb B = [b_{ij}]_{N\times M}\)，其中<br/>
\[<br/>
b_{i}(j) = P(y_t = o_j|x_t = s_i),\quad i\le i\le N,1\le j\le M<br/>
\]</p>

<p>表示在任意时刻 \(t\)，若状态为 \(s_i\)，则观察值 \(o_j\) 被获取的概率。</p></li>
<li><p>初始状态概率：模型在初始时刻各状态出现的概率，通常记为 \(\mathbb\pi=(\pi_1,\pi_2,...,\pi_N)\)，其中：<br/>
\[<br/>
\pi_i = P(x_1 = s_i),\quad 1\le i\le N<br/>
\]</p>

<p>表示模型初始状态为 \(s_i\) 的概率。</p></li>
</ol>

<p>通过指定状态空间 \(\mathcal Y\)、观察空间 \(\mathcal X\) 和上述三组参数，就能确定一个隐马尔可夫模型，通常用其参数 \(\lambda = [\mathbb A,\mathbb B,\mathbb \pi]\) 来指代。给定隐马尔可夫模型 \(\lambda\)，它按如下过程产生观察序列 \(\{y_1,y_2,...,y_n\}\)：</p>

<p>(1)、设置 \(t=1\)，并根据初始状态概率 \(\mathbb \pi\) 选择初始状态 \(x_1\)；</p>

<p>(2)、根据状态 \(x_t\) 和输出观测概率 \(B\) 选择观测变量取值 \(y_t\)；</p>

<p>(3)、根据状态 \(x_t\) 和状态转移矩阵 \(A\) 转移模型隐藏状态，即确定 \(x_{t+1}\)；</p>

<p>(4)、若 \(t\lt n\)，设置 \(t=t+1\)，并转移到第 (2) 步，否则停止。</p>

<p>其中 \(x_t\in\{s_1,s_2,...,s_N \}\) 和 \(y_t\in\{o_1,o_2,...,o_M \}\) 分别为第 \(t\) 时刻的隐藏状态和观测状态。</p>

<p>在实际应用中，人们常关注隐马尔可夫模型的三个基本问题：</p>

<ul>
<li><p>给定模型 \(\lambda = [\mathbb A,\mathbb B,\mathbb \pi]\)，如何有效计算其产生观测序列 \(\mathbb y=\{y_1,y_2,...,y_n \}\) 的概率 \(P(\mathbb x|\lambda)\)，换言之，如何评估模型与观测序列之间的匹配程度？</p></li>
<li><p>给定模型 \(\lambda = [\mathbb A,\mathbb B,\mathbb \pi]\) 和观测序列 \(\mathbb y=\{y_1,y_2,...,y_n \}\)，如何找到与此观测序列最匹配的隐藏状态序列 \(\mathbb y=\{ y_1,y_2,...,y_n \}\)？换言之，如何根据观测序列推断出一串的模型状态？</p></li>
<li><p>给定模型 \(\mathbb x = \{x_1,x_2,...,x_n \}\)，如何调整模型参数 \(\lambda = [\mathbb A,\mathbb B,\mathbb \pi]\) 使得该序列出现的概率 \(\pi(\mathbb x,\lambda)\)？换言之，如何训练模型使其能最好的描述观测数据？</p></li>
</ul>

<p>现在简单说一下如何解决这三个问题：</p>

<h4 id="toc_1">评估</h4>

<p>计算模型 \(\lambda = [\mathbb A,\mathbb B,\mathbb \pi]\) 产生观测序列的状态通常有三种方法：</p>

<ol>
<li><p><strong>穷举法</strong>：</p>

<p>一种计算观察序列概率的方法是找到每一个可能的隐藏状态，并且将这些隐藏状态下的观察序列概率相加。考虑到之前的例子，隐藏状态集合为 \(S=\{\text{Sun},\text{Cloud},\text{Rain}\}\)，观测状态为 \(V=\{\text{Dry},\text{Dryish},\text{Damp},\text{Soggy}\}\)，假设现在观测序列只有一个元素“\(\text{Dry}\)”，我们知道观测值“\(\text{Dry}\)”对应的隐藏状态可能是 \(S\) 中的任意元素，所以观测序列的概率可以表示为：<br/>
\[<br/>
\begin{align*}<br/>
P(\text{Dry}) &amp;= P(\text{Dry},\{\text{Sun},\text{Cloud},\text{Rain}\})\\<br/>
&amp;= P(\text{Dry}|\text{Sun})P(\text{Sun}) + P(\text{Dry}|\text{Cloud})P(\text{Cloud}) + P(\text{Dry}|\text{Rain})P(\text{Rain})\\<br/>
&amp;= \sum_{\text{h} \in |S|} P(\text{Dry}|\text{h})\\<br/>
\end{align*}<br/>
\]</p>

<p>假设现在观测序列有两个元素 \(\{\text{Dry},\text{Soggy}\}\)，类比上面我们首先找出所有可能的隐藏状态序列：<br/>
\[<br/>
\{(\text{Sun},\text{Sun}),(\text{Sun},\text{Cloud}),(\text{Sun},\text{Rain}),(\text{Cloud},\text{Sun}),\\<br/>
  (\text{Cloud},\text{Cloud}),(\text{Cloud},\text{Rain}),(\text{Rain},\text{Sun}),(\text{Rain},\text{Cloud}),(\text{Rain},\text{Rain})\}<br/>
\]</p>

<p>总共是3*3 = 9 种可能，根据每种可能的隐藏状态序列，计算其生成观测序列的概率，比如 \((\text{Sun},\text{Cloud})\)：<br/>
\[<br/>
\begin{align}<br/>
P(\text{Dry},\text{Soggy}|\text{Sun},\text{Cloud}) &amp;= \underbrace{P(\text{Sun})P(\text{Dry}|\text{Sun})}_{t=1} \underbrace{P(\text{Sun}\rightarrow\text{Cloud})P(\text{Soggy}|\text{Cloud})}_{t=2}\nonumber\\<br/>
&amp;= \underbrace{P(\text{Sun})}_{\text{初始概率}} \underbrace{P(\text{Sun}\rightarrow\text{Cloud})}_{\text{转移概率}} \underbrace{P(\text{Dry}|\text{Sun})P(\text{Soggy}|\text{Cloud})}_{\text{观测概率}}\label{uttu}\\<br/>
\end{align}<br/>
\]</p>

<p>同理可以求出其他可能的隐藏状态序列生成观测序列的概率，将9 种可能概率加起来便是观测序列的概率。</p>

<p>由式(\ref{uttu})可以知道状态序列到观测序列的概率分成三个部分，初始概率、状态转移概率和观测概率（对应位置隐藏状态表现为观测值的概率）。现在我们推广到一般形式，列举所有可能的长度为 \(T\) 的状态序列 \(I=(i_1,i_2,...,i_T)\)，求各个状态序列 \(I\) 与观测序列 \(O=(o_1,o_2,...,o_T)\) 的联合概率 \(P(O,I,\lambda)\) ，然后对所有可能的状态序列求和得到 \(P(O|\lambda)\)。</p>

<p>状态序列 \(I=(i_1,i_2,...,i_T)\) 的初始概率为 \(\pi_{i_1}\)；<br/>
状态序列 \(I=(i_1,i_2,...,i_T)\) 的转移概率为 \(a_{i_1 i_2}a_{i_2 i_3}\cdots a_{i_{T-1} i_T}\)；</p>

<p>这两部分合起来就是状态序列 \(I=(i_1,i_2,...,i_T)\) 的概率：<br/>
\[<br/>
P(I|\lambda) = \pi_{i_1}a_{i_1 i_2}a_{i_2 i_3}\cdots a_{i_{T-1} i_T}<br/>
\]</p>

<p>对固定状态序列 \(I=(i_1,i_2,...,i_T)\) ，得到观测序列 \(O=(o_1,o_2,...,o_T)\) 的观测概率：<br/>
\[<br/>
P(O|I,\lambda) = b_{i_1}(o_1)b_{i_2}(o_2)\cdots b_{i_T}(o_T)<br/>
\]</p>

<p>所以 \(O\) 和 \(I\) 同时出现的联合概率为：<br/>
\[<br/>
\begin{align*}<br/>
P(O,I|\lambda) &amp;= \sum_{I} P(I|\lambda) P(O|I,\lambda)\\<br/>
&amp;= \sum_{I} \pi_{i_1}b_{i_1}(o_1) a_{i_1 i_2} b_{i_2}(o_2)\cdots a_{i_{T-1} i_T}b_{i_T}(o_T)<br/>
\end{align*}<br/>
\]</p>

<p>但是，利用上面公式计算量巨大，是 \(O(TN^T)\) 阶的，这种算法不可行的。</p></li>
<li><p><strong>前向算法</strong>：</p>

<p>前向算法定义局部概率 \(\alpha_t(i)\) 表示观测时刻 \(t\) 之前观测状态的序列为 \(o_1,o_2,...o_t\)的概率为前向概率。记为：<br/>
\[<br/>
\alpha_t(i) = P(o_1,o_2,...,o_t,i_t = s_i|\lambda)<br/>
\]</p>

<p>递推关系为<br/>
\[<br/>
\alpha_{t+1}(i) = \bigg[\sum_{j=1}^N \alpha_t(j) a_{ji}\bigg] b_{i}(o_{t+1})<br/>
\]</p>

<p>算法步骤为</p>

<p><strong>输入</strong>：假设隐马尔可夫模型 \(\lambda=(\mathbb A,\mathbb B,\pi)\)，观测序列 \(O=(o_1,o_2,...,o_T)\)，隐藏状态总共包含 N 种状态 \(S = (s_1,s_2,...,s_N)\)；<br/>
<strong>输出</strong>：观测序列概率 \(P(O|\lambda)\)<br/>
<strong>算法过程</strong>：</p>

<ul>
<li><p>初值<br/>
\[<br/>
\alpha_1(i) = \pi_{i}b_i(o_1),\quad i=1,2,\cdots,N<br/>
\]</p></li>
<li><p>递推 对 \(t=1,2,\cdots,T-1\)<br/>
\[<br/>
\alpha_{t+1}(i) = \bigg[ \sum_{j=1}^N \alpha_t(j)a_{ji} \bigg] b_i(o_{t+1}),\quad i=1,2,\cdots,N<br/>
\]</p></li>
<li><p>终止<br/>
\[<br/>
P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)<br/>
\]</p></li>
</ul>

<p>还是以天气的例子，假设 \(T=3\)，\(O=(\text{Dry,Dryish,Soggy})\)，状态转移矩阵如下图：<br/>
\[<br/>
\begin{array}{c|ccc}<br/>
&amp;\quad \text{Sun}\quad &amp;\quad \text{Cloud}\quad &amp;\quad \text{Rain}\quad \\\hline<br/>
\text{Sun}\quad  &amp;\quad \text{0.5}\quad &amp;\quad \text{0.2}\quad &amp;\quad \text{0.3}\quad \\<br/>
\text{Cloud}\quad &amp;\quad \text{0.3}\quad &amp;\quad \text{0.5}\quad &amp;\quad \text{0.2}\quad\\<br/>
\text{Rain}\quad &amp;\quad \text{0.2}\quad &amp;\quad \text{0.3}\quad &amp;\quad \text{0.5}\quad\\<br/>
\end{array}<br/>
\]</p>

<p>初始值概率为\(\pi=(\text{Sun,Cloud,Rain})=(0.2,0.4,0.4)\)</p>

<p>按照算法，（1）计算初值<br/>
\[<br/>
\alpha_1(1) = \pi_1b_1(o_1) = 0.2*0.6 = 0.12\\<br/>
\alpha_1(2) = \pi_2b_2(o_1) = 0.4*0.25 = 0.1\\<br/>
\alpha_1(3) = \pi_3b_3(o_1) = 0.4*0.05 = 0.02\\<br/>
\]</p>

<p>（2）递推计算<br/>
\[<br/>
\alpha_2(1) = \bigg[\sum_{i=1}^3 \alpha_1(i) a_{i1} \bigg] b_1(o_2) = \bigg[ 0.12 * 0.5 + 0.1 * 0.3 + 0.02 * 0.2\bigg ] * 0.2 = 0.0188 \\<br/>
\alpha_2(2) = \bigg[\sum_{i=1}^3 \alpha_1(i) a_{i2} \bigg] b_2(o_2) = \bigg[ 0.12 * 0.2 + 0.1 * 0.5 + 0.02 * 0.3\bigg ] * 0.25 =  0.02 \\<br/>
\alpha_2(3) = \bigg[\sum_{i=1}^3 \alpha_1(i) a_{i3} \bigg] b_3(o_2) = \bigg[ 0.12 * 0.3 + 0.1 * 0.2 + 0.02 * 0.5\bigg ] * 0.10  = 0.0066\\<br/>
\\<br/>
\alpha_3(1) = \bigg[\sum_{i=1}^3 \alpha_2(i) a_{i1} \bigg] b_1(o_3) = \bigg[ 0.0188 * 0.5 + 0.02 * 0.3 + 0.0066 * 0.2 \bigg] * 0.05 = 0.000836 \\<br/>
\alpha_3(2) = \bigg[\sum_{i=1}^3 \alpha_2(i) a_{i2} \bigg] b_2(o_3) = \bigg[ 0.0188 * 0.2 + 0.02 * 0.5 + 0.0066 * 0.3 \bigg] * 0.25 = 0.003935 \\<br/>
\alpha_3(3) = \bigg[\sum_{i=1}^3 \alpha_2(i) a_{i3} \bigg] b_3(o_3) = \bigg[ 0.0188 * 0.3 + 0.02 * 0.2 + 0.0066 * 0.5 \bigg] * 0.5 = 0.00647 \\<br/>
\]</p>

<p>（3）终止：<br/>
\[<br/>
P(O|\lambda) = \sum_{i=1}^3 \alpha_3(i) = 0.000836+0.003935 + 0.00647 = 0.011241<br/>
\]</p></li>
<li><p><strong>后向算法</strong></p>

<p>后向算法与前向算法原理，定义局部概率 \(\beta_t(i)\) 表示观测时刻 \(t\) 之后的观测序列 \(o_{t+1},o_{t+2},...,o_T\) 的概率为后向概率。记为：<br/>
\[<br/>
\beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t=s_i,\lambda)<br/>
\]</p>

<p>后向算法的递推公式为<br/>
\[<br/>
\beta_t(i-1) = \sum_{j=1}^N \beta_t(j) a_{ij} b_{j}(o_{t+1})<br/>
\]</p>

<p>算法初始时，\(t=T\)，此时不存在后续时刻观测序列，此时 \(\beta_T(i) = 1\)。</p>

<p><strong>输入</strong>：假设隐马尔可夫模型 \(\lambda=(A,B,\pi)\)，观测序列 \(O=(o_1,o_2,...,o_T)\)。隐藏状态总共包含 \(m\) 种状态 \(S = (s_1,s_2,...,s_m)\)；<br/>
<strong>输出</strong>：观测序列概率 \(P(O|\lambda)\)<br/>
<strong>算法过程</strong>：</p>

<ul>
<li><p>初值<br/>
\[<br/>
\beta_T(i) = 1,\quad i=1,2,\cdots,T<br/>
\]</p></li>
<li><p>递推 对 \(t=T-1,T-2,\cdots,1\)<br/>
\[<br/>
\beta_{t-1}(i) = \sum_{j=1}^N \beta_t(j)a_{ij} b_j(o_{t+1}),\quad i=1,2,\cdots,N<br/>
\]</p></li>
<li><p>终止<br/>
\[<br/>
P(O|\lambda) = \sum_{i=1}^N \beta_1(i)\pi(i)b_i(o_1)<br/>
\]</p></li>
</ul>

<p>以天气的例子，假设 \(T=3\)，\(O=(\text{Dry,Dryish,Soggy})\)，状态转移矩阵如前向算法所示。初始值概率为\(\pi=(\text{Sun,Cloud,Rain})=(0.2,0.4,0.4)\)</p>

<p>按照算法，（1）计算初值<br/>
\[<br/>
\beta_3(1) = 1\\<br/>
\beta_3(2) = 1\\<br/>
\beta_3(3) = 1\\<br/>
\]</p>

<p>（2）递推计算<br/>
\[<br/>
\begin{align*}<br/>
\beta_2(1) &amp;= \sum_{j=1}^3 \beta_3(j)a_{1j}b_{j}(o_t)\\<br/>
&amp;= 1 * 0.5 * 0.05 + 1 * 0.2 * 0.25 + 1 * 0.3 * 0.5 \\<br/>
&amp;= 0.225\\<br/>
\beta_2(2) &amp;= \sum_{j=1}^3 \beta_3(j)a_{2j}b_{j}(o_t)\\<br/>
&amp;= 1 * 0.3 * 0.05 + 1 * 0.5 * 0.25 + 1 * 0.2 * 0.5 \\<br/>
&amp;= 0.24\\<br/>
\beta_2(3) &amp;= \sum_{j=1}^3 \beta_3(j)a_{3j}b_{j}(o_t)\\<br/>
&amp;= 1 * 0.2 * 0.05 + 1 * 0.3 * 0.25 + 1 * 0.5 * 0.5 \\<br/>
&amp;= 0.335\\<br/>
\beta_1(1) &amp;= \sum_{j=1}^3 \beta_2(j)a_{1j}b_{j}(o_t)\\<br/>
&amp;= 0.225 * 0.5 * 0.20 + 0.24 * 0.2 * 0.25 + 0.335 * 0.3 * 0.10 \\<br/>
&amp;= 0.04455\\<br/>
\beta_1(2) &amp;= \sum_{j=1}^3 \beta_2(j)a_{2j}b_{j}(o_t)\\<br/>
&amp;= 0.225 * 0.3 * 0.20 + 0.24 * 0.5 * 0.25 + 0.335 * 0.2 * 0.10 \\<br/>
&amp;= 0.0502\\<br/>
\beta_1(3) &amp;= \sum_{j=1}^3 \beta_2(j)a_{3j}b_{j}(o_t)\\<br/>
&amp;= 0.225 * 0.2 * 0.20 + 0.24 * 0.3 * 0.25 + 0.335 * 0.5 * 0.10 \\<br/>
&amp;= 0.04375\\<br/>
\end{align*}<br/>
\]</p>

<p>（3）终止：<br/>
\[<br/>
\begin{align*}<br/>
P(O|\lambda) &amp;= \sum_{i=1}^3 \beta_1(i) \pi(i) b_i(o_1) \\<br/>
&amp;= 0.04455 * 0.2 * 0.6 + 0.0502 * 0.4 * 0.25 + 0.04375 * 0.4 * 0.05\\<br/>
&amp;= 0.011241<br/>
\end{align*}<br/>
\]</p></li>
</ol>

<h4 id="toc_2">解码</h4>

<p>对于一个特殊的隐马尔科夫模型(HMM)及一个相应的观察序列，我们常常希望能找到生成此序列最可能的隐藏状态序列。</p>

<ol>
<li><p><strong>穷举法</strong></p>

<p>类似前面的穷举法，对于观测序列 \(O=(o_1,o_2,...,o_t)\)，总共有 \(N^t\) 个可能的隐藏状态序列，其中 \(N\) 是隐藏状态 \(S\) 的个数。然后对于每一个可能隐藏状态序列，计算生成观测序列的状态的概率，再从中选出最大的概率的隐藏状态序列。这种方法可行，但是复杂度很高，不推荐使用。</p></li>
<li><p><strong>维特比算法</strong></p>

<p>关于维特比算法可以看之前的一篇文章介绍，这里直接介绍如何用维比特算法求解解码问题。</p>

<p>导入两个变量 \(\delta\) 和 \(\psi\)，定义在时刻 \(t\) 状态为 \(i\) 的所有单个路径 \((i_1,i_2,...,i_t)\) 中的概率最大值为<br/>
\[<br/>
\delta_t(i) = \max_{i_1,i_2,...,i_{t-1}} P(i_t=i,i_{t-1},...,i_1,o_t,...,o_1|\lambda),\quad i=1,2,...,N<br/>
\]</p>

<p>由定义可得变量 \(\delta\) 的递推公式：<br/>
\[<br/>
\begin{align*}<br/>
\delta_{t+1}(i) &amp;= \max_{i_1,i_2,...,i_{t}} P(i_{t+1}=i,i_{t},...,i_1,o_{t+1},...,o_1|\lambda)\\<br/>
&amp;= \max_{1\le j\le N}[\delta_t(j) a_{ji}]b_i(o_{t+1}),\quad i=1,2,...,N<br/>
\end{align*}<br/>
\]</p>

<p>定义在时刻 \(t\) 状态为 \(i\) 的所有单个路径 \((i_1,i_2,...,i_{t-1},i)\) 中概率最大的路径的第 \(t-1\) 个结点为<br/>
\[<br/>
\psi_t(i) = \arg\max_{1\le j\le N}[\delta_{t-1}(j) a_{ji}],\quad i=1,2,...,N<br/>
\]</p>

<p>下面介绍维比特算法。</p>

<p><strong>输入</strong>：模型 \(\lambda=(A,B,\pi)\) 和观测 \(O=(o_1,o_2,...,o_T)\)；<br/>
<strong>输出</strong>：最优状态序列 \(I=(i^*_1,i^*_2,...,i^*_T)\)；<br/>
<strong>算法过程</strong>：</p>

<ul>
<li><p>初始化<br/>
\[<br/>
\delta_1(i) = \pi_i b_i(o_1),\quad i=1,2,...,N\\<br/>
\psi_1(i) = 0<br/>
\]</p></li>
<li><p>递推，对 \(t=2,3,...,T\)<br/>
\[<br/>
\delta_t(i) = \max_{1\le j\le N}[\delta_t(j) a_{ji}]b_i(o_{t+1}),\quad i=1,2,...,N\\<br/>
\psi_t(i) = \arg\max_{1\le j\le N}[\delta_{t-1}(j) a_{ji}],\quad i=1,2,...,N<br/>
\]</p></li>
<li><p>终止<br/>
\[<br/>
P^* = \max_{1\le i\le N} \delta_{T}(i)\\<br/>
i^*_T = \arg\max_{1\le i\le N}[\delta_T(i)]<br/>
\]</p></li>
<li><p>最优路径回溯，对 \(t=T-1,T-2,...,1\)<br/>
\[<br/>
i^*_t = \psi_{t+1}(i^*_{t+1})<br/>
\]</p></li>
</ul>

<p>求得最优路径 \(I^* = (i^*_1,i^*_2,...,i^*_T)\)。</p>

<p>还是以天气的例子，假设 \(T=3\)，\(O=(\text{Dry,Dryish,Soggy})\)，状态转移矩阵同前向算法中所示，初始值概率为\(\pi=(\text{Sun,Cloud,Rain})=(0.2,0.4,0.4)\)。</p>

<p>按照算法，（1） 初始化，当 \(t=1\) 时，对每一个状态 \(i\)，\(i=1,2,3\)，求状态为 \(i\) 观测 \(o_1\) 为 Dry 的概率，记此概率为 \(\delta_1(i)\)，则：<br/>
\[<br/>
\delta_1(i) = \pi_ib_i(o_i) = \pi_ib_i(\text{Dry}),\quad i=1,2,3<br/>
\]</p>

<p>代入实际数据<br/>
\[<br/>
\delta_1(1) = \pi_1b_1(o_1) = 0.2*0.6 = 0.12\\<br/>
\delta_1(2) = \pi_2b_2(o_1) = 0.4*0.25 = 0.1\\<br/>
\delta_1(3) = \pi_3b_3(o_1) = 0.4*0.05 = 0.02\\<br/>
\]</p>

<p>记录 \(\psi_1(i) = 0\)，\(i=1,2,3\)。</p>

<p>（2）在 \(t=2\) 时，对每一个状态 \(i\)，\(i=1,2,3\)，求在 \(t=1\) 时状态为 \(j\) 观测为 Dry 并在 \(t=2\) 时状态为 \(i\) 观测 \(o_2\) 为 Dryish 的最大概率，记此最大概率为 \(\delta_2(i)\)，则<br/>
\[<br/>
\delta_2(i) = \max_{1\le j\le 3}[\delta_1(j) a_{ji}] b_i(o_2)<br/>
\]</p>

<p>同时，对每一个状态 \(i\)，\(i=1,2,3\)，记录概率最大路径的前一个状态：<br/>
\[<br/>
\psi_2(i) = \arg\max_{1\le j\le 3}[\delta_1(j)a_{ji}],\quad i=1,2,3<br/>
\]</p>

<p>计算：<br/>
\[<br/>
\begin{align*}<br/>
\delta_2(1) &amp;= \max_{1\le j\le 3}[\delta_1(j)a_{j1}] b_1(o_2)\\<br/>
&amp;= \max{j}\{0.12* 0.5,0.1 * 0.3,0.02*0.2 \}*0.2\\<br/>
&amp;= 0.012\\<br/>
\psi_2(1) &amp;= 1\\<br/>
\delta_2(2) &amp;= \max_{1\le j\le 3}[\delta_1(j)a_{j2}] b_2(o_2)\\<br/>
&amp;= \max{j}\{0.12* 0.2,0.1 * 0.5,0.02*0.3 \}*0.25\\<br/>
&amp;= 0.0125\\<br/>
\psi_2(2) &amp;= 2\\<br/>
\delta_2(3) &amp;= \max_{1\le j\le 3}[\delta_1(j)a_{j3}] b_3(o_2)\\<br/>
&amp;= \max{j}\{0.12* 0.3,0.1 * 0.2,0.02*0.5 \}*0.1\\<br/>
&amp;= 0.0036\\<br/>
\psi_2(3) &amp;= 1\\<br/>
\end{align*}<br/>
\]</p>

<p>同样在 \(t=3\) 时，<br/>
\[<br/>
\begin{align*}<br/>
\delta_3(1) &amp;= \max_{1\le j\le 3}[\delta_2(j)a_{j1}] b_1(o_3)\\<br/>
&amp;= \max{j}\{0.012* 0.5,0.0125 * 0.3,0.0036*0.2 \}*0.05\\<br/>
&amp;= 0.0003\\<br/>
\psi_3(1) &amp;= 1\\<br/>
\delta_3(2) &amp;= \max_{1\le j\le 3}[\delta_2(j)a_{j2}] b_2(o_3)\\<br/>
&amp;= \max{j}\{0.012* 0.2,0.0125 * 0.5,0.0036*0.3 \}*0.25\\<br/>
&amp;= 0.0015625\\<br/>
\psi_3(2) &amp;= 2\\<br/>
\delta_3(3) &amp;= \max_{1\le j\le 3}[\delta_2(j)a_{j3}] b_3(o_3)\\<br/>
&amp;= \max{j}\{0.012* 0.3,0.0125 * 0.2,0.0036*0.5 \}*0.5\\<br/>
&amp;= 0.0018\\<br/>
\psi_3(3) &amp;= 1\\<br/>
\end{align*}<br/>
\]</p>

<p>（3）以 \(P^*\) 表示最优路径的概率，则<br/>
\[<br/>
P^* = \max_{1\le i\le 3}\delta_3(i) = 0.0018<br/>
\]</p>

<p>最优路径终点是 \(i^*_3\)：<br/>
\[<br/>
i^*_3 = \arg\max_{i}[\delta_3(i)] = 3<br/>
\]</p>

<p>（4）由最优路径的终点 \(i^*_3\) ，逆向找到 \(i^*_2\) 和 \(i^*_1\)：<br/>
\[<br/>
i^*_2 = \psi_3(i^*_3) = 1\\<br/>
i^*_1 = \psi_2(i^*_2) = 1\\<br/>
\]</p>

<p>最优路径为 \(I=(i^*_1,i^*_2,i^*_3) = (1,1,3)\)</p></li>
</ol>

<h4 id="toc_3">学习</h4>

<p>与HMM模型相关的“有用”的问题是评估（前向算法）和解码（维特比算法）——它们一个被用来测量一个模型的相对适用性，另一个被用来推测模型隐藏的部分在做什么（“到底发生了”什么）。可以看出它们都依赖于隐马尔科夫模型（HMM）参数这一先验知识——状态转移矩阵，混淆（观察）矩阵，以及 \(\pi\) 向量（初始化概率向量）。可以分为监督学习算法和非监督学习算法--Baum-Welch算法。</p>

<ol>
<li><p><strong>监督学习算法</strong></p>

<p>假设已给出训练数据包含 \(S\) 个长度相同的观测序列和对应的状态序列 \(\{(O_1,I_1),(O_2,I_2),...,(O_S,I_S)\}\)，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数。具体方法如下.</p>

<p>（1）转移概率 \(a_{ij}\) 的估计</p>

<p>设样本中时刻 \(t\) 处于 \(i\) 状态，时刻 \(t+1\) 转移到 \(j\) 状态的频数为 \(A_{ij}\) ，那么转移概率 \(a_{ij}\) 的估计为：<br/>
\[<br/>
\hat a_{ij} = \frac{A_{ij}}{\sum_{j=1}^M A_{ij}},\quad i=1,2,...,N;j=1,2,...,N <br/>
\]</p>

<p>（2）观测概率 \(b_j(k)\) 的估计<br/>
设样本汇总状态为 \(j\) 并观测为 \(k\) 的频数为 \(B_{jk}\)，那么状态为 \(j\) 观测为 \(k\) 的概率 \(b_j(k)\) 为<br/>
\[<br/>
\hat b_j(k) = \frac{B_{jk}}{\sum_{k=1}^N B_{jk}},\quad j=1,2,...,N,k=1,2,...,M<br/>
\]</p>

<p>（3）初始状态概率 \(\pi_i\) 的估计 \(\hat \pi_i\) 为 \(S\) 个样本中初始状态为 \(q_i\) 的频率。</p>

<p>由于监督学习需要使用训练数据，而人工标注训练数据往往代价很高，有时候就会利用非监督学习的方法。</p></li>
<li><p><strong>Baum-Welch算法</strong></p>

<blockquote>
<p>在讲解Baum-Welch算法之前，我们先讲解一下单个状态和两个状态的计算公式<br/>
( a ) 给定模型 \(\lambda\) 和观测 \(O\)，在时刻 \(t\) 处于状态 \(s_i\) 的概率，记<br/>
\[<br/>
\gamma_t(i) = P(i_t = s_i|O,\lambda)<br/>
\]</p>

<p>容易得<br/>
\[<br/>
\gamma_t(i) = \frac{P(i_t = s_i,O|\lambda)}{P(O|\lambda)}<br/>
\]</p>

<p>前向概率 \(\alpha_t(i)\) 的定义为<br/>
\[<br/>
\alpha_t(i) = P(o_1,o_2,...,o_t,i_t = s_i|\lambda)<br/>
\]</p>

<p>后向概率 \(\beta_t(i)\) 的定义为<br/>
\[<br/>
\beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t=s_i,\lambda)<br/>
\]</p>

<p>由前向概率和后向概率的定义可知：<br/>
\[<br/>
\alpha_t(i) \beta_t(i) = P(O,i_t = s_i|\lambda)<br/>
\]</p>

<p>考虑到<br/>
\[<br/>
P(O|\lambda) = \sum_{j=1}^N P(O,i_t=s_j|\lambda)<br/>
\]</p>

<p>于是可得<br/>
\[<br/>
\begin{equation}<br/>
\gamma_t(i) = \frac{P(O,i_t=s_j|\lambda)}{P(O|\lambda)} = \frac{\alpha_t(i) \beta_t(i)}{\sum_{j=1}^N \alpha_t(j) \beta_t(j)}\label{gtf1}<br/>
\end{equation}<br/>
\]</p>

<p>( b ) 给定模型 \(\lambda\) 和观测 \(O\)，在时刻 \(t\) 处于状态 \(s_i\) 在下一时刻处于 \(s_j\) 的概率，记为<br/>
\[<br/>
\begin{align*}<br/>
\xi_t(i,j) = P(i_t=s_i,i_{t+1}=s_j|O,\lambda) &amp;= \frac{P(i_t=s_i,i_{t+1}=s_j,O|\lambda)}{P(O|\lambda)}\\<br/>
&amp;= \frac{P(i_t=s_i,i_{t+1}=s_j,O|\lambda)}{\sum_{i=1}^N \sum_{j=1}^N P(i_t=s_i,i_{t+1}=s_j,O|\lambda)}<br/>
\end{align*}<br/>
\]</p>

<p>而<br/>
\[<br/>
\begin{align*}<br/>
P(i_t=s_i,i_{t+1}=s_j,O|\lambda) &amp;= P(i_t=s_i,o_1,o_2,...,o_t|\lambda)a_{ij}b_j(o_{t+1}) P(o_{t+2},o_{t+3},...,o_T|i_{t+1} = s_j,\lambda) \\<br/>
&amp;= \alpha_t(i)a_{ij} b_j(o_{t+1})\beta_{t+1}(j)<br/>
\end{align*}<br/>
\]</p>

<p>于是<br/>
\[<br/>
\begin{equation}<br/>
\xi_t(i,j) = \frac{\alpha_t(i)a_{ij} b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N \alpha_t(i)a_{ij} b_j(o_{t+1})\beta_{t+1}(j)}\label{gtf2}<br/>
\end{equation}<br/>
\]</p>

<p>( c ) 将 \(\gamma_t(i)\) 和 \(\xi_t(i,j)\) 对各个时刻 \(t\) 求和，可以得到一些有用的期望值：</p>

<ul>
<li><p>在观测 \(O\) 下状态 \(i\) 出现的期望值<br/>
\[<br/>
\sum_{t=1}^T \gamma_t(i)<br/>
\]</p></li>
<li><p>在观测 \(O\) 下由状态 \(i\) 转移的期望值<br/>
\[<br/>
\sum_{t=1}^{T-1} \gamma_t(i)<br/>
\]</p></li>
<li><p>在观测 \(O\) 下由状态 \(i\) 转移到状态 \(j\) 的期望值<br/>
\[<br/>
\sum_{t=1}^{T-1} \xi_t(i,j)<br/>
\]</p></li>
</ul>
</blockquote>

<p>假设给定训练数据只包含 \(S\) 个长度为 \(T\) 的观测序列 \(\{O_1,O_2,...,O_S\}\) 而没有对应的状态序列，目标是学习隐马尔可夫模型 \(\lambda=(\mathbb A,\mathbb B,\pi)\) 的参数，我们将观测序列看做观测数据 \(O\) ，状态序列看做不可观测的隐数据 \(I\) ，那么隐马尔可夫模型事实上是一个含有隐变量的概率模型<br/>
\[<br/>
P(O|\lambda) = \sum_{I} P(O|I,\lambda) P(I|\lambda)<br/>
\]</p>

<p>它的学习参数可以由 EM 算法实现。</p>

<p>（1）确定完全数据的对数似然函数</p>

<p>所有观测数据写成 \(O=(o_1,o_2,...,o_T)\)，所有隐数据写成 \(I=(i_1,i_2,...,i_T)\)，完全数据是 \((O,I)=(o_1,o_2,...,o_T,i_1,i_2,...,i_T)\)。完全数据的对数似然函数是 \(\log P(O,I|\lambda)\)。</p>

<p>（2）EM 的E步：求Q函数 \(Q(\lambda,\overline \lambda)\)：<br/>
\[<br/>
\begin{align}<br/>
Q(\lambda,\overline \lambda) &amp;= \mathbb E_I[\log P(O,I|\lambda)|O,\overline \lambda]\nonumber\\<br/>
&amp;= \sum_I \log P(O,I|\lambda) P(I|O,\overline \lambda)\nonumber\\<br/>
&amp;= \sum_I \log P(O,I|\lambda) \frac{P(O,I|\overline \lambda)}{P(O|I,\overline \lambda)}\label{silp}\\<br/>
\end{align}<br/>
\] </p>

<p>其中，\(\overline \lambda\) 是隐马尔可夫模型参数的当前估计值，\(\lambda\) 是要极大化的隐马尔可夫模型参数。式(\ref{silp})中 \(P(O|I,\overline \lambda)\) 我们知道对于已知模型和隐状态序列这个概率是固定常数，可以略去。</p>

<p>Q函数可以写成<br/>
\[<br/>
\begin{equation}<br/>
Q(\lambda,\overline \lambda) = \sum_I \log P(O,I|\lambda){P(O,I|\overline \lambda)}\label{qlol}\\<br/>
\end{equation}<br/>
\]</p>

<p>在前文中，我们已经知道已知 \(\lambda\) 模型，可以通过前向算法得出给定观测序列的概率：<br/>
\[<br/>
\begin{align*}<br/>
P(O,I|\lambda) = \pi_{i_1} b_{i_1}(o_1) a_{i_1i_2}b_{i_2}(o_2) \cdots a_{i_{T-1} i_T}b_{i_T}(o_T)<br/>
\end{align*}<br/>
\]</p>

<p>于是函数 \(Q(\lambda,\overline \lambda)\) 可以写成：<br/>
\[<br/>
\begin{align}<br/>
Q(\lambda,\overline \lambda) &amp;= \sum_I \log P(O,I|\lambda){P(O,I|\overline \lambda)}\\<br/>
&amp;= \sum_I \log\Big[ \pi_{i_1} b_{i_1}(o_1) a_{i_1i_2}b_{i_2}(o_2) \cdots a_{i_{T-1} i_T}b_{i_T}(o_T) \Big]P(O,I|\overline \lambda)\nonumber\\<br/>
&amp;= \sum_I \log\Big[ \pi_{i_1} \prod_{t=1}^{T-1} a_{i_ti_{t+1}}  \prod_{t=1}^T b_{i_t}(o_t)\Big]P(O,I|\overline \lambda)\nonumber\\<br/>
&amp;= \sum_I \bigg\{\log\pi_{i_1} + \log\Big[\prod_{t=1}^{T-1} a_{i_ti_{t+1}}\Big] + \log\Big[\prod_{t=1}^T b_{i_t}(o_t)\Big]\bigg\}P(O,I|\overline \lambda)\nonumber\\<br/>
&amp;= \sum_I P(O,I|\overline \lambda) \log \pi_{i_1} + \sum_I P(O,I|\overline \lambda) \log\Big[\prod_{t=1}^{T-1} a_{i_ti_{t+1}}\Big] + \sum_I P(O,I|\overline \lambda) \log\Big[\prod_{t=1}^T b_{i_t}(o_t)\Big]\nonumber\\<br/>
&amp;= \sum_I P(O,I|\overline \lambda) \log \pi_{i_1} + \sum_I P(O,I|\overline \lambda) \sum_{t=1}^{T-1} \log a_{i_ti_{t+1}} + \sum_I P(O,I|\overline \lambda) \sum_{t=1}^T \log b_{i_t}(o_t)\label{sipo}\\<br/>
\end{align}<br/>
\]</p>

<p>式中求和都是对所有训练数据的序列总长度 \(T\) 进行的。</p>

<p>（3）EM算法的M步：极大化Q函数 \(Q(\lambda,\overline \lambda)\) 的模型参数 \(\mathbb A,\mathbb B,\pi\)。</p>

<p>可以看到，我们将三项中分别的对 \(I\) 的求和进行了划分。由于隐变量 \(I=(i_1,i_2,...,i_T)\) 。原来的求和需要遍历所有 \(I\) 的取值，然后进行求和，然而这基本是不可能完成的任务。改写后，我们将遍历的空间进行了划分，同时很好地将 \(P(O,I|,\overline \lambda)\) 部分改写后也融入到求和其中。比如第一项，对 \(I\) 的遍历等价于先固定状态 \(i_1\)，使其分别取值所有可能的状态（共有S个可取的离散状态），而 \(i_2,...,i_T\) 仍然像原来一样随便取值。这样，就把 \(I\) 空间划分成了S个更小的空间。然后再把这N个空间的结果相加，等价于原来对空间 \(I\) 进行遍历。</p>

<p><strong>a.式(\ref{sipo})的第一项</strong>可以写成<br/>
\[<br/>
\sum_I P(O,I|\overline \lambda) \log \pi_{i_1} = \sum_{i=1}^N P(O,i_1=i|\overline \lambda) \log \pi_{i}<br/>
\]</p>

<blockquote>
<p>之前一直很纠结为什么可以这么转换，后面通过一个小例子让我明白了，假设我们有长度为3，即 \(S=3\) 的观测数据 \(O=(o_1,o_2,o_3)\)，隐藏状态的个数为2，即 \(N=2\)，可能的隐藏状态序列有<br/>
\[<br/>
s_1 s_1 s_1;\\<br/>
s_1 s_1 s_2;\\<br/>
s_1 s_2 s_1;\\<br/>
s_1 s_2 s_2;\\<br/>
s_2 s_1 s_1;\\<br/>
s_2 s_1 s_2;\\<br/>
s_2 s_2 s_1;\\<br/>
s_2 s_2 s_2;\\<br/>
\]</p>

<p>第一项可以写成<br/>
\[<br/>
\begin{align*}<br/>
\sum_I P(O,I|\overline \lambda) \log \pi_{i_1} &amp;= P(O,\{s_1 s_1 s_1\}|\lambda)\log \pi_{s_1}  + P(O,\{s_1 s_1 s_2\}|\lambda)\log \pi_{s_1}  + P(O,\{s_1 s_2 s_1\}|\lambda)\log \pi_{s_1}  + P(O,\{s_1 s_2 s_2\}|\lambda)\log \pi_{s_1} + P(O,\{s_2,s_1,s_1\}|\pi_{s_2} + P(O,\{s_2,s_1,s_2\}|\pi_{s_2} + P(O,\{s_2,s_2,s_1\}|\pi_{s_2} + P(O,\{s_2,s_2,s_2\}|\pi_{s_2} \\<br/>
&amp;= P(O,{s_i,s_?,s_?}|\lambda)\log \pi_{s_i} \quad(i=1,2)\\<br/>
&amp;= \sum_{i=1}^2 P(O,s_i|\lambda)\log \pi_{s_i} <br/>
\end{align*}<br/>
\]</p>

<p>所以我们可以看出这样转换是可以的。</p>
</blockquote>

<p>我们又考虑到 \(\pi_i\) 满足约束条件 \(\sum_{i=1}^N \pi_i = 1\)，利用拉格朗日乘子法，写出拉格朗日函数：<br/>
\[<br/>
\sum_{i=1}^N P(O,i_1=i|\overline \lambda) \log \pi_{i} + \gamma\bigg(\sum_{i=1}^N \pi_i - 1 \bigg)<br/>
\]</p>

<p>对其求偏导并令结果为0<br/>
\[<br/>
\frac{\partial}{\partial \pi_i}\bigg[ \sum_{i=1}^N P(O,i_1=i|\overline \lambda) \log \pi_{i} + \gamma\bigg(\sum_{i=1}^N \pi_i - 1 \bigg) \bigg] = 0<br/>
\]</p>

<p>得<br/>
\[<br/>
\begin{align}<br/>
P(O,i_1=i|\overline \lambda) + \gamma \pi_i = 0\label{poli}<br/>
\end{align}<br/>
\]</p>

<p>对 \(i\) 求和得到 \(\lambda\)<br/>
\[<br/>
\lambda = -P(O|\overline \lambda)<br/>
\]</p>

<p>代入(\ref{poli})式得<br/>
\[<br/>
\pi_i = \frac{P(O,i_1=i|\overline \lambda)}{P(O|I)}<br/>
\]</p>

<p><strong>b.式(\ref{sipo})的第二项</strong>可以写成<br/>
\[<br/>
\sum_I P(O,I|\overline \lambda) \sum_{t=1}^{T-1} \log a_{i_ti_{t+1}} = \sum_{i=1}^N \sum_{j=1}^N \sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda) \log a_{ij}<br/>
\]</p>

<p>应用具有约束条件 \(\sum_{j=1}^N a_{ij} = 1\) 的拉格朗日乘子法可以求出<br/>
\[<br/>
\sum_{i=1}^N \sum_{j=1}^N \sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda) \log a_{ij} + \gamma\bigg(\sum_{j=1}^N a_{ij} - 1\bigg)<br/>
\]</p>

<p>对 \(a_{ij}\) 求偏导并令结果为0<br/>
\[<br/>
\begin{align*}<br/>
&amp;\frac{\partial}{\partial a_{ij}}\bigg[\sum_{i=1}^N \sum_{j=1}^N \sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda) \log a_{ij} + \gamma\bigg(\sum_{j=1}^N a_{ij} - 1\bigg) \bigg]\\<br/>
&amp;= \frac{\sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda) }{a_{ij}} + \gamma = 0 \\<br/>
\end{align*}<br/>
\]</p>

<p>得<br/>
\[<br/>
\begin{equation}<br/>
\sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda) = \gamma a_{ij}\label{stto}<br/>
\end{equation}<br/>
\]</p>

<p>对 \(j\) 求和可得<br/>
\[<br/>
\begin{align*}<br/>
&amp;\sum_{j=1}^N \sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda) = \sum_{j=1}^N \gamma a_{ij}\\<br/>
&amp;\Rightarrow \sum_{t=1}^{T-1} P(O,i_t=i|\overline \lambda) = \gamma\\<br/>
\end{align*}<br/>
\]</p>

<p>上式代入(\ref{stto})得<br/>
\[<br/>
\begin{align*}<br/>
&amp;\sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda) = \sum_{t=1}^{T-1} P(O,i_t=i|\overline \lambda) a_{ij}\\<br/>
&amp;\Rightarrow a_{ij} = \frac{\sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda)}{\sum_{t=1}^{T-1} P(O,i_t=i|\overline \lambda)}<br/>
\end{align*}<br/>
\]</p>

<p><strong>c.式(\ref{sipo})的第三项</strong>可以写成<br/>
\[<br/>
\sum_I P(O,I|\overline \lambda) \sum_{t=1}^T \log b_{i_t}(o_t) = \sum_{j=1}^N\sum_{t=1}^T  P(O,i_t=j|\overline \lambda) \log b_j(o_t)<br/>
\]</p>

<p>同样用拉格朗日乘子法，约束条件是 \(\sum_{k=1}^M b_j(k) = 1\)，拉格朗日方程为<br/>
\[<br/>
\sum_{j=1}^N \sum_{t=1}^TP(O,i_t=j|\overline \lambda)  \log b_j(o_t) + \gamma\bigg(\sum_{k=1}^M b_j(k) - 1 \bigg)<br/>
\]</p>

<p>对 \(b_j(k)\) 求导得<br/>
\[<br/>
\begin{align*}<br/>
&amp;\frac{\partial}{\partial b_j(k)}\bigg[\sum_{j=1}^N \sum_{t=1}^T P(O,i_t=j|\overline \lambda) \log b_j(o_t) + \gamma\bigg(\sum_{k=1}^M b_j(k) - 1 \bigg)\bigg]\\<br/>
&amp;= \frac{\sum_{t=1}^T P(O,i_t=j|\overline \lambda) \mathbf I(o_t = k)}{b_j(k)}+ \gamma<br/>
\end{align*}<br/>
\]</p>

<p>这里只有在 \(o_t = k\) 时 \(b_j(o_t)\) 对 \(b_j(k)\) 的偏导数才不为0，以指示函数 \(\mathbf I(o_t = k)\) 表示。</p>

<p>令上式偏导结果为0得<br/>
\[<br/>
\begin{align}<br/>
\sum_{t=1}^T P(O,i_t=j|\overline \lambda) \mathbf I(o_t = k) = b_j(k) \gamma\label{stoto}<br/>
\end{align}<br/>
\]</p>

<p>对上式 \(k\) 求和，可得<br/>
\[<br/>
\begin{align*}<br/>
&amp;\sum_{k=1}^N \sum_{t=1}^T P(O,i_t=j|\overline \lambda) \mathbf I(o_t = k) = \sum_{k=1}^N b_j(k) \gamma\\<br/>
&amp;\Rightarrow \sum_{t=1}^T P(O,i_t=j|\overline \lambda) = \gamma<br/>
\end{align*}<br/>
\]</p>

<p>代入(\ref{stoto})式得<br/>
\[<br/>
\begin{align*}<br/>
&amp;\sum_{t=1}^T P(O,i_t=j|\overline \lambda) \mathbf I(o_t = k) = b_j(k) \sum_{t=1}^T P(O,i_t=j|\overline \lambda)\\ <br/>
&amp;\Rightarrow b_j(k) = \frac{\sum_{t=1}^T P(O,i_t=j|\overline \lambda) \mathbf I(o_t = k)}{\sum_{t=1}^T P(O,i_t=j|\overline \lambda)}<br/>
\end{align*}<br/>
\]</p>

<p>到现在我们已经推出了 \(\pi_i\)、\(a_{ij}\) 和 \(b_{j}(k)\) 的迭代公式<br/>
\[<br/>
\begin{align*}<br/>
\pi_i &amp;= \frac{P(O,i_1=i|\overline \lambda)}{P(O|I)}\\<br/>
a_{ij} &amp;= \frac{\sum_{t=1}^{T-1} P(O,i_t=i,i_{t+1}=j|\overline \lambda)}{\sum_{t=1}^{T-1} P(O,i_t=i|\overline \lambda)}\\<br/>
b_j(k) &amp;= \frac{\sum_{t=1}^T P(O,i_t=j|\overline \lambda) \mathbf I(o_t = k)}{\sum_{t=1}^T P(O,i_t=j|\overline \lambda)}\\<br/>
\end{align*}<br/>
\]</p>

<p>将式(\ref{gtf1})和式(\ref{gtf2}) 代入<br/>
\[<br/>
\begin{align*}<br/>
\pi_i &amp;= \gamma_1(i)\\<br/>
a_{ij} &amp;= \frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i) }\\<br/>
b_j(k) &amp;= \frac{\sum_{t=1}^T \gamma_t(j) \mathbf I(o_t=k)}{\sum_{t=1}^T \gamma_t(j)}\\<br/>
\end{align*}<br/>
\]</p>

<p>现在Baum-Welch算法可以描述为<br/>
<strong>输入</strong>：观测数据 \(O=(o_1,o_2,...,o_T)\)；<br/>
<strong>输出</strong>：隐马尔可夫模型参数<br/>
<strong>算法过程</strong></p>

<ul>
<li>初始化，对 \(n=0\)，选取 \({a_{ij}}^{(0)}\)，\({b_j(k)}^{(0)}\) 和 \({\pi_i}^{(0)}\)，得到模型 \(\lambda^{(0)} = (\mathbb A^{(0)},\mathbb B^{(0)},\pi^{(0)})\)</li>
<li><p>递推，对 \(n=1,2,...,\)<br/>
\[<br/>
\begin{align*}<br/>
{a_{ij}^{(n+1)}} &amp;= \frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i) }\\<br/>
{b_j(k)}^{(n+1)} &amp;= \frac{\sum_{t=1}^T \gamma_t(j) \mathbf I(o_t=k)}{\sum_{t=1}^T \gamma_t(j)}\\<br/>
{\pi_i}^{(n+1)} &amp;= \gamma_1(i)\\<br/>
\end{align*}<br/>
\]</p>

<p>右侧各值按观测 \(O=(o_1,o_2,...,o_T)\) 和模型 \(\lambda^{(n)} = (\mathbb A^{(n)},\mathbb B^{(n)},\pi^{(n)})\) 计算。式中 \(\gamma_t(i)\) 和 \(\xi_t(i,j)\) 由式(\ref{gtf1})和式(\ref{gtf2})给出。</p></li>
<li><p>终止。得到模型 \(\lambda^{(n+1)} = (\mathbb A^{(n+1)},\mathbb B^{(n+1)},\pi^{(n+1)})\)</p></li>
</ul></li>
</ol>

<hr/>

<p>[李航 统计学习方法]<br/>
<a href="http://www.52nlp.cn/category/hidden-markov-model">HMM相关文章索引</a></p>

			</div>

		
	  
		<footer>
		 <p class="meta">

			<strong>Categories:</strong>&nbsp; 
			<span class="categories">
			
			    <a class='category' href='HMM.html'>HMM</a>&nbsp;
			 
			</span>
		    </p>
		    <p class="meta">
		      
		 </p>
	    
		<div class="sharing">
		  
          

          

		</div>

	    <p class="meta">
	    
	        <a class="basic-alignment left" href="15218222204104.html" 
	        title="Previous Post: Rademacher 复杂度">&laquo; Rademacher 复杂度</a>
	    
	    
	        <a class="basic-alignment right" href="15200089476142.html" 
	        title="Next Post: 正态分布的累积分布函数 Cumulative distribution function of Normal distribution">正态分布的累积分布函数 Cumulative distribution function of Normal distribution &raquo;</a>
	    
	    </p>
	  </footer>
	</article>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>