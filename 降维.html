
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  降维 - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div class="blog-index">

	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15041093080052.html">线性判别分析 LDA</a></h1>
			<p class="meta"><time datetime="2017-08-31T00:08:28+08:00" 
			pubdate data-updated="true">2017/8/31</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>前面我们讨论过PCA、SVD对数据降维，但是PCA和SVD都是无监督的方法，而在一些有标签的数据中，如果能结合标签对数据降维，效果会更好。因此产生了LDA的方法，LDA英文全称是Linear Discriminant Analysis，线性判别方法，LDA作为一种降维方法，在有效降低维度的同时保证了数据的可分性。</p>

<div align="center">
    <img src="media/15041093080052/15274469363430.jpg" width="400px" />
</div>
如上图如果不考虑数据的标签，采用PCA方法对数据降维到一维空间，由PCA可知将按照最大方差的方向进行映射也就是映射到红线的位置，本来易分的数据将会变得不易分，对于这个例子来说是得不偿失的。而我们如果将所有点都投影到黑线上，效果将非常好，这就是LDA方法所考虑的。

LDA方法的基本思想是将高维数据映射到最佳鉴别空间，达到降维和提取特征的目的，投影后保证在新空间数据点具有最大的类间距离和最小的类内距离。也就是上图中蓝色点之间的距离最小，而蓝色点和绿色点之间的距离最大。在继续讨论LDA之前，先看一些相关的前置知识。

#前置知识

#### 投影

对于一个高维空间里的样本$X_i$投影到一个向量$W$上，如果$X_i$在投影前是在是一个$n$维空间，我们期望投影后降低到$k$维空间，那么$W$将是一个$n\times k$形状的矩阵。设在新空间中（可能是低维）的点为$Z_i$，满足：
$$
Z_i = W^T X_i
$$

其中$Z_{ij}=W_j^TX_i$表示点$X_i$在新坐标空间中第$j$维的坐标。如果我们将$Z_i$还原到原空间，可以通过$X^*_i = WZ_i$，其中$X_{ij} = W_jZ_i$表示新空间点$Z_i$转换回原空间在第j维的坐标。在本文中如果我们将二维空间数据点投影到一条直线上，如下图所示，点$X=(2,4)^T$在向量$w=(-\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})^T$上的投影为$Z=W^TX = \sqrt{2}$。

<div align="center">
    <img src="media/15041093080052/15276993092656.jpg" width="350px" />
</div>

<p>在SVD的前置知识里，我们已经提到过通过正交矩阵将点可以从一个坐标系转换到另一个坐标系，这是一种\(W\)是正交矩阵的投影特例。</p>

<h4 id="toc_0">Rayleigh商矩阵</h4>

<p>定义Rayleigh商矩阵：<br/>
\[<br/>
R(A,x) = \frac{x^HAx}{x^Hx}<br/>
\]</p>

<p>其中\(x\)是非零向量，而\(A\)是\(n\times n\)的Hermitian矩阵（厄米特矩阵），厄米特矩阵是共轭转置等于本身的矩阵。假设\(A\)的\(n\)个特征值依次为\(\lambda_1\le\lambda_2\le...\le\lambda_n\)，则有：<br/>
\[<br/>
\lambda_1 \le R(A,x) \le \lambda_n<br/>
\]</p>

<p>证明：假设特征值\(\lambda_1,\lambda_2,...,\lambda_n\)对应的单位特征向量为：\(x_1,x_2,...,x_n\)，由于\(A\)是厄米特矩阵，可知\(A\)可以正交对角化，即特征矩阵\(X=(x_1,x_2,...,x_n)\)同时也是个单位正交矩阵，每一个元素都可看成向量空间的一组正交基。将向量\(x\)用这组正交基表示：<br/>
\[<br/>
x = a_1x_1+a_2x_2+....a_nx_n=(x_1,x_2,...,x_n)(a_1,a_2,...,a_n)^H = X\boldsymbol a<br/>
\]</p>

<p>其中\(\boldsymbol a = (a_1,a_2,...,a_n)^H\)，将上式代入Rayleigh矩阵可得：<br/>
\[<br/>
\begin{align*}<br/>
R(A,x) = \frac{x^HAx}{x^Hx} &amp;= \frac{(X\boldsymbol a)^HA(X\boldsymbol a)}{(X\boldsymbol a)^H(X\boldsymbol a)} \\<br/>
&amp;= \frac{\boldsymbol a^H X^H A X \boldsymbol a}{\boldsymbol a^H X^H X \boldsymbol a}<br/>
\end{align*}<br/>
\]</p>

<p>由实对称矩阵对角会性质可知：\(X^TAX=\Lambda\)，其中每一个对角线元素都是\(A\)的特征值。且正交矩阵\(X\)满足\(X^HX = 1\)所以：<br/>
\[<br/>
\begin{align*}<br/>
R(A,x) = \frac{x^HAx}{x^Hx} &amp;= \frac{\boldsymbol a^H X^H A X \boldsymbol a}{\boldsymbol a^H X^H X \boldsymbol a} \\<br/>
&amp;= \frac{\boldsymbol a^H \Lambda \boldsymbol a}{\boldsymbol a^H \boldsymbol a} \\<br/>
&amp;= \frac{[a_1,a_2,...,a_n] \left[ \begin{array}\\\lambda_1&amp;&amp;&amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_n\\\end{array}\right] \left[\begin{array}\\a_1\\a_2\\...\\a_n\\\end{array}\right]}{\boldsymbol a^H \boldsymbol a} \\<br/>
&amp;=\frac{\sum_{i=1}^N\lambda_i a_i^2}{\sum_{i=1}^N a_i^2}<br/>
\end{align*}<br/>
\]</p>

<p>所以我们可以将\(R(A,x)\)看成\(\lambda_n\)的加权平均值，权系数是\(a_i^2\)，假设\(\lambda_1\le\lambda_2\le...\le\lambda_n\)，明显对于任意\(i=(1,2,...,n)\)有\(\lambda_ia_i^2\ge \lambda_1a_i^2\)，所以\(\sum_{i=1}^N\lambda_ia_i^2\ge n\lambda_1a_i^2=\sum_{i=1}^N\lambda_1a_i^2 \)，所以：<br/>
\[<br/>
R(A,x) = \frac{\sum_{i=1}^N\lambda_i a_i^2}{\sum_{i=1}^N a_i^2} \ge \frac{\sum_{i=1}^N\lambda_1a_i^2}{\sum_{i=1}^N a_i^2} = \lambda_1<br/>
\]</p>

<p>同理可证：\(R(A,x) \le \lambda_n\)。综合可知：<br/>
\[<br/>
\lambda_1 \le R(A,x) \le \lambda_n<br/>
\]</p>

<p>当\(R(A,x)\)取最大值或最小值时，令\(R(A,x)=\lambda\)，此时\(\lambda\)是A的特征值：<br/>
\[<br/>
\begin{align*}<br/>
&amp;R(A,x) = \frac{x^HAx}{x^Hx} = \lambda \\<br/>
&amp;\Rightarrow x^HAx = \lambda x^Hx<br/>
\end{align*}<br/>
\]</p>

<p>两边同时左乘\((x^{H})^{-1}\)得：\(Ax = \lambda x\)，所以此时\(\lambda\)对应的特征向量为\(x\)。</p>

<p>另外当\(x\)是正交矩阵时，\(x^Hx=1\)，\(R(A,x) = x^HAx\)，这个形式在很多理论中都有使用。</p>

<h4 id="toc_1">广义Rayleigh商矩阵</h4>

<p>定义广义Rayleigh商矩阵:<br/>
\[<br/>
R(A,B,x) = \frac{x^HAx}{x^HBx}<br/>
\]</p>

<p>其中A,B都是\(n\times n\)的厄米特矩阵，且B正定，那么这种形式的R(A,B,x)最大值与最小值是多少呢？其实很容易想到，只要我们设\(x^{&#39;}\)，然后能将上式化为R(A,x)形式即可。</p>

<p>首先看分母，我们希望\(x^HBx=x^{&#39;H}x^{&#39;}\)，所以可以分解得：<br/>
\[<br/>
x^HBx =x^H(B^{1/2})^2x= x^H(B^{1/2})^HB^{1/2}x=(B^{1/2}x)^H(B^{1/2}x)<br/>
\]</p>

<p>这样如果令\(x{&#39;} = B^{1/2}x\)，则满足我们期望，此时\(x = B^{-1/2}x^{&#39;}\)，代入得：<br/>
\[<br/>
\begin{align*}<br/>
x^HBx &amp;= (B^{-1/2}x^{&#39;})^HBB^{-1/2}x^{&#39;} = x^{&#39;H}(B^{-1/2})^HBB^{-1/2}x^{&#39;} = x^{&#39;H}B^{-1/2}BB^{-1/2}x^{&#39;}  = x^{&#39;H}x^{&#39;} \\<br/>
x^HAx &amp;= (B^{-1/2}x^{&#39;})^HAB^{-1/2}x^{&#39;} = x^{&#39;H}(B^{-1/2})^HAB^{-1/2}x^{&#39;} = x^{&#39;H}B^{-1/2}AB^{-1/2}x^{&#39;} <br/>
\end{align*}<br/>
\]</p>

<p>所以\(R(A,B,x)\)可以化为\(R(A,B,x{&#39;})\)：<br/>
\[<br/>
R(A,B,x&#39;) = \frac{x^{&#39;H}B^{-1/2}AB^{-1/2}x^{&#39;} }{x^{&#39;H}x^{&#39;}}<br/>
\]</p>

<p>由普通Rayleigh商的性质可知\(R(A,B,x^{&#39;})\)的最大值为\(B^{-1/2}AB^{-1/2}\)特征值的最大值，最小值为\(B^{-1/2}AB^{-1/2}\)特征值的最小值。此时我们考虑假设特征值为\(\lambda\)，对应的特征向量为\(x^{&#39;}\)，有：<br/>
\[<br/>
B^{-1/2}AB^{-1/2} x^{&#39;} = \lambda x^{&#39;}<br/>
\]</p>

<p>两边同左乘上\(B^{-1/2}\)，可得：<br/>
\[<br/>
B^{-1}AB^{-1/2} x^{&#39;} = \lambda B^{-1/2} x^{&#39;} \Leftrightarrow B^{-1}A(B^{-1/2} x^{&#39;}) = \lambda (B^{-1/2} x^{&#39;})<br/>
\]</p>

<p>所以\(B^{-1}AB^{-1/2}\)的特征值等同于\(B^{-1}A\)的特征值，所以\(R(A,B,x)\)的最大值为\(B^{-1}A\)特征值的最大值，最小值为\(B^{-1}A\)特征值的最小值，对应的特征向量为\(B^{-1/2}x^{&#39;}\)，即\(x\)，实现了一个完美的统一。</p>

<h1 id="toc_2">LDA解释</h1>

<p>为什么叫线性判别分析呢？所谓的线性是将高维空间投影到直线上（可能是多条直线），直线的函数解析式叫做线性函数，通常函数解析式如下面的形式：<br/>
\[<br/>
y=w^Tx+b<br/>
\]</p>

<p>由前面可知如果将两个数据点\(x_i\)投影到直线上，那么投影后的点的坐标为\(W^Tx_i\)，LDA希望通过线性函数投影后的数据点可以很好通过一个判别式划分。我们首先来看一下二分类问题，然后将其推广到多分类问题上。</p>

<h2 id="toc_3">二分类问题</h2>

<p>假设我们有数据集\(D = ((x_1,y_1),(x_2,y_2),...,(x_m,y_m))\)，其中任意\(x_i\)一个\(n\)维的向量，\(y_i \in (0,1)\)。我们使用\(X_i(i=0,1)\)表示\(D\)中的两类样本集，\(N_i(i=0,1)\)表示样本集\(X_i\)的数量，\(u_i(i=0,1)\)表示样本集\(X_i\)的均值向量，\(\Sigma_i(i=0,1)\)表示样本集\(X_i\)的方差（没有分母的）。</p>

<p>\(u_i\)方程表示为：<br/>
\[<br/>
u_i = \frac{1}{N_i}\sum_{x \in X_i}x\quad(i=0,1)<br/>
\]<br/>
\(\Sigma_i\)方程表示：<br/>
\[<br/>
\Sigma_{i} = \sum_{x\in X_i}(x-u_i)(x-u_i)^T\quad(i=0,1)<br/>
\]</p>

<p>由于是二分类问题，当我们将数据投影到一条直线上，假设投影向量为\(W\)，由\(W\)投影后两类样本集的中心点分别为\(W^Tu_0\)和\(W^Tu_1\)，投影后的方差为：<br/>
\[<br/>
\begin{align*}<br/>
\Sigma^{new}_{i} &amp;= \sum_{x\in X_i}(w^Tx-w^Tu_i)^2 = \sum_{x\in X_i}(w^Tx-w^Tu_i)(w^Tx-w^Tu_i)^T \\<br/>
&amp;= \sum_{x\in X_i} w^T(x-u_i)(x-u_i)^Tw = w^T\Sigma_iw<br/>
\end{align*}<br/>
\]</p>

<p>LDA期望投影后的数据易区分，满足同类数据尽可能密集，也就是希望最小化方差\(w^T\Sigma_0w+w^T\Sigma_1w\)。非同类元素尽可能分离，即最大化均值距离\(||w^Tu_0-w^Tu_1||^2\)，综上可以定义优化目标为：<br/>
\[<br/>
\begin{equation}<br/>
J=\frac{||w^Tu_0-w^Tu_1||^2}{w^T\Sigma_0w+w^T\Sigma_1w} = \frac{w^T(u_0-u_1)(u_0-u_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w} \label{J}<br/>
\end{equation}<br/>
\]</p>

<p>定义类内方差\(S_w\)为：<br/>
\[<br/>
S_w = \Sigma_0+\Sigma_1 = \sum_{x\in X_0}(x-u_0)(x-u_0)^T + \sum_{x\in X_1}(x-u_1)(x-u_1)^T<br/>
\]</p>

<p>定义类间距离\(S_b\)为：<br/>
\[<br/>
S_b = (u_0-u_1)(u_0-u_1)^T<br/>
\]</p>

<p>这样优化目标可以改写为：<br/>
\[<br/>
J = \frac{w^TS_bw}{w^TS_ww}<br/>
\]</p>

<p>对比广义Rayleigh商矩阵，发现形式一模一样，现在利用广义Rayleigh商矩阵的性质可知\(J\)的最大值即为矩阵\(S_w^{-1}S_b\)的最大特征值，此时对应的特征向量为\(w\)。</p>

<p>对于二分类问题来说，设\(\lambda_w = (u_0-u_1)^Tw\)，所以\(S_bw = (u_0-u_1)\lambda_w\)，将其代入特征方程\(S_w^{-1}S_bw = \lambda w\)得\(S_w^{-1} (u_0-u_1)\lambda_w = \lambda w\)，可解得\(w = \frac{\lambda_w}{\lambda}S_w^{-1} (u_0-u_1)\)。</p>

<p>由于\((u_0-u_1)\)是一个\(n\)维数据，并由前面投影的知识\(w\)的形状为\((n \times 1)\)，所以乘积\(\lambda_w=(u_0-u_1)^Tw\)是一个标量。由\(J\)的方程（\ref{J}）可知我们\(w\)的大小增大或缩小多少倍对于结果并没有影响，所以我们并不关心\(w\)的大小，只关心\(w\)的方向，将常数省去可得\(w = S_w^{-1} (u_0-u_1)\)，也就意味着当我们求出二分类的方差和均值就可以确定最佳投影方向\(w\)了。</p>

<h2 id="toc_4">多分类问题</h2>

<p>同样我们假设数据集\(D=((x_0,y_0),(x_1,y_1),...,(x_m,y_m))\)，其中任意的\(x_i\)是一个\(n\)维向量；\(y_i\in (C_1,C_2,...,C_k)\)，也就是数据集包含\(k\)类数据；数据集大小为\(N\)。在二类LDA中，是将原数据投影到一条直线（一维）上，由前置知识中可知\(W\)是一个\((n \times 1)\)的矩阵，而在多分类中，需要将数据投影到低维空间上，假设投影后的维度为 \(d\) ，投影向量\(W\)是一个\((n\times d)\)的矩阵，定义\(u_i\)为第\(\,i\,\)类的均值向量。<br/>
\[<br/>
u_i = \frac{1}{N_i}\sum_{x \in X_i} x<br/>
\]</p>

<p>此时需要将之前我们定义的概念做一下替换，定义类内散度矩阵\(S_W\)替换二分类LDA中的方差，即：<br/>
\[<br/>
S_W = \sum_{i=1}^k \Sigma_i = \sum_{i=1}^k \sum_{x\in X_i}(x-u_i)(x-u_i)^T<br/>
\]</p>

<p>定义投影后的类内距离\(J_W\)，其中点\(x\)投影后为\(W^Tx\)，中心点\(u_i\)投影后为\(W^Tu_i\)，可得：<br/>
\[<br/>
J_W = \sum_{i=1}^k \sum_{x\in X_i}(W^Tx-W^Tu_i)(W^Tx-W^Tu_i)^T = W^T\sum_{i=1}^k \sum_{x\in X_i}(x-u_i)(x-u_i)^TW = W^TS_WW<br/>
\]</p>

<p>定义类间散度矩阵\(S_B\)替换二分类LDA中的类间距离，但是此时已经不能用两个类中心的距离来表示类间散度距离了，考虑下图：</p>

<div align="center">
    <img width="300px" src="media/15041093080052/15280361373656.jpg" />
</div>

<p>定义全局中心点 \(u\) ：<br/>
\[<br/>
\begin{equation}<br/>
u = \frac{1}{N}\sum_{i=1}^N x_i = \frac{1}{N} \sum_{i=1}^k \sum_{x\in X_i} x= \frac{1}{N} \sum_{i=1}^k N_i u_i \label{u_u_i}<br/>
\end{equation}<br/>
\]</p>

<p>定义全局散度矩阵\(S_t\)为每一个点到全局中心点\(u\)的向量距离：<br/>
\[<br/>
S_t = \sum_{i=1}^N (x_i-u)(x_i-u)^T<br/>
\]</p>

<p>考虑到全局散度矩阵\(S_t\)=类内散度矩阵\(S_W\)+类间散度矩阵\(S_B\)，所以：<br/>
\[<br/>
\begin{align}<br/>
S_B &amp;= S_t - S_W = \sum_{i=1}^N (x_i-u)(x_i-u)^T - \sum_{i=1}^k \sum_{x\in X_i}(x-u_i)(x-u_i)^T \nonumber\\<br/>
&amp;=\sum_{i=1}^k \sum_{x\in X_i}(x - u)(x-u)^T - \sum_{i=1}^k \sum_{x\in X_i}(x-u_i)(x-u_i)^T \nonumber\\<br/>
&amp;=\sum_{i=1}^k \sum_{x\in X_i}[(x - u)(x-u)^T-(x-u_i)(x-u_i)^T ]\nonumber\\<br/>
&amp;=\sum_{i=1}^k \sum_{x\in X_i}(xx^T - xu^T-ux^T+uu^T-xx^T+xu_i^T+u_ix^T-u_iu_i^T) \nonumber\\<br/>
&amp;=\sum_{i=1}^k \sum_{x\in X_i}(- xu^T-ux^T+uu^T+xu_i^T+u_ix^T-u_iu_i^T) \nonumber\\<br/>
&amp;=\sum_{i=1}^k(-\sum_{x\in X_i}xu^T-\sum_{x\in X_i}ux^T+\sum_{x\in X_i}uu^T+\sum_{x\in X_i}xu_i^T+\sum_{x\in X_i}u_ix^T-\sum_{x\in X_i}u_iu_i^T) \label{S_B_T_W}\\<br/>
\end{align}<br/>
\]</p>

<p>考虑到\(u_i\)为第\(i\)类的均值向量，即：<br/>
\[<br/>
\begin{align*}<br/>
&amp;u_i = \frac{1}{N_i}\sum_{x\in X_i}x \quad\Rightarrow\quad \sum_{x\in X_i}x = N_iu_i \\<br/>
&amp;u_i^T = \frac{1}{N_i}\sum_{x\in X_i}x^T \quad\Rightarrow\quad \sum_{x\in X_i}x^T = N_iu_i^T \\<br/>
\end{align*}<br/>
\]</p>

<p>将上式代入公式（\ref{S_B_T_W}）中得：<br/>
\[<br/>
\begin{align}<br/>
S_B &amp;= \sum_{i=1}^k(-\sum_{x\in X_i}xu^T-\sum_{x\in X_i}ux^T+\sum_{x\in X_i}uu^T+\sum_{x\in X_i}xu_i^T+\sum_{x\in X_i}u_ix^T-\sum_{x\in X_i}u_iu_i^T)  \nonumber\\<br/>
&amp;= \sum_{i=1}^k(-N_iu_iu^T-N_iuu_i^T+N_iuu^T+N_iu_iu_i^T+N_iu_iu_i^T-N_iu_iu_i^T) \nonumber\\<br/>
&amp;= \sum_{i=1}^kN_i(-u_iu^T-uu_i+uu^T+u_iu_i^T) \nonumber\\<br/>
&amp;= \sum_{i=1}^kN_i(u_i-u)(u_i-u)^T \label{S_B}<br/>
\end{align}<br/>
\]</p>

<p>抛去数学推导，直观上看，观察类间散度矩阵\(S_B\)，其实就是每一个类的中心点到全局中心点\(u\)的向量距离，由于每一个类包含数据数量不同，使用了\({N_i}\)作为第\(i\)类的权重。</p>

<p>定义投影后的类间距离\(J_B\)，局部中心点\(u_i\)投影后为\(W^Tu_i\)，全局中心点\(u\)在投影后为\(W^Tu\)，得：<br/>
\[<br/>
J_B = \sum_{i=1}^kN_i(W^Tu_i-W^Tu)(W^Tu_i-W^Tu)^T = W^T\sum_{i=1}^kN_i(u_i-u)(u_i-u)^TW = W^TS_BW<br/>
\]</p>

<p>则目标函数\(J(W)为\)：<br/>
\[<br/>
J(W) = \frac{J_B}{J_W} = \frac{W^TS_BW}{W^TS_WW}<br/>
\]</p>

<p>我们的目标是最大化\(J(W)\)，而当成比例的放大或缩小\(W\)时，并不影响\(J(W)\)的取值，因此我们关心的是\(W\)的方向，而不在意\(W\)的大小。因此我们可以固定分母为1，那么目标方程组为（这里我将求最大化问题变成了求最小值问题，其实等价）：<br/>
\[<br/>
\begin{align*}<br/>
&amp;min_W\quad -W^TS_BW \\<br/>
&amp;s.t. \quad \quad W^TS_WW = 1<br/>
\end{align*}<br/>
\]</p>

<p>利用拉格朗日乘子法定义拉格朗日朗日方程：<br/>
\[<br/>
L(W,\alpha) = -W^TS_BW + \alpha(W^TS_WW-1)<br/>
\]</p>

<p>其中\(\lambda\)为乘子，满足\(\lambda \neq 0\)。<br/>
先求\(L(W,\alpha)\)对\(W\)求导，并令其等于0，即：<br/>
\[<br/>
\begin{align}<br/>
\frac{\nabla L(W,\alpha)}{\nabla W} &amp;= \frac{-W^TS_BW + \lambda(W^TS_WW-1)}{\nabla W} \nonumber\\<br/>
&amp;= -(S_B+S_B^T)W+\alpha(S_W+S_W^T)W \label{LW}\\<br/>
\end{align}<br/>
\]</p>

<blockquote>
<p>矩阵求导公式，\(\alpha\)是实数，\(\beta\)和\(X\)为向量，\(A\),\(B\),\(C\)是与\(X\)无关的矩阵：<br/>
\[<br/>
\begin{align*}<br/>
&amp;\frac{\partial \beta X}{\partial X^T} = \beta \\<br/>
&amp;\frac{\partial X^T\beta}{\partial X} = \beta \\<br/>
&amp;\frac{\partial X^TAX}{\partial X} = (A+A^T)X \\<br/>
\end{align*}<br/>
\]</p>
</blockquote>

<p>因为：<br/>
\[<br/>
S_B^T = （\sum_{i=1}^kN_i(u_i-u)(u_i-u)^T)^T = \sum_{i=1}^kN_i(u_i-u)(u_i-u)^T = S_B \\<br/>
S_W^T = (\sum_{i=1}^k \sum_{x\in X_i}(x-u_i)(x-u_i)^T)^T = \sum_{i=1}^k \sum_{x\in X_i}(x-u_i)(x-u_i)^T = S_W <br/>
\]</p>

<p>上式带入公式（\ref{LW}）可得：<br/>
\[<br/>
\frac{\nabla L(W,\alpha)}{\nabla W} = -2S_BW + 2\lambda S_WW = 0<br/>
\]</p>

<p>如果\(S_W\)为非奇异，即\(S_W^{-1}\)存在，上式可得：\(S_W^{-1}S_BW = \lambda W\)，也就是\(W\)的每一列都是\(S_W^{-1}S_B\)的特征向量。特征值越大的特征向量包含的信息越丰富，所以首先求出\(S_W^{-1}S_B\)的特征值，\(W\)为前 \(d\) 个非零特征值对应的特征向量组成的特征矩阵。</p>

<h4 id="toc_5">多分类的维度问题</h4>

<p>多分类中我们假设经过\(W\)投影后的数据集在 \(d\) 维空间，那么接下来将讨论 \(d\) 的范围。由于 \(S_B\) 矩阵中的 \(\mu_i-\mu\) 的秩为1，结合（\ref{S_B}）和秩的性质6（见附录：矩阵的秩小于等于各个相加矩阵的秩的和），因此SB的秩最多为k。由公式（\ref{u_u_i}）知\(\mu_k\) 可以用前k-1个 \(\mu_i\) 表示出来，因此 \(S_B\) 的秩最多为k-1，由秩的性质7可知\(R(S_W^{-1}S_B) \le min\{R(S_W^{-1}),R(S_B)\}\) ，所以\(S_W^{-1}S_B\)的秩最大为k-1。</p>

<p>由矩阵的秩与非零特征值的关系：对于任何n阶方阵，都有\(\mu (A) \le R(A)\)，即非零特征值的个数小于矩阵的秩。具体证明见文章最后。所以可以得出\(S_W^{-1}S_B\)非零特征值个数最大为k-1个，所以对应的特征向量最多也为k-1个，即\(W\)的列数最多有\(k-1\)列，那么 \(d\) 也最大为k-1维。</p>

<h2 id="toc_6">LDA算法流程</h2>

<p>输入：同样我们假设数据集\(D=((x_0,y_0),(x_1,y_1),...,(x_m,y_m))\)，其中任意的\(x_i\)是一个\(n\)维向量；\(y_i\in (C_1,C_2,...,C_k)\)，也就是数据集包含\(k\)类数据；<br/>
输出：降维后的样本集\(D′\)。</p>

<p>1） 计算类内散度矩阵\(S_W\)<br/>
2） 计算类间散度矩阵\(S_B\)<br/>
3） 计算矩阵\(S_W^{-1}S_B\)<br/>
4） 计算\(S_W^{-1}S_B\)的最大的 \(d\) 个特征值和对应的 \(d\) 个特征向量\((w_1,w_2,...,w_d)\),得到投影矩阵\(W\)<br/>
5） 对样本集中的每一个样本特征\(x_i\)，转化为新的样本\(z_i=W^Tx_i\)<br/>
6） 得到输出样本集\(D′={(z_1,y_1),(z_2,y_2),...,((z_m,y_m))}\)</p>

<h2 id="toc_7">LDA的局限性</h2>

<ol>
<li><p>存在秩限制，对于 \(k\) 类问题，至多可生成 \(k-1\) 维子空间。LDA降维后的维度区间在 \([1,k-1]\) ，与原始特征数 \(m\) 无关，对于二值分类，最多投影到1维。</p></li>
<li><p>类间散度矩阵必须非奇异。在一些小样本数据集中，样本总数可能小于样本维数，此时\(S_W\)奇异，将不能使用上述LDA算法。</p></li>
</ol>

<h2 id="toc_8">LDA 改进算法</h2>

<h4 id="toc_9">PCA + LDA</h4>

<p>在一些人脸识别等小样本问题上，需要面对的一个小问题是类内散度奇异，这是由于训练图像的个数 N 远小于每一个图像的维度。Belhomecour 等人提出一个解决方案，先做一次PCA降维，解决\(S_W\)的奇异问题，然后再应用LDA将训练集将维到\(k-1\)维度。</p>

<h6 id="toc_10">还有一些其他的改进算法 MLDA 等，相关文献较少，以后需要时再看吧</h6>

<h2 id="toc_11">附录：</h2>

<h4 id="toc_12">秩的性质：</h4>

<p>1） \(0 \le R(A_{m\times n}) \le min\{m,n\}\)<br/>
2） \(R(A^T) = R(A)\)<br/>
3） 若A~B，则\(R(A) = R(B)\)<br/>
4） 若P,Q可逆，则\(R(PAQ) = R(A)\)<br/>
5） \(max\{R(A),R(B)\} \le R(A,B) \le R(A)+R(B)\)<br/>
6） \(R(A+B) \le R(A)+R(B)\)<br/>
7） \(R(AB) \le min\{R(A),R(B)\}\)<br/>
8） 若\(A_{m\times n}B_{n\times l} = O\)，则\(R(A) + R(B) \le n\)</p>

<h4 id="toc_13">矩阵的秩与非零特征值的关系</h4>

<p>论文看<a href="http://xueshu.baidu.com/s?wd=paperuri%3A%28f0a2034422bc0816eb555fa33cb95b1f%29&amp;filter=sc_long_sign&amp;tn=SE_xueshusource_2kduw22v&amp;sc_vurl=http%3A%2F%2Fwww.doc88.com%2Fp-317741888504.html&amp;ie=utf-8&amp;sc_us=4282568566058718085">这里</a>(Chrome打开)</p>

<h6 id="toc_14">定理：对于任意 \(n\) 阶方阵，都有 \(\mu (A) \le r(A)\)，即矩阵的非零特征值的个数不大于矩阵的秩。</h6>

<p>证明：<br/>
当\(|A|\neq 0\)时，此时\(A\)可逆，并有\(r(A) = n\)。假设\(\lambda_1,\lambda_2,...,\lambda_n\)是方阵\(A\)的全部特征值，那么\(|A| = \lambda_1\lambda_2...\lambda_n\neq 0\)，所以\(\lambda_i\neq 0(i=1,2,...,n)\)，所以\(\mu(A) = r(A)\)。</p>

<p>当\(|A| = 0\)时，设\(r(A) = r &lt; n\)，对于特征值\(\lambda\)满足：<br/>
\[<br/>
|\lambda I-A| = \left ( \begin{array}{cccccc} <br/>
\lambda-a_{11}&amp;-a_{12}&amp;...&amp;-a_{1r}&amp;...&amp;-a_{1n}\\<br/>
-a_{21}&amp;\lambda-a_{22}&amp;...&amp;-a_{2r}&amp;...&amp;-a_{2n}\\<br/>
...&amp;...&amp;...&amp;...&amp;...&amp;...\\<br/>
-a_{r1}&amp;-a_{r2}&amp;...&amp;\lambda-a_{rr}&amp;...&amp;-a_{rn}\\<br/>
-a_{r+1,1}&amp;-a_{r+1,2}&amp;...&amp;\lambda-a_{r+1,r}&amp;...&amp;-a_{r+1,n}\\<br/>
...&amp;...&amp;...&amp;...&amp;...&amp;...\\<br/>
-a_{n1}&amp;-a_{n2}&amp;...&amp;-a_{nr}&amp;...&amp;\lambda-a_{nn}<br/>
\end{array} \right )<br/>
\]</p>

<p>考虑矩阵的行向量组\((a_1,a_2,...,a_n)^T\)，这里不妨设\(a_1,a_2,...,a_r\)为线形无关的 \(r\) 个向量。则\(a_{r+1},a_{r+2},...,a_n\)均可由\(a_1,a_2,...,a_r\)线形表示，设\(a_i=k_{i1}a_1+k_{i2}a_2+...+k_{ir}a_r\)，其中\(i=r+1,r+2,...,n\)，我们现在只看第 \(r+1\) 行的每一个元素，用前 \(r\) 行表示得：<br/>
\[<br/>
\begin{align*}<br/>
a_{r+1,1}&amp;=k_{r+1,1}a_{11}+k_{r+1,2}a_{21}+...+k_{r+1,r}a_{r1}\\<br/>
a_{r+1,2}&amp;=k_{r+1,1}a_{12}+k_{r+1,2}a_{22}+...+k_{r+1,r}a_{r2}\\<br/>
&amp;...\\<br/>
a_{r+1,n}&amp;=k_{r+1,1}a_{1n}+k_{r+1,2}a_{2n}+...+k_{r+1,r}a_{rr}<br/>
\end{align*}<br/>
\]</p>

<p>现在对\(|\lambda I-A|\)的第\((r+1)\)行做行列式变换，分别用\(k_{r+1,j}\)乘上\(|\lambda I-A|\)的第 \(j\)(\(j=1,2,...,r\)) 行后加上它，我们知道行列式变换结果不变，分开计算每一个元素，先看第一项：</p>

<p>\[<br/>
\begin{align*}<br/>
|\lambda I-A|_{r+1,1} &amp;\sim |\lambda I-A|_{r+1,1} + k_{r+1,1}(\lambda-a_{11}) + k_{r+1,2}a_{21}+...+k_{r+1,r}a_{r1} \\<br/>
&amp;= -a_{r+1,1} + k_{r+1,1}(\lambda-a_{11}) + k_{r+1,2}a_{21}+...+k_{r+1,r}a_{r1} \\<br/>
\end{align*}<br/>
\]</p>

<p>将之前\(a_{r+1,1}\)的线形表示形式带入上式得：</p>

<p>\[<br/>
\begin{align*}<br/>
|\lambda I-A|_{r+1,1} &amp;\sim -a_{r+1,1} + k_{r+1,1}(\lambda-a_{11}) + k_{r+1,2}a_{21}+...+k_{r+1,r}a_{r1}\\<br/>
&amp;= (-k_{r+1,1}a_{11}-k_{r+1,2}a_{21}-...k_{r+1,r}a_{r1}) + k_{r+1,1}(\lambda-a_{11}) + k_{r+1,2}a_{21}+...+k_{r+1,r}a_{r1} = k_{r+1,1}\lambda<br/>
\end{align*}<br/>
\]</p>

<p>同理可得其他项，按照这种方式最终可得：<br/>
\[<br/>
\begin{align*}<br/>
|\lambda I-A| &amp;\sim \left ( \begin{array}{cccccc} <br/>
\lambda-a_{11}&amp;-a_{12}&amp;...&amp;-a_{1r}&amp;-a_{1,r+1}&amp;...&amp;-a_{1n}\\<br/>
-a_{21}&amp;\lambda-a_{22}&amp;...&amp;-a_{2r}&amp;-a_{2,r+1}&amp;...&amp;-a_{2n}\\<br/>
...&amp;...&amp;...&amp;...&amp;...&amp;..&amp;...\\<br/>
-a_{r1}&amp;-a_{r2}&amp;...&amp;\lambda-a_{rr}&amp;a_{r,r+1}&amp;...&amp;-a_{rn}\\<br/>
k_{r+1,1}\lambda&amp;k_{r+1,2}\lambda&amp;...&amp;k_{r+1,r}\lambda&amp;\lambda&amp;...&amp;0\\<br/>
...&amp;...&amp;...&amp;...&amp;...&amp;...&amp;...\\<br/>
k_{n,1}\lambda&amp;k_{n,2}\lambda&amp;...&amp;k_{n,r}\lambda&amp;0&amp;...&amp;\lambda<br/>
\end{array} \right ) \\<br/>
&amp;= \lambda^{n-r}\left ( \begin{array}{cccccc} <br/>
\lambda-a_{11}&amp;-a_{12}&amp;...&amp;-a_{1r}&amp;-a_{1,r+1}&amp;...&amp;-a_{1n}\\<br/>
-a_{21}&amp;\lambda-a_{22}&amp;...&amp;-a_{2r}&amp;-a_{2,r+1}&amp;...&amp;-a_{2n}\\<br/>
...&amp;...&amp;...&amp;...&amp;...&amp;..&amp;...\\<br/>
-a_{r1}&amp;-a_{r2}&amp;...&amp;\lambda-a_{rr}&amp;a_{r,r+1}&amp;...&amp;-a_{rn}\\<br/>
k_{r+1,1}&amp;k_{r+1,2}&amp;...&amp;k_{r+1,r}&amp;1&amp;...&amp;0\\<br/>
...&amp;...&amp;...&amp;...&amp;...&amp;...&amp;...\\<br/>
k_{n,1}&amp;k_{n,2}&amp;...&amp;k_{n,r}&amp;0&amp;...&amp;1<br/>
\end{array} \right ) \\<br/>
\end{align*}<br/>
\]</p>

<p>由此可见至少\(|\lambda I-A| = 0\)有\((n-r)\)个零特征值，也就是它最多拥有\(n-(n-r)=r\)个非零特征值。结合\(r(A) = r\)，所以\(u(A) \le r(A)\)。<br/>
得证。</p>

<h6 id="toc_15">其他关于矩阵秩的论文：<a href="https://wenku.baidu.com/view/f0deb94a8762caaedc33d426.html">方阵的秩与特征值的关系</a></h6>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15037711738928.html">奇异值分解 SVD</a></h1>
			<p class="meta"><time datetime="2017-08-27T02:12:53+08:00" 
			pubdate data-updated="true">2017/8/27</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>SVD，英文全称是Singular Values Decomposition，关于奇异值的解释，参考一下百度百科里的介绍，设\(A\)是一个m*n的矩阵，那么\(AA^T\)的q个特征值得平方根叫做\(A\)的特征值。在PCA的介绍中，我们提到了特征值分解的方式来实现提取特征，特征值分解要求必须是方阵。而奇异值分解的目的也是提取特征值，但是针对的是任意矩阵。SVD除此之外还可以用来做图片压缩，在推荐系统中的应用也是名声大噪。</p>

<h2 id="toc_0">前置知识</h2>

<p>在学习SVD之前需要一些线性代数的知识，下面做一些简单介绍。</p>

<h4 id="toc_1">正交矩阵</h4>

<p>在线形代数里，若矩阵\(A\)满足\(AA^T=I\)或\(A^TA=I\)，其中\(E\)为单位矩阵，那么可以说\(n\)阶实矩阵为正交矩阵。<br/>
\[<br/>
\begin{align*}<br/>
(AA^T)_{ij} = \sum_{k=0}^N a_{ik} a_{kj} = I_{ij}<br/>
\end{align*}<br/>
\]</p>

<p>由单位矩阵的性质可知主对角线上的元素不为0，其他元素都为0，可知\(A\)的各行两两相交，且\(||A^T||=1\)，同理\(A\)的列向量两两正交且\(||A||=1\)。</p>

<p>一个正交矩阵对应的变换叫做正交变换，这个变换有一些特点。假设我们在空间里有一个点\(X=(2,4)^T\)，那么现在我们用一个正交矩阵\(A=\left [ \begin{array}{cc}\frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} \end{array} \right ]\)来对其做正交变换，所以得到的新的坐标点的位置为\(X^{&#39;}=A^TX=(3\sqrt{2},\sqrt{2})^T\)，我们将得到的新的坐标点在新的正交基构成的坐标系中表示出来，如下图：</p>

<div align="center">
    <img src="media/15037711738928/15273990942816.jpg" width="500px" />
</div>

<p>由上图可知，坐标点的\(X\)的位置其实并没有发生改变，只是在相对应不同的坐标系位置发生了改变，实际上也就是只是将坐标系旋转了。</p>

<p>正交矩阵其实是一种特殊的每一个元素都是实数的酉矩阵，一个\(n\times n\)的复数矩阵\(U\)，满足：<br/>
\[<br/>
U^{H}U = UU^{H} = I<br/>
\]<br/>
其中\(U^{H}\)是\(U\)的共轭转置矩阵，那么\(U\)则被称为酉矩阵。</p>

<h2 id="toc_2">SVD介绍</h2>

<p>设\(X\)是一个\(m*n\)的矩阵，则存在\(m\)阶正交矩阵\(U\)和\(n\)阶正交矩阵\(V\)，在复数领域就是酉矩阵，满足：<br/>
\[<br/>
X = U\Sigma V^T = U\left [ \begin{array}{cccc}\sigma_1&amp;&amp;&amp;\\&amp;\sigma_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\sigma_r\\&amp;&amp;&amp;0\\\end{array}\right ]V<br/>
\]</p>

<p>其中\(\sigma_1&gt;\sigma_2&gt;\sigma_r\)被称为奇异值，\(U\)和\(V\)的前r列向量被称为奇异向量。一般前10%甚至1%的奇异值，将能代表超过99%的信息量。关于奇异值和奇异向量的求解就是SVD分解。</p>

<h2 id="toc_3">SVD分解</h2>

<p>在PCA中操作往往是对数据集\(X\)的协方差矩阵\((X-\overline{X})(X-\overline{X})^T\)进行提取特征值，但是特征值提取只能是方阵，那么对于非方阵提取特征值便可以通过SVD的方式。奇异值分解是一种能分解任意矩阵的方法，对于\(m\times n\)的矩阵\(X\)有：<br/>
\[<br/>
X = U\Sigma V^T<br/>
\]</p>

<p>其中\(\Sigma\)是一个\(m\times n\)的矩阵，除主对角线的元素外都为0，主对角线的元素则是奇异值，按照从大到小排列。\(U\)是一个\(m\times m\)的方阵，称为\(X\)的左奇异向量，\(V\)是一个\(n \times n\)的方阵，称为\(X\)的右奇异向量，它们都是正交矩阵，满足\(UU^T=I,VV^T=I\)。</p>

<p>现在开始讨论\(U\)和\(V\)和\(\Sigma\)的求解：<br/>
\[<br/>
X = U\Sigma V^T \Rightarrow X^T = V\Sigma^TU^T \Rightarrow XX^T = U\Sigma V^TV\Sigma^TU^T=U(\Sigma\Sigma^T)U^T<br/>
\]</p>

<p>因为\((XX^T)^T = XX^T\)，所以\(XX^T\)是一个对称向量，由对称向量的性质，\(n\)阶对称向量，必有正交矩阵\(P\)，满足\(A = P\Lambda P^T\)，所以结合上式可以认为左奇异向量\(U\)是对称矩阵\(XX^T\)的特征向量。同理：<br/>
\[<br/>
X^TX = V\Sigma^TU^TU\Sigma V^T = V(\Sigma^T\Sigma) V^T<br/>
\]</p>

<p>可知右奇异向量\(V\)可看做对称矩阵\(X^TX\)的特征向量。同时我们也能将\(\Sigma^T\Sigma\)看做对称矩阵\(XX^T\)或\(X^TX\)的特征值组成的矩阵，也就是奇异值可以通过\(X^TX\)或\(XX^T\)特征值取平方根来求得。</p>

<p>这里说奇异值可以通过\(X^TX\)或\(XX^T\)特征值取平方根来求得，那就必然存在两个必然条件：<br/>
(1) \(X^TX\)或\(XX^T\)的特征值必须是非负数<br/>
(2) \(A^TA\)和\(AA^T\)的非零特征值必须相等（为了说明清楚，换个表示）</p>

<p>证明（1）：假设矩阵的\(X^TX\)的特征值\(\sigma\)对应的特征向量是\(v\)，有正交矩阵的性质可知\(v^Tv = 1\)：<br/>
\[<br/>
X^TX v = \sigma v \Rightarrow \sigma=v^TX^TX v = ||Xv||^2<br/>
\]</p>

<p>所以得证\(\sigma\ge 0\)，同理可知\(XX^T\)的特征向量必是非负数。</p>

<p>证明（2）：首先证明向量\(A^TA\)与\(AA^T\)有相等个非零特征值：<br/>
设矩阵\(A\)是\(m\times n\)的矩阵，由矩阵秩的性质可知\(r(A)=r(A^T)\)。对于任意的向量\(x\)，如果\(Ax = 0\)则 \(A^TAx = A^T0 = 0\)，即\(Ax=0\)的解空间是\(A^Ax=0\)的解空间。如果\(A^TAx = 0\)时，两边同左乘\(x^T\)得\(x^TA^TAx = (Ax)^TAx= 0 \)，也就是\(||Ax||=0\)，所以\(Ax=0\)，即\(A^TAx=0\)的解空间是\(Ax=0\)的解空间。结合前面知：\(Ax=0\)与\(A^TAx=0\)拥有相同的解空间，所以\(r(A)=r(A^TA)\)，同理可证：\(r(A^T)=r(AA^T)\)。</p>

<p>设\(r(A) = r\)，所以\(r=r(A)=r(A^TA)\)=\(r(A^T)=r(AA^T)\)，因为\(AA^T\)是实对称向量，并由秩的定义可知\(AA^T\)化成行列式后有\(r\)个非零行，即\(AA^T\sim\left[\begin{array}\\\sigma_1&amp;&amp;&amp;\\&amp;\ddots&amp;&amp;\\&amp;&amp;\sigma_r&amp;\\&amp;&amp;&amp;0\\\end{array}\right]\)，其中\(\sigma_n\)是\(AA^T\)的特征值，\(r=AA^T\)非零特征值得个数。同理\(r=A^TA\)非零特征值得个数。</p>

<p>再证明\(A^TA\)的非零特征值也是\(AA^T\)的特征值：<br/>
设\(\sigma\)是\(A^TA\)的特征值，所以有\(A^TAv=\sigma v\)，两边同时左乘\(A\)得：\(AA^T(Av) = \sigma Av\)，这可以看成\(\sigma\)是\(AA^T\)的特征值，其中对应的特征向量是\(Av\)，所以\(A^TA\)的特征值也是\(AA^T\)的特征值。反之也可证明\(AA^T\)的特征值也是\(A^TA\)的特征值。</p>

<p>综上（1）（2）可知\(A^TA\)与\(AA^A\)特征值的区别就在零特征值的数量。</p>

<p>那么我们在求出\(X^TX\)或\(XX^T\)任意一个特征值后，可以利用左右奇异向量的关系，在求出其中一个的前提下，求出另一个，而不用再通过特征值得方式计算，假设\(\sigma_i\)为\(X\)的奇异值：<br/>
\[<br/>
X = U\Sigma V^T \Rightarrow AV = U\Sigma \Rightarrow Xv_i = \sigma_iu_i\\\Rightarrow u_i = Xv_i/\sigma_i<br/>
\]</p>

<p>因为特征值与奇异值的关系，可知\(X^TX\)的特征值为\(\sigma_i^2\)，假设\(X^TX\)对应的特征向量为\(v_i\)，则：<br/>
\[<br/>
\begin{align*}<br/>
&amp;X^TX v_i = \sigma_i^2 v_i \\<br/>
&amp;\Rightarrow v_i^T X^TXv_i = \sigma_i^2 v_i^Tv_i \\<br/>
&amp;\Rightarrow \sigma_i^2 = ||Xv_i||^2 \\<br/>
\end{align*}<br/>
\]</p>

<p>注意到：<br/>
\[<br/>
XX^Tu_i = XX^T\frac{Xv_i}{\sigma_i} = X\frac{X^TXv_i}{\sigma_i}=X\frac{\sigma_i^2 v_i}{\sigma_i} = \sigma_i^2\frac{Xv_i}{\sigma_i} = \sigma_i^2p_i<br/>
\]</p>

<p>所以\(u_i\)是\(XX^T\)的特征向量，其实这个在上面已经知道了。</p>

<p>且有：<br/>
\[<br/>
u_i^Tu_j = (\frac{Xv_i}{\sigma_i})^T\frac{Xv_j}{\sigma_j} = \frac{(Xv_i)^T}{\sigma_i}\frac{Xv_j}{\sigma_j} = \frac{v_i^TX^TXv_j}{\sigma_i\sigma_j} = \frac{v_i^T\sigma_j^2v_j}{\sigma_i\sigma_j}=\frac{\sigma_j}{\sigma_i}v_i^Tv_j<br/>
\]</p>

<p>令\(\delta_{ij}=v_i^Tv_j\)，则：<br/>
\[<br/>
u_i^Tu_j=\frac{\sigma_j}{\sigma_i}\delta_{ij}<br/>
\]</p>

<p>当特征向量\(v_i\)是一组正交单位特征向量时：<br/>
当\(i \neq j\)时\(\delta_{ij}=v_iv_j=0\)，此时\(u_i^Tu_j=\frac{\sigma_j}{\sigma_i}\delta_{ij}=0=\delta_{ij}\)；<br/>
当\(i = j\)时\(\sigma_i=\sigma_j\)，此时\(u_i^Tu_j=\frac{\sigma_j}{\sigma_i}\delta_{ij}=\delta_{ij}\)，<br/>
所以\(u_i\)也是一组正交单位特征向量。</p>

<h4 id="toc_4">SVD分解实例</h4>

<p>求解\(A = \left ( \begin{array}{cc} 1 &amp; 0\\ 1&amp;1\\ 0&amp;0\\0&amp;1\\ \end{array} \right)\)<br/>
首先计算\(AA^T=\left ( \begin{array}{cc} 1 &amp; 0\\ 1&amp;1\\ 0&amp;0\\0&amp;1\\ \end{array} \right)\cdot\left ( \begin{array}{cc}  1&amp;1&amp;0&amp;0\\ 0&amp;1&amp;0&amp;1\\ \end{array} \right) = \left(\begin{array}{cc}1 &amp; 1&amp;0&amp;0\\1&amp;2&amp;0&amp;1\\0&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;1\\\end{array}\right)\) ，然后计算\(A^TA\)的特征向量就是左奇异矩阵\(U\)。<br/>
\[<br/>
|AA^T-\lambda I| = \left | \begin{array}{cccc}1-\lambda &amp;1&amp;0&amp;0 \\ 1&amp;2-\lambda &amp;0&amp;1 \\0&amp;0&amp; -\lambda&amp;0 \\0&amp;1&amp;0&amp;1-\lambda \\\end{array}\right | = -\lambda\left | \begin{array}{cc}1-\lambda&amp;1&amp;0\\1&amp;2-\lambda&amp;1\\0&amp;1&amp;1-\lambda\end{array}\right|=0\\<br/>
\Rightarrow -\lambda[(1-\lambda)^2(2-\lambda)-2(1-\lambda)] =0<br/>
\]</p>

<p>可以求出\(\lambda_1=3\,,\lambda_2=1\,,\lambda_3=\lambda_4=0\)。<br/>
当\(\lambda_1=3\)时：<br/>
\[<br/>
(AA^T-\lambda I)\overrightarrow{\nu} = \left(\begin{array}{cccc}-2&amp;1&amp;0&amp;0\\1&amp;-1&amp;0&amp;1\\0&amp;0&amp;-3&amp;0\\0&amp;1&amp;0&amp;-2\\\end{array}\right)\left(\begin{array}{c}x_1\\x_2\\x_3\\x_4\\\end{array}\right) \\<br/>
\Rightarrow \left \{ \begin{array}\\-2x_1+x_2=0\\x_1-x_2+x_4=0\\-3x_3=0\\x_2-2x_4=0\\\end{array}\right .<br/>
\]</p>

<p>可以求得其中一个解为\(x1=1\,,x2=2\,,x_3=0\,,x4=1\)，归一化后得：<br/>
\[<br/>
\overrightarrow{\nu} = \left ( \begin{array}{c}<br/>
\frac{1}{\sqrt{6}}\\\frac{2}{\sqrt{6}}\\0\\\frac{1}{\sqrt{6}}\\\end{array}\right)<br/>
\]</p>

<p>同样的方式，可以求出特征矩阵为：<br/>
\[<br/>
V = \left ( \begin{array}{c}<br/>
\frac{1}{\sqrt{6}}&amp;\frac{\sqrt{2}}{2}&amp;\frac{1}{2}&amp;-\frac{1}{2}\\<br/>
\frac{2}{\sqrt{6}}&amp;0&amp;-\frac{1}{2}&amp;\frac{1}{2}\\<br/>
0&amp;0&amp;-\frac{1}{2}&amp;\frac{1}{2}\\<br/>
\frac{1}{\sqrt{6}}&amp;-\frac{\sqrt{2}}{2}&amp;\frac{1}{2}&amp;-\frac{1}{2}\\<br/>
\end{array}\right)<br/>
\]</p>

<p>我们知道这就是左奇异矩阵\(U\)，同理再计算\(A^TA\)的特征值\(\lambda_1=3\,,\lambda_2=1\)，代入求得右奇异矩阵\(V\)：<br/>
\[<br/>
V=\left ( \begin{array}{cc}<br/>
\frac{\sqrt{2}}{2}&amp;\frac{\sqrt{2}}{2}\\\frac{\sqrt{2}}{2}&amp;-\frac{\sqrt{2}}{2}\\<br/>
\end{array} \right )<br/>
\]</p>

<p>前面我们说过\(\Sigma\)主对角线的元素，也就是奇异值，是特征值得平方根，所以可求得：<br/>
\[<br/>
\Sigma = \left ( \begin{array}{cc}<br/>
\sqrt{3}&amp;0\\0&amp;1\\0&amp;0\\0&amp;0\\<br/>
\end{array} \right )<br/>
\]</p>

<p>所以矩阵\(A\)被分解为了3个矩阵的乘积：<br/>
\[<br/>
A = \left ( \begin{array}{c}<br/>
\frac{1}{\sqrt{6}}&amp;\frac{\sqrt{2}}{2}&amp;\frac{1}{2}&amp;-\frac{1}{2}\\<br/>
\frac{2}{\sqrt{6}}&amp;0&amp;-\frac{1}{2}&amp;\frac{1}{2}\\<br/>
0&amp;0&amp;-\frac{1}{2}&amp;\frac{1}{2}\\<br/>
\frac{1}{\sqrt{6}}&amp;-\frac{\sqrt{2}}{2}&amp;\frac{1}{2}&amp;-\frac{1}{2}\\<br/>
\end{array}\right) \cdot \left ( \begin{array}{cc}<br/>
\sqrt{3}&amp;0\\0&amp;1\\0&amp;0\\0&amp;0\\<br/>
\end{array} \right ) \cdot \left( \begin{array}{cc}<br/>
\frac{\sqrt{2}}{2}&amp;\frac{\sqrt{2}}{2}\\\frac{\sqrt{2}}{2}&amp;-\frac{\sqrt{2}}{2}\\<br/>
\end{array} \right )<br/>
\]</p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15034227314732.html">主成分分析 PCA</a></h1>
			<p class="meta"><time datetime="2017-08-23T01:25:31+08:00" 
			pubdate data-updated="true">2017/8/23</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>在介绍主成分分析之前，简单看一下方差、协方差，特征值，特征向量的概念，了解这些后才能更好了解主成分分析。</p>

<h2 id="toc_0">代数基础知识</h2>

<h4 id="toc_1">方差 variance</h4>

<p>在统计学中，方差被描述为样本与总体平均数之间的差异，假设样本数据为\(X_i\,:\,i=1;2;...;N\)，定义：<br/>
\[<br/>
\mu = \frac{\sum_{i=1}^N X_i}{N}<br/>
\]</p>

<p>为总体平均数，对于总体的方差公式为（\(\sigma^2\)代表方差，\(\sigma\)代表标准差）：<br/>
\[<br/>
\sigma^2 = \frac{\sum_{i=1}^N (X_i - \mu)^2}{N}<br/>
\]</p>

<p>假设样本均值为\(\overline{X}\)，样本方差定义为为：<br/>
\[<br/>
S^2 = \frac{\sum_{i=1}^N(x_i-\overline{X})^2}{n-1}<br/>
\]</p>

<p>这里的n为样本x的数量，这里和总体方差不同的是除以的是n-1，而不是n，这是由于除以n-1是无偏方差，如果除以n会比总体方差要小。</p>

<blockquote>
<h4 id="toc_2">无偏方差为什么除以n-1？</h4>

<p>假设数据集X上，有多次抽样\(X_1\)、\(X_2\)、...、\(X_n\)，每一个样本的平均值即为\(\overline{X_1}\)、\(\overline{X_2}\)、...、\(\overline{X_n}\)，随着抽样的次数逐渐增多，样本平均值的平均值将会越来越近总体平均值，即\(E(\overline{X}) = \mu\)，此时也就是无偏估计。</p>

<p>首先我们来假设无偏方差是如下形式：<br/>
\[<br/>
S^2 = \frac{1}{n}\sum_{i=1}^N(x_i-\overline X)^2<br/>
\]<br/>
所以求无偏方差的期望为：<br/>
\[<br/>
\begin{align*}<br/>
E[S^2] &amp;= E[\frac{1}{n} \sum_{i=1}^N(x_i-\overline X)^2] \\<br/>
&amp;= E[\frac{1}{n} \sum_{i=1}^N (x_i - \mu + \mu - \overline X)^2] \\<br/>
&amp;= E[\frac{1}{n} \sum_{i=1}^N [(x_i - \mu)^2 + 2(x_i - \mu)(\mu - \overline{X}) + (\mu - \overline{X})^2]] \\<br/>
&amp;= E[\frac{1}{n} \sum_{i=1}^N(x_i-\mu)^2 + \frac{1}{n}\sum_{i=1}^N 2(x_i - \mu)(\mu-\overline{X})+\frac{1}{n}\sum_{i=1}^N(\mu-\overline{X})^2] \\<br/>
&amp;\because\quad\frac{1}{n}\sum_{i=1}^N(x_i-\mu) = \frac{1}{n}\sum_{i=1}^Nx_i-\frac{1}{n}\sum_{i=1}^N\mu = \overline{X} - \mu \\<br/>
E[S^2]&amp;= E[\frac{1}{n}\sum_{i=1}^N(x_i-\mu)^2 + 2(\overline{X}-\mu)(\mu - \overline{X}) + (\mu-\overline{X})^2] \\<br/>
&amp;=E[\frac{1}{n}\sum_{i=1}^N(x_i-\mu)^2 - (\overline{X} - \mu)^2] \\<br/>
&amp;=E[\frac{1}{n}\sum_{i=1}^N(x_i-\mu)^2] - E[(\overline{X}-\mu)^2] \\<br/>
&amp;=\sigma^2-E[(\overline{X} - \mu)^2]<br/>
\end{align*} <br/>
\]</p>

<p>并有：<br/>
\[<br/>
\begin{align*}<br/>
E[(\overline{X}-\mu)^2] &amp;= E[(\overline{X}-E(\overline{X}))^2] = var(\overline{X})\\<br/>
&amp;= var(\frac{\sum_{i=1}^n X_i}{n}) \\<br/>
&amp;= \frac{1}{n^2}var(\sum_{i=1}^n X_i) \\<br/>
&amp;= \frac{1}{n^2}\sum_{i=1}^N var(X_i) \\<br/>
&amp;= \frac{1}{n}\sigma^2<br/>
\end{align*}<br/>
\]<br/>
所以：<br/>
\[<br/>
E(S^2) = \sigma^2 - E[(\overline{X} - \mu)^2] = \sigma^2-\frac{\sigma^2}{n} = \frac{n-1}{n}\sigma^2<br/>
\]<br/>
由证明其实可知我们假设的无偏方差其实是有偏差的，它比总体方差要小，此时如果我们想要得到无偏方差\(\sigma^2\)，我们希望\(S^2\)乘上\(\frac{n}{n-1}\)，得到：<br/>
\[<br/>
S^2 = \frac{n}{n-1}(\frac{1}{n} \sum_{i=1}^N(x_i-\overline X)^2) = \frac{1}{n-1}\sum_{i=1}^N(x_i-\overline X)^2<br/>
\]<br/>
证明：<br/>
\[<br/>
\begin{align*}<br/>
E[S^2] &amp;= E[\frac{1}{n-1}\sum_{i=1}^N (x_i - \mu + \mu - \overline{X})^2] \\<br/>
&amp;= E[\frac{1}{n-1}\sum_{i=1}^N (x_i-\mu)^2+\frac{1}{n-1}\sum_{i=1}^N2(x_i-\mu)(\mu-\overline{X}) + \frac{1}{n-1}\sum_{i=1}^N (\mu-\overline{X})^2] \\<br/>
&amp;= E[\frac{1}{n-1}\sum_{i=1}^N (x_i-\mu)^2 - \frac{2n}{n-1}(\mu-\overline{X})^2 + \frac{n}{n-1}(\mu-\overline{X})^2] \\<br/>
&amp;= E[\frac{1}{n-1}\sum_{i=1}^N (x_i-\mu)^2 - \frac{n}{n-1}(\mu-\overline{X})^2] \\<br/>
&amp;=E[\frac{1}{n-1}\sum_{i=1}^N (x_i-\mu)^2] - E[\frac{n}{n-1}(\mu-\overline{X})^2] \\<br/>
&amp;=\frac{n}{n-1}E[\frac{1}{n}\sum_{i=1}^N (x_i-\mu)^2] - \frac{n}{n-1}E[(\mu-\overline{X})^2] \\<br/>
&amp;=\frac{n}{n-1}\sigma^2-\frac{n}{n-1} \cdot\frac{\sigma^2}{n} \\<br/>
&amp;= \sigma^2<br/>
\end{align*}<br/>
\]<br/>
得证。无偏方差为：<br/>
\[<br/>
S^2 = \frac{1}{n-1}\sum_{i=1}^N(x_i-\overline X)^2<br/>
\]</p>
</blockquote>

<h4 id="toc_3">协方差 Covariance</h4>

<p>由前面对方差的定义可知，方差是衡量数据集中数据与平均值的偏离程度，方差处理的是一维数据的情况，而在现实生活中我们处理的多是多维数据，描述的往往是多个数据同时偏离平均值的程度。我们可以仿照方差的形式：<br/>
\[<br/>
var(X) = \frac{1}{n-1}\sum_{i=1}^N (X_i - \overline{X}) = \frac{1}{n-1} \sum_{i=1}^N (X_i - \overline{X})(X_i-\overline{X})<br/>
\]<br/>
定义协方差：<br/>
\[<br/>
cov(X,Y) = \frac{1}{n-1}(X_i - \overline{X})(Y_i - \overline{Y})<br/>
\]</p>

<p>通常我们可以用偏方差来描述两个维度的数据的相关性，偏方差为正数表示两者是正相关的，反之为负数表示两者负相关，如果为0，表示两者互相独立。由协方差的定义容易知：<br/>
\[<br/>
var(X) = cov(X,X)<br/>
cov(X,Y) = cov(Y,X)<br/>
\]</p>

<p>由定义可知，协方差每次只可以描述两个维度数据，如果我们有n维数据，那么往往就需要计算\(\frac{1}{2}n(n-1)\)个协方差，于是便引入了协方差矩阵。一个三维的协方差矩阵如下所示：<br/>
\[<br/>
C = \left ( \begin{array}{ccc}<br/>
cov(x,x) &amp;&amp; cov(x,y) &amp;&amp; cov(x,z) \\<br/>
cov(y,x) &amp;&amp; cov(x,x) &amp;&amp; cov(y,z) \\<br/>
cov(z,x) &amp;&amp; cov(z,y) &amp;&amp; cov(z,z) \\<br/>
\end{array} \right)<br/>
\]</p>

<blockquote>
<p>散度矩阵：协方差矩阵的计算有除以\((n-1)\)的步骤，如果没有没有这个步骤获得的矩阵即是散度矩阵。换言之，散度矩阵等于协方差矩阵乘以\((n-1)\)。</p>
</blockquote>

<h4 id="toc_4">特征值和特征向量</h4>

<p>对于一个向量\(A\)，通过应用非零向量\(\overrightarrow{\nu}\)后仅仅相当于将\(\overrightarrow{\nu}\)同比例的缩放\(\gamma\)倍，那么我们称\(\overrightarrow{\nu}\)是特征向量，\(\gamma\)是特征值：<br/>
\[<br/>
A\overrightarrow{\nu} = \lambda\overrightarrow{\nu}<br/>
\]</p>

<p>1、对于一个方阵A，如果存在可逆向量P，满足\(P^{-1}AP=\Lambda\)，\(\Lambda\)是一个对角矩阵，这样我们可以说方阵A可对角化。</p>

<p>首先变换表示形式，将P用其列向量的形式表示\(P = (p_1,p_2,...,p_n)\)，对角矩阵\(\Lambda\)的表示形式如下：<br/>
\[<br/>
\Lambda = \left [ \begin{array} \\<br/>
\lambda_1&amp;&amp;           &amp;&amp;        &amp;&amp; \\<br/>
         &amp;&amp; \lambda_2 &amp;&amp;        &amp;&amp; \\<br/>
         &amp;&amp;           &amp;&amp; \ddots &amp;&amp; \\<br/>
         &amp;&amp;           &amp;&amp;        &amp;&amp; \lambda_n \\<br/>
\end{array}\right ]<br/>
\]</p>

<p>得：<br/>
\[<br/>
P^{-1}AP=\Lambda \Rightarrow AP = P\Lambda<br/>
\Rightarrow A(p_1,p_2,...,p_n) = (p_1,p_2,...,p_n)\left [\begin{array} \\<br/>
\lambda_1&amp;&amp;           &amp;&amp;        &amp;&amp; \\<br/>
         &amp;&amp; \lambda_2 &amp;&amp;        &amp;&amp; \\<br/>
         &amp;&amp;           &amp;&amp; \ddots &amp;&amp; \\<br/>
         &amp;&amp;           &amp;&amp;        &amp;&amp; \lambda_n \\<br/>
\end{array}\right] \\<br/>
\Rightarrow (Ap_1,Ap_2,...,Ap_n) = (\lambda_1p_1,\lambda_2p_2,...,\lambda_np_n)<br/>
\]</p>

<p>由上式可知：\(Ap_n=\lambda_np_n\)，所以可以看成对角矩阵的每一个元素\(\lambda_n\)都是方阵A的特征值，可逆矩阵P对应位置的列向量\(p_n\)是其特征向量。</p>

<p>2、对于一个可逆矩阵A，它的逆矩阵的特征值是原矩阵的倒数。设\(\lambda\)是其特征值，\(\overrightarrow{\nu}\)是特征向量，则有：<br/>
\[<br/>
A\overrightarrow{\nu} = \lambda\overrightarrow{\nu}<br/>
\] </p>

<p>两边同时左乘\(A^{-1}\)，可得：<br/>
\[<br/>
\overrightarrow{\nu} = \lambda A^{-1}\overrightarrow{\nu} \\<br/>
\Rightarrow \frac{1}{\lambda}\overrightarrow{\nu} = A^{-1} \overrightarrow{\nu} \\<br/>
\]</p>

<p>所以可知，逆矩阵的特征值是原矩阵的倒数。</p>

<p>3、复正规矩阵的特征向量和特征值</p>

<p>对角矩阵、实对称矩阵(\(A^T=A\))、反实对称矩阵(\(A^T = -A\))、厄米特矩阵(\(A^H=A\))、反厄米特矩阵(\(A^H=-A\))、正交矩阵(\(AA^T=A^TA=I\))、酉矩阵(\(AA^H=A^HA=I\))等满足\(AA^H=A^HA\)条件的矩阵都叫正规矩阵。并不是正规矩阵，都是对角矩阵、实对称、反实对称矩阵等，比如\(\left [ \begin{array}{lr} 1&amp; -1\\1&amp;1\\\end{array}\right]\)，不属于上面，但是它满足\(AA^T=A^TA\)，因此也是正规矩阵。</p>

<p>定理：对于正规矩阵\(A\)都存在酉矩阵\(Q\)使得：<br/>
\[<br/>
Q^HAQ = Q^{-1}AQ = \Sigma=\left [ \begin{array}\\\lambda_1&amp;&amp;&amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_n\\\end{array}\right]<br/>
\]</p>

<p>证明：可以通过\(A\)可以通过酉向量\(P\)化为上（下）三角矩阵\(B\)，即\(B=P^TAP\)，再由\(B^TB=P^TA^TPP^TAP=P^TA^TAP=P^TAA^TP=BB^T\)，因为\(B\)是上（下）三角矩阵，通过比较\(BB^T=B^TB\)对角线位置的元素，可知\(B\)是对角矩阵。</p>

<p>所以对于任意的复正规矩阵具有一组正交特征向量基，复正规矩阵可以被分解为：<br/>
\[<br/>
A = U \Sigma U^H<br/>
\]</p>

<p>其中U为一个酉矩阵，进一步：<br/>
\(A\)为厄米特矩阵\(\quad\Leftrightarrow\quad\)\(\Sigma\)的每个对角元都为实数。<br/>
\(A\)为反厄米特矩阵\(\quad\Leftrightarrow\quad\)\(\Sigma\)的每个对角元都为0或虚数。<br/>
\(A\)为酉矩阵\(\quad\Leftrightarrow\quad\)\(\Sigma\)的全部特征元的模为1。</p>

<p>继续：<br/>
\[<br/>
\begin{align*}<br/>
&amp;A = U \Sigma U^H \\<br/>
\Rightarrow &amp;AU = U\Sigma\\<br/>
\Rightarrow &amp;A(u_1,u_2,...,u_3) = (u_1,u_2,...,u_3)\left [ \begin{array}\\\lambda_1&amp;&amp;&amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_n\\\end{array}\right]\\<br/>
\Rightarrow &amp;(Au_1,Au_2,...,Au_3) = (\lambda_1u_1,\lambda_2u_2,...,\lambda_nu_n)\\<br/>
\Rightarrow &amp; Au = \lambda u\\<br/>
\end{align*}<br/>
\]</p>

<p>4、求解特征值和特征矩阵。由特征值和特征向量的定义易看出如何求解，首先是求出其特征值：<br/>
\[<br/>
A\overrightarrow{\nu} = \lambda \overrightarrow{\nu} \\<br/>
\Rightarrow (A-\lambda I)\overrightarrow{\nu} = 0<br/>
\]</p>

<p>由于特征向量\(\overrightarrow{\nu}\)不为0，所以可知\(|A-\lambda I| = 0\)，这样便可以求出特征值\(\lambda\)，再带回原式中求出特征向量\(\overrightarrow{\nu}\)</p>

<p>假设求解\(A = \left( \begin{array}{ccc}<br/>
-1 &amp;&amp; 1 &amp;&amp; 0 \\<br/>
-4 &amp;&amp; 3 &amp;&amp; 0 \\<br/>
1 &amp;&amp; 0 &amp;&amp; 2 \\<br/>
\end{array} \right)<br/>
\)的特征向量和特征值，按照之前描述需要先求出特征值，所以：<br/>
\[<br/>
\begin{align*}<br/>
|A - \lambda I | &amp;= \left | \begin{array}{ccc}<br/>
-1-\lambda &amp;&amp; 1 &amp;&amp; 0 \\<br/>
-4 &amp;&amp; 3 - \lambda &amp;&amp; 0\\<br/>
1 &amp;&amp; 0 &amp;&amp; 2-\lambda \\<br/>
\end{array} \right| = (2-\lambda)\left | \begin{array}{ccc}<br/>
-1-\lambda &amp;&amp; 1 \\<br/>
-4 &amp;&amp; 3-\lambda \\<br/>
\end{array} \right | \\<br/>
&amp;=(2-\lambda)[(-1-\lambda)(3-\lambda)+4] \\<br/>
&amp;=(2-\lambda)(\lambda^2-3\lambda+\lambda+1) \\<br/>
&amp;=(2-\lambda)(\lambda^2-2\lambda+1) \\<br/>
&amp;=(2-\lambda)(\lambda-1)^2 = 0<br/>
\end{align*}<br/>
\]</p>

<p>所以可得\(\lambda=1\)或\(\lambda=2\)。<br/>
当\(\lambda=1\)时：<br/>
\[<br/>
(A-\lambda I)\overrightarrow{\nu} = \left ( \begin{array}{ccc}<br/>
-2 &amp;&amp; 1 &amp;&amp; 0 \\<br/>
-4 &amp;&amp; 2 &amp;&amp; 0 \\<br/>
1 &amp;&amp; 0 &amp;&amp; 1 \\<br/>
\end{array} \right )\left( \begin{array} \\<br/>
x_1 \\<br/>
x_2 \\<br/>
x_3 \\<br/>
\end{array} \right) = 0\\<br/>
\Rightarrow \left \{ \begin{array}\\<br/>
-2x_1 + x_2 = 0\\<br/>
-4x_1 + 2x_2 = 0\\<br/>
x_1 + x_3 = 0\\<br/>
\end{array} \right.<br/>
\]</p>

<p>易得相应的特征向量为：\(P=\left( \begin{array}{ccc} -1 \\ -2 \\ 1 \\ \end{array} \right)\)，<br/>
同理可求得当\(\lambda=2\)是，特征向量为：\(P=\left( \begin{array}{ccc} 0 \\ 0 \\ 1 \\ \end{array} \right)\)</p>

<h2 id="toc_5">主成分分析 PCA</h2>

<p>PCA的英文名称就做Principle Component analysis，是一种常用的数据分析方法。他通过对原始数据进行线性变换，将数据转变为各维度线性无关的表示，可以用于数据降维和压缩。在机器学习中，经常我们需要处理的数据是几千维甚至是几十万维度的数据，如果直接处理，不论是对计算机性能、算法性能还是对训练时间度的要求都大大增大，这时候对高维数据降维，且不能丢失主要的信息。那么我们该如何确定该删掉哪些维度的数据呢？还有如何在最大限度减小维度的同时又能让信息损失最低呢？这就是PCA可以做的事。</p>

<p>PCA的主要目的在于保留最大信息量的同时降噪，如图：</p>

<div align="center">
    <img src="media/15034227314732/15272993250127.jpg" width="400px" />
</div>

<p>直观上来看将元素在红线上的投影来描述信息对信息的损失比蓝线要小很多，所以也就是希望投影后的方差最大化。信息处理认为信号具有较大的方差，噪音拥有较小的方差，信噪比就是信号与噪音的方差比。因此在PCA中，我们需要将数据有原始的m维数据转换到新的k维空间中，也就是我们需要找到k个正交基来描述空间。新的坐标系的选择与原始数据的构成有关，通常我们选择能使原始数据方差最大的方向作为第一个坐标轴，第二个坐标轴的选择是与第一个坐标轴正交且原始数据的投影方差最大的方向，依次类推选择k个坐标轴。</p>

<p>用一个例子来描述PCA的过程，假设我们有一个数据集如下：</p>

<p>\[<br/>
Data = \left . \begin{array}{c|c}<br/>
x &amp; y \\<br/>
\hline<br/>
2.5 &amp; 2.4 \\<br/>
0.5 &amp; 0.7 \\<br/>
2.2 &amp; 2.9 \\<br/>
1.9 &amp; 2.2 \\<br/>
3.1 &amp; 3.0 \\<br/>
2.3 &amp; 2.7 \\<br/>
2 &amp; 1.6 \\<br/>
1 &amp; 1.1 \\<br/>
1.5 &amp; 1.6 \\<br/>
1.1 &amp; 0.9 \\<br/>
\end{array} \right .<br/>
\]</p>

<p>我们首先需要求出数据集的协方差，由协方差计算公式可知需要先求出x和y分别的平均数：\(\overline{x} = 1.81,\,\overline{y} = 1.91\)，然后易求出协方差：<br/>
\[<br/>
Cov(Data) = \left ( \begin{array}{cc}<br/>
cov(x,x) &amp; cov(x,y) \\<br/>
cov(y,x) &amp; cov(y,y) \\<br/>
\end{array}\right) = \left ( \begin{array}{cc}<br/>
0.616555556 &amp; 0.615444444 \\<br/>
0.615444444 &amp; 0.716555556 \\<br/>
\end{array} \right )<br/>
\]</p>

<p>接下来求出该协方差矩阵的特征值，令\(A = Cov(Data)\)：<br/>
\[<br/>
\begin{align*}<br/>
| A - \lambda I| &amp;= \left | \begin{array}{cc}<br/>
0.616555556-\lambda &amp; 0.615444444 \\<br/>
0.615444444 &amp; 0.716555556-\lambda \\<br/>
\end{array} \right | = (0.616555556-\lambda)(0.716555556-\lambda)-0.615444444^2 \\<br/>
&amp;= \lambda^2 - 1.333111112\lambda + 0.06302445 = 0 \\<br/>
\end{align*}<br/>
\]</p>

<p>由一元二次方程的求解易得：<br/>
\[<br/>
\lambda = \left ( \begin{array}{cc}<br/>
1.28402770 \\<br/>
0.04908340 \\<br/>
\end{array}\right )<br/>
\]</p>

<p>将特征值排序后，选择k个最大的特征值，因为这里只有2个特征值，假设k=1，则取最大特征值\(\lambda\)=1.28402770，然后计算对应的特征向量：<br/>
\[<br/>
(A - \lambda I )\overrightarrow{\nu} = \left ( \begin{array}{cc}<br/>
-0.667472144 &amp; 0.615444444 \\<br/>
0.615444444 &amp; -0.567472144 \\<br/>
\end{array} \right ) \left(\begin{array} \\<br/>
x_1\\<br/>
x_2\\<br/>
\end{array} \right ) \\<br/>
\Rightarrow \left \{ \begin{array}\\<br/>
-0.667472144 x_1 + 0.615444444 x_2 = 0\\<br/>
0.615444444 x_1 - 0.567472144 x_2 = 0<br/>
\end{array} \right .<br/>
\]</p>

<p>很容易算出其中一个解:<br/>
\[<br/>
\overrightarrow{\nu} = \left \{ \begin{array} \\0.67787339 \\0.73517867 \\\end{array} \right.<br/>
\]</p>

<p>再将样本数据乘上特征向量，得到投影后的在一维空间内新数据：<br/>
\[<br/>
data^1 = \left ( \begin{array}{cc}<br/>
3.45911228 \\<br/>
0.85356176\\<br/>
3.6233396 \\<br/>
2.90535252 \\<br/>
4.30694352 \\<br/>
3.54409121 \\<br/>
2.53203265 \\<br/>
1.48656993 \\<br/>
2.19309596 \\<br/>
1.40732153 \\<br/>
\end{array} \right )<br/>
\]</p>

<p>如果说我们的目的是将原数据从二维空间降维到一维空间，那么现在已经完成了。这里也完成了对数据的压缩，现在我们可以通过\(data^1\)和\(\overrightarrow{\nu}\)来表示原始数据，可以用\(data^1\)乘上\(\overrightarrow{\nu}^T\)来还原原始数据：<br/>
\[<br/>
data^{new} = data^1 \overrightarrow{\nu}^T = \left ( \begin{array}{cc}<br/>
2.34484017 &amp;  2.54306557\\<br/>
0.57860681 &amp;  0.6275204 \\<br/>
2.4561655 &amp;  2.66380199\\<br/>
1.96946116 &amp;  2.1359532 \\<br/>
2.9195624 &amp;  3.16637301\\<br/>
2.40244512 &amp;  2.60554026\\<br/>
1.71639756&amp;   1.8614964 \\<br/>
1.0077062 &amp;   1.0928945 \\<br/>
1.48664139&amp;  1.61231737 \\<br/>
0.95398582&amp;  1.03463277 \\<br/>
\end{array} \right )<br/>
\]</p>

<p>和原始数据对比发现，压缩后的数据保留了大多数信息。</p>

<h4 id="toc_6">矩阵的迹</h4>

<p>在继续下面的PCA的最近重构解释小节之前，需要一些矩阵的迹的了解。在线性代数中，一个n*n的方阵A，主对角线上的各元素之和为矩阵的迹或者迹数，一般即为tr(A)。</p>

<p>矩阵\(A=(a_1,a_2,...,a_i)\)的F范数与矩阵的迹的关系为\(||A||_F^2 = tr(AA^T)\)：<br/>
\[<br/>
||A||_F^2 = \sum_{i=1}^N \sum_{j=1}^N a_{ij}^2 \\<br/>
(AA^T)_{ij} = A_i^TA_j = \sum{k=1}^N a_{ik}a_{jk} \\<br/>
tr(AA^T) = \sum_{i=1}^N (AA^T)_{ii} = \sum_{i=1}^N \sum_{k=1}^N a_{ik}^2 = ||A||_F^2<br/>
\]</p>

<p>矩阵的迹满足几个性质：</p>

<p>(1) \(tr(A+B) = tr(A)+tr(B)\)</p>

<p>证明：<br/>
\[<br/>
\because\quad(A+B)_{ij} = A_{ij}+B_{ij} \\<br/>
\therefore\quad tr(A+B) = \sum_i^N (A+B)_{ii} = \sum_{i=1}^N (A_{ii}+B_{ii}) = \sum_{i=1}^N A_{ii} + \sum_{j=1}^N B_{jj} = tr(A) + tr(B)<br/>
\]</p>

<p>(2) \(tr(AB) = tr(BA)\)</p>

<p>证明：<br/>
\[<br/>
\begin{align*}<br/>
\because \quad &amp;(AB)_{ij} = \sum_{k=1}^N a_{i,k}b_{k,j} \\<br/>
&amp;(BA)_{ij} = \sum_{k=1}^N b_{i,k} a_{k,j} \\<br/>
\therefore \quad &amp; tr(AB) = \sum_{i=1} (AB)_{ii} = \sum_{i=1}^N \sum_{k=1}^N a_{i,k} b_{k,i} =    tr(BA)<br/>
\end{align*}<br/>
\]</p>

<p>(3) 由(2)知：\(tr(ABC)=tr(CAB) = tr(CBA)\)<br/>
(4) 由转置的性质可知：\(tr(A) = tr(A^T)\)<br/>
(5) \(\nabla_{A} tr(AB) = B^T \)<br/>
(6) \(\nabla_{A} tr(ABA^TC) = CAB+C^T+AB^T \)<br/>
使用链式求导证明：<br/>
\[<br/>
\nabla_A tr(ABA^TC) = (A^TC)^T\nabla_A tr(AB) + AB \nabla_A tr(A^TC) = C^TAB^T + C^TAB<br/>
\]</p>

<h4 id="toc_7">PCA的最近重构解释</h4>

<p>假设数据集\(D=(x_1,x_2,...,x_m)\)，样本大小为m，原始数据为x_i为\(n\)维数据，再假定投影变换后得到的新坐标系为\((w_1,w_2,...,w_n)\)，正交向量有\(||w_i||=1,||w_iw_j||=0:j\neq i\)，若丢弃了部分新坐标系的坐标，即维度降低到\(k\)维（\(k&lt;m\)），那么\(x_i\)在低维坐标系中\(z_i=(z_{i1},z_{i2},...,z_{ik})\)，其中\(z_{ij}=w_j^Tx_i\)是\(x_i\)在低维坐标系第\(j\)维的坐标，若基于\(z_i\)来重构\(x_i\)，则\(\hat{x_i} = \sum_{j=1}^k z_{ij}w_j\)：</p>

<p>我们希望在映射到新空间后保存最大信息，定义损失函数为：<br/>
\[<br/>
\begin{align*}<br/>
\sum_{i=1}^m ||\hat {x_i}-x_i||_F^2 &amp;= \sum_{i=1}^m ||\sum_{j=1}^k z_{ij}w_j - x_i||_F^2 \\<br/>
&amp;= \sum_{i=1}^m(\sum_{j=1}^k\sum_{l=1}^k z_{ij}w_jz_{il}w_l - 2\sum_{j=1}^k z_{ij}w_j x_i + x_i^2) \\<br/>
\end{align*}<br/>
\]</p>

<p>由\(w_i^2 = 1\)和\(w_iw_j=0\)可知：<br/>
\[<br/>
\begin{align*}<br/>
\sum_{i=1}^m ||\hat {x_i}-x_i||_F^2 &amp;= \sum_{i=1}^m z^Tz - 2\sum_{i=1}^m z_i^TW^Tx_i + const<br/>
\end{align*}<br/>
\]</p>

<p>根据\(tr(AB)=tr(BA)\)以及\(z = W^Tx_i\)（关于矩阵的迹的性质不了解的可以，看文章最后），得：<br/>
\[<br/>
\begin{align*}<br/>
\sum_{i=1}^m ||\hat {x_i}-x_i||_F^2 &amp;= -tr(W^T(\sum_{i=1}^m x_ix_i^T) W) + const \\<br/>
&amp;= -tr(W^TXX^TW) + const<br/>
\end{align*}<br/>
\]</p>

<p>所以原始问题方程组为求解方程组的极大值：<br/>
\[<br/>
\begin{align*}<br/>
min_{W}\quad&amp; -tr(W^TXX^TW) + const\\<br/>
s.t. \quad&amp;WW^T = I \\<br/>
\end{align*}<br/>
\]</p>

<p>利用拉格朗日乘子法，引入拉格朗日乘子\(\alpha\)：<br/>
\[<br/>
L(W,\alpha) = -tr(W^TXX^TW) + const + \alpha(I-WW^T)<br/>
\]</p>

<p>那现在原始问题便变成了求解拉格朗日函数极大极小问题，先求\(L(W,\alpha)\)的最小值，让\(L(W,\alpha)\)对\(W\)求导得：<br/>
\[<br/>
\nabla_W L(W,\alpha) = X^TXW - \alpha W = 0 \\<br/>
\Rightarrow X^TXW = \alpha W<br/>
\]</p>

<p>所以求得最优解时\(W\)为协方差矩阵\(X^TX\)的特征向量，特征值越大的特征向量所包含的信息越丰富。</p>

<h4 id="toc_8">高维空间的PCA运算</h4>

<p>在现实生活中，由于样本的高维特征，比如在图像处理中，图像的每一个像素都是一个维度。如果我们采用上面的PCA方法运算，先计算协方差，那么我们将会得到一个n*n的协方差矩阵，这个矩阵不论是计算还是存储都比较复杂，因此不可取。</p>

<p>在高维特征中，假设有100个样本组成数据集\(X=(x_1,x_2,...,x_{100})\)，每一个样本的维度为10000维，那么协方差矩阵\(C=XX^T\)将是10000维的方阵，考虑一个替代的矩阵\(T=X^T\)代替协方差矩阵，，假设\(\lambda\)和\(P\)分别为协方差矩阵的特征值和特征向量，所以有：<br/>
\[<br/>
XX^TP = \lambda P \Leftrightarrow CP = \lambda P<br/>
\]</p>

<p>若\(\lambda^{new}\)和\(P^{new}\)为替代矩阵的特征值和特征向量：<br/>
\[<br/>
X^TXP^{new} = \lambda^{new} P^{new}<br/>
\]</p>

<p>两边同时左乘\(X\)，那么：<br/>
\[<br/>
XX^TXP^{new} = X\lambda^{new} P^{new} \Rightarrow XX^T(XP^{new}) = \lambda^{new}(XP^{new}) \\<br/>
\Rightarrow C(XP^{new}) = \lambda (XP^{new})<br/>
\]</p>

<p>通过上式可以知道协方差矩阵的特征值等于替代矩阵的特征值，其特征向量等于替代矩阵的特征向量左乘上\(X\)，所以我们可以通过求解替代矩阵的特征值和特征向量来完成PCA操作。</p>


		</div>

		

	</article>
  
	<div class="pagination">
	
<a href="archives.html">Blog Archives</a>
	 
	    
	</div>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>