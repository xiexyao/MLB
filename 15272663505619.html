
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  深度学习中正则化 - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div>
	<article class="hentry" role="article">
	<header>
			  	<h1 class="entry-title">深度学习中正则化</h1>
				<p class="meta"><time datetime="2018-05-26T00:39:10+08:00" pubdate data-updated="true">2018/5/26</time></p>
			 </header>
		  	<div class="entry-content">
			  	<p>在神经网络的训练过程中很容易发生过拟合现象，最好的降低过度拟合的方式之一就是增加训练样本的量。有了足够的训练数据，就算是一个规模非常大的网络也不容易过度拟合。不幸的是，训练数据其实是很难或者很昂贵的资源，所以这不是一种太切实际的选择。</p>

<p>还有一种可行的方式就是降低网络的规模。然而，大的网络拥有一种小的网络更强的潜力，所以这里存在一种应用冗余性的选项。幸运的是，还有其他的技术能够缓解过度拟合，即使我们只有一个固定的网络和固定的训练集合。这种技术就是正则化。</p>

<h3 id="toc_0">正则化方法</h3>

<p>正则化技术不仅可以应用在神经网络中，线性模型，如线性回归和逻辑回归都可以使用简单、直接、有效的正则化策略。在SVM中和决策树上就有这方面的应用。</p>

<p>许多正则化方法通过对目标函数 \(J\) 假设一个参数范数惩罚 \(\Omega(\theta)\) 限制模型的学习能力。我们将正则化后的目标函数记为 \(\widetilde J\)：<br/>
\[<br/>
\widetilde J = J + \alpha\Omega(\theta)<br/>
\]</p>

<p>其中 \(\alpha\in [0,\infty)\) 是权衡范数惩罚项 \(\Omega\) 和标准目标函数 \(J\) 相对贡献的超参数。将 \(\alpha\) 设为0表示没有正则化。 \(\alpha\) 越大，对应正则化惩罚越大。</p>

<p>当我们的训练算法最小化正则化后的目标函数 \(\widetilde J\) 时，它会降低原始目标 \(J\) 关于训练数据的误差并同时减小在某些衡量标准下参数 \(\theta\)（或参数子集）的规模。选择不同的参数范数 \(\Omega\) 会偏好不同的解。</p>

<p>在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常只对权重做惩罚而不对偏置做正则惩罚。精确拟合偏置所需的数据通常比拟合权重少得多。每个权重会指定两个变量如何相互作用。我们需要在各种条件下观察这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。因此，我们使用向量 \(w\) 表示所有应受范数惩罚影响的权重，而向量 \(\theta\) 表示所有参数 (包括 \(w\) 和无需正则化的参数)。</p>

<p>在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分配不同的 \(\alpha\) 系数。寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。</p>

<h4 id="toc_1">\(L^2\) 正则化（权重衰减）</h4>

<p>\(L^2\)正则化就是在代价函数 \(J\) 后面再加上一个正则化项 \(\Omega(\theta) = \frac{1}{2}||w||_2^2\) ：</p>

<p>\[<br/>
\widetilde J = J + \frac{\alpha}{2}w^T w <br/>
\]</p>

<p>与之对应梯度为：<br/>
\[<br/>
\nabla_w \widetilde J = \nabla_w J + \alpha w<br/>
\]</p>

<p>使用单步梯度下降更新权重，即执行以下更新:<br/>
\[<br/>
w \leftarrow w − \eta(\alpha w + \nabla_w J)<br/>
\]</p>

<p>换种写法就是:<br/>
\[<br/>
w \leftarrow (1 − \eta \alpha)w − \eta \nabla_w J<br/>
\]</p>

<p>我们可以看到，加入权重衰减后会引起学习规则的修改，即在每步执行通常的梯度更新之前先收缩权重向量（将权重向量乘以一个常数因子）。</p>

<p>我们进一步简化分析，令 \(w^∗\) 为未正则化的目标函数取得最小训练误差时的权重向量，即 \(w^∗ = \arg\min_w J (w)\)，并在 \(w^∗\) 的邻域对目标函数做二次近似。如果目标函数确实是二次的（如以均方误差拟合线性回归模型的情况），则该近似是完美的。 近似的 \(\hat J(θ)\) 如下<br/>
\[<br/>
\hat J(θ) = J(w^∗) + \frac 1 2 (w − w^∗)^T H(w − w^∗),<br/>
\]</p>

<p>其中 \(H\) 是 \(J\) 在 \(w^*\) 处计算的 Hessian 矩阵（关于 \(w\)）。因为 \(w^*\) 被定义为最优，即梯度消失为 0，所以该二次近似中没有一阶项。同样地，因为 \(w^*\) 是 \(J\) 中的一个最优点，我们可以得出 \(H\) 是半正定的结论。</p>

<p>当 \(\hat J\) 取得最小时，其梯度<br/>
\[<br/>
\nabla_w \hat J(w) = H(w-w^*) <br/>
\]</p>

<p>为0。</p>

<p>如果我们加上权重衰减梯度之后再来看最小化正则化后的 \(\hat J\)，我们使用 \(\widetilde w\) 表示此时的最优点：<br/>
\[<br/>
\begin{align}<br/>
H(\widetilde w - w^*) + \alpha \widetilde w = 0\nonumber\\<br/>
(H + \alpha I)\widetilde w = Hw^*\nonumber\\<br/>
\widetilde w = (H+\alpha I)^{-1}Hw^*\label{wwha}\\<br/>
\end{align}<br/>
\]</p>

<p>当 \(\alpha\) 趋向于 0 时，正则化的解 \(\widetilde w\) 会趋向于 \(w^*\)。因为 \(H\) 是实对称矩阵，我们可以将其分解为一个对角矩阵 \(\Lambda\) 和一组特征向量的标准正交基 \(Q\)，并且有 \(H=Q\Lambda Q^T\)。应用于式 ( \ref{wwha} ) 得<br/>
\[<br/>
\begin{align}<br/>
\widetilde w &amp;= (Q\Lambda Q^T + \alpha I)^{-1} Q\Lambda Q^T w^*\label{wwqq}\\<br/>
&amp;= [Q(\Lambda + \alpha I)Q^T]^{-1} Q\Lambda Q^Tw^*\label{wwqq2}\\<br/>
&amp;= Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^*\label{wwqq3}\\<br/>
\end{align}<br/>
\]</p>

<p>从 ( \ref{wwqq} ) 式到 ( \ref{wwqq2} ) 式是因为</p>

<blockquote>
<p>正交矩阵和它的转置的乘积是单位矩阵，即 \(Q\) 是正交矩阵，有 \(QQ^T = I\)</p>
</blockquote>

<p>从 ( \ref{wwqq2} ) 式到 ( \ref{wwqq3} ) 式是因为</p>

<blockquote>
<p>正交矩阵的转置矩阵等于逆矩阵，即 \(Q\) 是正交矩阵，有 \(Q^T = Q^{-1}\)</p>
</blockquote>

<p>现在将 \((\Lambda + \alpha I)^{-1} \Lambda\) 分解结算<br/>
\[<br/>
\begin{align*}<br/>
(\Lambda + \alpha I)\Lambda &amp;= \Bigg(\left [ \begin{array}\\\lambda_{1}&amp;&amp;&amp;\\&amp;\lambda_{2}&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_{n}\\ \end{array} \right ] + \alpha \cdot\left [ \begin{array}\\1&amp;&amp;&amp;\\&amp;1&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;1\\\end{array}\right ] \Bigg)^{-1}\cdot \left [ \begin{array}\\\lambda_{1}&amp;&amp;&amp;\\&amp;\lambda_{2}&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_{n}\\ \end{array} \right ]\\<br/>
&amp;= \left [ \begin{array}\\\lambda_{1}+\alpha&amp;&amp;&amp;\\&amp;\lambda_{2}+\alpha&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_{n}+\alpha\\\end{array}\right ]^{-1}\cdot \left [ \begin{array}\\\lambda_{1}&amp;&amp;&amp;\\&amp;\lambda_{2}&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_{n}\\ \end{array} \right ]\\<br/>
&amp;= \left [ \begin{array}\\\frac{1}{\lambda_{1}+\alpha}&amp;&amp;&amp;\\&amp;\frac{1}{\lambda_{2}+\alpha}&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\frac{1}{\lambda_{n}+\alpha}\\\end{array}\right ]\cdot \left [ \begin{array}\\\lambda_{1}&amp;&amp;&amp;\\&amp;\lambda_{2}&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\lambda_{n}\\ \end{array} \right ]\\<br/>
&amp;= \left [ \begin{array}\\\frac{\lambda_1}{\lambda_{1}+\alpha}&amp;&amp;&amp;\\&amp;\frac{\lambda_2}{\lambda_{2}+\alpha}&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;\frac{\lambda_n}{\lambda_{n}+\alpha}\\\end{array}\right]<br/>
\end{align*}<br/>
\]</p>

<p>我们可以看到权重衰减的效果是沿着由 \(H\) 的特征向量所定义的轴缩放 \(w^*\)。也就是说，我们会根据 \(\frac{\lambda_i}{\lambda_i + \alpha}\) 因子缩放与 \(H\) 的第 \(i\) 个特征向量对齐的 \(w^*\) 的分量。沿着 \(H\) 特征值较大的方向，\(\lambda_i \gg \alpha\) 时，正则化的影响较小。而 \(\lambda_i \ll \alpha\) 时，此时分量将收缩到几乎为零。</p>

<p>我们借助一张图来看看 \(L^2\) 正则化的效果。</p>

<div align="center">
    <img width="350" src="media/15272663505619/15380583249307.jpg" />
</div>

<p>坐标轴右上方是原始目标函数 \(J(w)\) 的等高线，中心 \(w^∗\) 是没有正则化的原始最优解。图中虚线的同心圆是L2正则化项的等高线。在 \(\widetilde w\) 点，这两个竞争目标达到平衡。目标函数 J 的 Hessian 的 第一维特征值很小，当 \(w^∗\) 沿水平方向移动时，目标函数值不会增加的太多，因为目标函数对这个方向没 有强烈的偏好，所以正则化项对该轴具有强烈的影响，正则化项将 \(w_1\) 拉向零；当 \(w^∗\) 沿竖直方向移动时，目标函数值变化比较剧烈，对应于特征值大的方向，表示高区率。然后对比图中 \(\widetilde w\) 和 \(w^∗\) 的位置，发现 \(\widetilde w\) 在水平方向上移动距离比较大，竖直方向上比较小，这也印证了之前权重衰减的规律。 </p>

<blockquote>
<ol>
<li>Hessian矩阵中，在最大特征值所对应的特征向量的方向上有二阶导数的最大值，事实上，在每一个特征向量方向上，二阶导数都等于相应的特征值。在其他方向上，二阶导数是特征值的加权平均。特征值是相应特征向量方向上的二阶导数。</li>
<li>等高线的疏密与地势的坡度有关。等高线越密集,代表该地区的坡度越陡；等高线越稀疏,说明地势坡度越小越平坦。</li>
<li>假设有一大一小两个特征值，较小曲率（二阶导数）落在小特征值对应的特征向量方向上，这里较平坦；较大曲率（二阶导数）落在大特征值对应的特征向量方向上，这里陡峭；所以说，特征向量被称为函数等高线的主轴。</li>
</ol>

<p>综上：Hessian矩阵的特征值控制了梯度更新步长，对于二维图像的某点的Hessian矩阵，其最大特征值和对应的特征向量对应其邻域二维曲线最大曲率的强度和方向，即山坡陡的那面，最小特征值对应的特征向量对应与其垂直的方向，即平缓的方向。简单来讲，图像某点的 Hessian 矩阵特征值大小和符号决定了该点邻域内的几何结构。三维图像同理。</p>
</blockquote>

<p>如果我们在线性回归的基础上研究权重衰减对二次代价函数的影响。线性回归的代价函数是平方误差之和<br/>
\[<br/>
(Xw - y)^T(Xw - y)<br/>
\]</p>

<p>我们添加 \(L^2\) 正则化项后，目标函数变为<br/>
\[<br/>
(Xw - y)^T(Xw - y) + \frac 1 2 \alpha w^T w<br/>
\]</p>

<p>这将普通方程的解（求导并令结果等于0）从<br/>
\[<br/>
\begin{align}<br/>
w = (X^TX)^{-1} X^T y\label{wx1}\\<br/>
\end{align}<br/>
\]</p>

<p>变为<br/>
\[<br/>
\begin{align}<br/>
w = (X^TX + \alpha I)^{-1} X^T y\label{wx2}<br/>
\end{align}<br/>
\]</p>

<p>式 ( \ref{wx1} ) 中的矩阵 \(X^TX\) 与协方差矩阵 \(\frac 1 m X^TX\) 成正比。\(L^2\) 正则后仅仅是在对角加了 \(\alpha\)。这个矩阵的对角项对应每个输入特征的方差（协方差的定义）。可以看到，\(L^2\) 正则化能让学习算法感知到具有较高方差的输入 \(x\)，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩。</p>

<h4 id="toc_2">\(L^1\)正则化</h4>

<p>\(L^2\)权重衰减是权重衰减最常见的形式，我们还可以使用其他的方法限制模型参数的规模。一个选择是使用\(L^1\)正则化。 形式地，对模型参数 \(w\) 的\(L^1\)正则化被定义为:<br/>
\[<br/>
\Omega(\theta) = ||w||_1 = \sum_{i} |w_i|<br/>
\]</p>

<p>即各个参数的绝对值之和。接着我们将讨论\(L^1\)正则化对简单线性回归模型的影响，与分析\(L^2\)正则化时一样不考虑偏置参数。我们尤其感兴趣的是找出\(L^1\)和\(L^2\)正则化之间的差异。与\(L^2\)权重衰减类似，我们也可以通过缩放惩罚项 \(\Omega\) 的正超参数 \(\alpha\) 来控制\(L^1\)权重衰减的强度。因此，正则化的目标函数 \(\widetilde J\) 如下所示<br/>
\[<br/>
\begin{align}<br/>
\widetilde J = J + \alpha ||w||_1\label{wjj}<br/>
\end{align}<br/>
\]</p>

<p>我们常说 \(L^1\) 正则化的优良性质是能产生稀疏性，导致 \(\mathbf w\) 中许多项变成零。这里我们先直观的看一下 \(L^1\) 的图像（下右图）：</p>

<div align="center">
    <img width="550" src="media/15272663505619/15380584017290.jpg" />
</div>

<p>可以看到，\(L^1\) 与 \(L^2\) 的不同就在于他在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置为产生稀疏性，例如图中的相交点就有 \(w_1 = 0\)，而更高维的时候除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。</p>

<p>相比之下，\(L^2\) 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么  \(L^1\) 正则化能产生稀疏性，而  \(L^2\) 正则化不行的原因了。</p>

<p>正则化后的目标函数 ( \ref{wjj} )对应的梯度 (实际上是次梯度):<br/>
\[<br/>
\nabla_w \hat J = \nabla_w J + \alpha \text{ sign}(w)<br/>
\]</p>

<p>其中 \(\text{sign}(w)\) 只是简单地取 \(w\) 各个元素的正负号。</p>

<p>由于 \(L^1\) 惩罚项在完全一般化的 Hessian 的情况下，无法得到直接清晰的代数表达式，因此我们将进一步简化假设 Hessian 是对角的，即 \(H = diag([H_{1,1},..., H_{n,n}])\)， 其中每个 \(H_{i,i} &gt; 0\)。如果线性回归问题中的数据已被预处理(如可以使用 PCA)，去 除了输入特征之间的相关性，那么这一假设成立。</p>

<p>我们可以将 \(L^1\) 正则化目标函数的二次近似分解成关于参数的求和（一阶导数为0）:<br/>
\[<br/>
\begin{align}<br/>
J(w) &amp;= J(w^*) + \frac 1 2 (w-w^*)^T H(w-w^*) + \alpha ||w||\nonumber\\<br/>
&amp;= J(w^*) + \frac 1 2 [w_1-w^*_1,w_2 - w^*_2 ,...,w_n-w^*_n]H\left[\begin{array}{c}w_1-w^*_1\\w_2 - w^*_2 \\\vdots\\w_n-w^*_n\\\end{array}\right ]+ \alpha ||w||\nonumber\\<br/>
&amp;= J(w^*) + \frac 1 2 \left [ \begin{array}{cccc}(w_1-w^*_1)H_{1,1}&amp;&amp;&amp;\\&amp;(w_2-w^*_2)H_{2,2}&amp;&amp;\\&amp;&amp;\ddots&amp;\\&amp;&amp;&amp;(w_n-w^*_n)H_{n,n}\end{array}\right ]\left[\begin{array}\\w_1-w^*_1\\w_2 - w^*_2 \\\vdots\\w_n-w^*_n\\\end{array}\right ]+ \alpha ||w||\nonumber\\<br/>
&amp;= J(w^*) + \frac 1 2 \Big(\sum_{i=1}^n H_{i,i}(w_i - w^*_i)^2\Big)+ \alpha ||w||\nonumber\\<br/>
&amp;= J(w^*) + \frac 1 2 \sum_{i=1}^n \Big(H_{i,i}(w_i - w^*_i)^2+ \alpha |w_i|\Big)\label{jfs}\\<br/>
\end{align}<br/>
\]</p>

<p>对二次近似的 ( \ref{jfs} ) 式求导，并结果为 0 来求解 \(w_i\)<br/>
\[<br/>
\begin{align}<br/>
&amp;\frac{\partial}{\partial w_i} J(w^*) + \frac 1 2 \sum_{i=1}^n \Big(H_{i,i}(w_i - w^*_i)^2+ \alpha |w_i|\Big) = H_{i,i}(w_i - w^*_i) + \text{sign}(w_i) \alpha\label{fpp}\\<br/>
\end{align}<br/>
\]</p>

<ol>
<li><p>当 \(w_i \gt 0\) 时<br/>
\[<br/>
H_{i,i}(w_i - w^*_i) + \alpha = 0\quad \Rightarrow w_i - w^*_i =- \frac{\alpha}{H_{i,i}} <br/>
\]</p>

<p>因为 \(\alpha \gt 0\) 和 \(H_{i,i} \gt 0\) 所以 \(w_i - w^*_i =- \frac{\alpha}{H_{i,i}} \lt 0\)，得出 \(w^*_i \gt w_i \gt 0\)</p>

<p>结合 \(w_i = w^* - \frac{\alpha}{H_{i,i}} \) 和 \(w_i \gt 0\) 可得 \(w_i = \max\big(w^*_i - \frac{\alpha}{H_{i,i}},0\big)\)</p></li>
<li><p>当 \(w_i \lt 0\) 时<br/>
\[<br/>
\begin{equation}<br/>
H_{i,i}(w_i - w^*_i) - \alpha = 0 \quad \Rightarrow w_i - w^*_i = \frac{\alpha}{H_{i,i}}\label{hiww}<br/>
\end{equation} <br/>
\]</p>

<p>因为 \(\alpha \gt 0\) 和 \(H_{i,i} \gt 0\) 所以 \(w_i - w^*_i =\frac{\alpha}{H_{i,i}} \gt 0\)，得出 \(w^*_i \lt w_i \lt 0\)</p>

<p>等式 ( \ref{hiww} ) 可以写成<br/>
\[<br/>
-w_i = -w^*_i - \frac{\alpha}{H_{i,i}}<br/>
\]</p>

<p>因为 \(-w_i \gt 0\)，所以 \(-w_i = \max\big(-w^*_i - \frac{\alpha}{H_{i,i}},0\big)\)，也就是 \(w_i = -\max\big(-w^*_i - \frac{\alpha}{H_{i,i}},0\big) \)</p></li>
</ol>

<p>结合上面两项<br/>
\[<br/>
w_i = \left \{ \begin{array} -\max\big(-w^*_i - \frac{\alpha}{H_{i,i}},0\big)&amp;\quad w^*_i \gt w_i \gt 0\\ -\max\big(-w^*_i - \frac{\alpha}{H_{i,i}},0\big)&amp;\quad w^*_i \lt w_i \lt 0\\\end{array}\right .<br/>
\]</p>

<p>可以写成一个式子<br/>
\[<br/>
w_i = \text{sign}(w^*_i)\max\big(||w^*_i|| - \frac{\alpha}{H_{i,i}},0\big)<br/>
\]</p>

<p>对每个 \(i\) , 考虑 \(w^∗_i &gt; 0\) 的情形，会有两种可能结果：</p>

<ol>
<li>\(w^*_i \le \frac{\alpha}{H_{i,i}}\) 的情况。正则化后目标中的 \(w_i\) 最优值是 \(w_i = 0\)。这是因为在方向 \(i\) 上的 \(J(w)\) 对 \(\hat J(w)\) 的贡献被抵消，\(L^1\) 正则化项将 \(w_i\) 推至 0。</li>
<li>\(w^∗_i \gt \frac{\alpha}{H_{i,i}}\) 的情况。在这种情况下，正则化不会将 \(w_i\) 的最优值推至 0，而仅仅在那个方向上移动 \(\frac{\alpha}{H_{i,i}}\) 的距离。</li>
</ol>

<p>\(w^*_i \lt 0\) 的情况与之类似，但是 \(L^1\) 正则化项使 \(w_i\) 更接近 0（或增加 \(\frac{\alpha}{H_{i,i}}\)）或等于0。</p>

<p>可以用下图形象表示出来</p>

<div align="center">
    <img width="300" src="media/15272663505619/15380681849481.jpg" />
</div>

<h4 id="toc_3">\(L^1\) 与 \(L^2\) 优点</h4>

<p>相比 \(L^2\) 正则化，\(L^1\) 正则化会产生更稀疏(sparse)的解。此处稀疏性指的是最优值中的一些参数为 0。和 \(L^2\)正则化相比，\(L^1\)正则化的稀疏性具有本质的不同，在 \(L^2\) 中 是对参数做了缩小 \(\widetilde w_i = \frac{H_{i,i}}{H_{i,i}+\alpha} w^*_i\) ，如果 \(w^*_i\) 不是 0，那么 \(\widetilde w_i\) 也会保持非 0，只是更接近0。</p>

<p>由 \(L^1\) 正则化导出的稀疏性质已经被广泛地用于特征选择(feature selection)机制。特征选择从可用的特征子集选择出有意义的特征，化简机器学习问题。著名的 LASSO (Tibshirani, 1995)(Least Absolute Shrinkage and Selection Operator)模型将 \(L^1\) 惩罚和线性模型结合，并使用最小二乘代价函数。\(L^1\) 惩罚使部分子集的权重为零，表明相应的特征可以被安全地忽略。</p>

<p>再来看看 \(L^2\) 正则化，首先 \(L^2\) 正则化可以解决过拟合问题，这是因为 \(L^2\) 正则化趋向于选择更小的参数，而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。</p>

<p>另外，\(L^2\) 正则化之前，我们知道可以通过下式求出权重<br/>
\[<br/>
W = (X^TX)^{-1}X^Ty<br/>
\]</p>

<p>然而，如果当我们的样本 \(X\) 的数目比每个样本的维度还要小的时候，矩阵 \(X^TX\) 将会不是满秩的，也就是 \(X^TX\) 会变得不可逆，所以 \(W\) 就没办法直接计算出来了，或者更确切地说，将会有无穷多个解（因为我们方程组的个数小于未知数的个数），也就是说，我们的数据不足以确定一个解。</p>

<p>但如果加上L2规则项，就变成了下面这种情况，就可以直接求逆了：<br/>
\[<br/>
W = (X^TX + \alpha I)^{-1}X^Ty<br/>
\]</p>

<p>这里面，要得到这个解，我们通常并不直接求矩阵的逆，而是通过解线性方程组的方式（例如高斯消元法）来计算。考虑没有规则项的时候，也就是 \(\lambda=0\) 的情况，如果矩阵 \(X^TX\) 的 condition number 很大的话，解线性方程组就会在数值上相当不稳定，而这个规则项的引入则可以改善 condition number。</p>

<p>另外，如果使用迭代优化的算法，condition number 太大仍然会导致问题：它会拖慢迭代的收敛速度，而规则项从优化的角度来看，实际上是将目标函数变成 \(\lambda\)-strongly convex（\(\lambda\)强凸）的了。</p>

<div align="center">
    <img width="450" src="media/15272663505619/15380731126993.jpg" />
</div>

<p>如图说一个函数是凸函数，指函数曲线位于改点处的切线之上，而强凸则进一步要求位于该处的一个二次函数之上，也就是说要求函数不要太“平坦”而是可以保证有一定的 “向上弯曲” 的趋势。如果我们有“强凸”的话，我们就可以得到一个更好的近似解。效果的好坏取决于 strongly convex性质中的常数 \(\lambda\) 的大小。</p>

<p>所以，如果我们想要获得 \(\lambda\)强凸的话，就是往目标函数里面加上 \(\frac{\alpha}{2}||w||^2\)。</p>

<p>如果我们的函数 \(f(w)\) 如右图红色那个函数，我们取我们的最优解 \(w^*\) 的地方都会位于蓝色虚线的那根二次函数之上，这样就算 \(w_t\) 和 \(w^*\) 离的比较近的时候，\(f(w_t)\) 和 \(f(w^*)\) 的值差别还是挺大的，也就是会保证在我们的最优解 \(w^*\) 附近的时候，还存在较大的梯度值，这样我们才可以在比较少的迭代次数内达到 \(w^*\) 。但对于左图，红色的函数 \(f(w)\) 只约束在一个线性的蓝色虚线之上，假设是如左图的很不幸的情况（非常平坦），那在 \(w_t\) 还离我们的最优点 \(w^*\) 很远的时候，我们的近似梯度 \((f(wt)-f(w*))/(wt-w*)\) 就已经非常小了，在 \(w_t\) 处的近似梯度\(\frac{\partial f}{\partial w}\) 就更小了，这样通过梯度下降得到的结果就是 \(w\) 的变化非常缓慢，非常缓慢的向我们的最优点 \(w^*\) 爬动，那在有限的迭代时间内，它离我们的最优点还是很远。我们有可能会找到一个很远的点。但如果我们有“强凸”的话，就能对情况做一些控制，我们就可以得到一个更好的近似解。</p>

<h3 id="toc_4">数据增强</h3>

<p>理论上来说，数据越多，模型训练得越充分，模型泛化能力越强。但是现实情况是，数据量总是有限的，解决此问题的一个方法是生成一部分的模拟数据。对于一些机器学习任务，创建新的假数据相当简单。</p>

<p>对分类来说这种方法是最简单的。分类器需要一个复杂的高维输入 x，并用单 个类别标识 y 概括 x。这意味着分类面临的一个主要任务是要对各种各样的变换保 持不变。我们可以轻易通过转换训练集中的 x 来生成新的 (x, y) 对。经典的方法有SMOTE方法。</p>

<p>这种方法对于其他许多任务来说并不那么容易。例如，除非我们已经解决了密 度估计问题，否则在密度估计任务中生成新的假数据是很困难的。</p>

<p><strong>图像识别</strong>：图像是高维的并包括各种巨大的变化因素，其中有许多可以轻易地模拟。即使模型已使用卷积和池化技术对部分平移保持不变，沿训练图像每个方向<strong>平移</strong>几个像素的操作通常可以大大改善泛化。许多其他操作如<strong>旋转图像</strong>或<strong>缩放图像</strong>也已被证明非常有效。数据增强用于特定领域分类问题，如图像识别很有效。但是切记，转换数据的时候不要改变图像的正确分类。比如不要将手写字识别图像中的6垂直转换成了9，b 水平翻转成了d。所以对这些任务来说，水平翻转和旋转 180 度并不是合适的数据集增强方式。</p>

<p><strong>语音识别</strong>：语音识别问题中，网络输入数据中也会注入一些随机噪音干扰，这也是一种数据增强（现实生活中语音环境有噪音）。</p>

<p>在神经网络的输入层注入噪声 (Sietsma and Dow, 1991) 也可以被看作是数据增强的一种方式。对于许多分类甚至一些回归任务而言，即使小的随机噪声被加到输入，任务仍应该是能够被解决的。然而，神经网络被证明对噪声不是非常健壮 (Tang and Eliasmith, 2010)。改善神经网络健壮性的方法之一是简单地将随机噪声添加到输入再进行训练。输入噪声注入是一些无监督学习算法的一部分，如去噪自编码器(Vincent et al., 2008a)。向隐藏单元施加噪声也是可行的，这可以被看作在多个抽象层上进行的数据集增强。Poole et al. (2014) 最近表明，噪声的幅度被细心调整后，该方法是非常高效的。正则化策略 Dropout 可以被看作是通过与噪声相乘构建新输入的过程。</p>

<p>在比较机器学习基准测试的结果时，考虑其采取的数据集增强是很重要的。通常情况下，人工设计的数据集增强方案可以大大减少机器学习技术的泛化误差。将一个机器学习算法的性能与另一个进行对比时，对照实验是必要的。在比较机器学习算法 A 和机器学习算法 B 时，应该确保这两个算法使用同一人工设计的数据集增强方案。假设算法 A 在没有数据集增强时表现不佳，而 B 结合大量人工转换的数据后表现良好。在这样的情况下，很可能是合成转化引起了性能改进，而不是机器学习算法 B 比算法 A 更好。有时候，确定实验是否已经适当控制需要主观判断。例如，向输入注入噪声的机器学习算法是执行数据集增强的一种形式。通常，普适操作(例如，向输入添加高斯噪声)被认为是机器学习算法的一部分，而特定于一个应用领域(如随机地裁剪图像)的操作被认为是独立的<strong>预处理</strong>步骤。</p>

<h3 id="toc_5">噪声鲁棒性</h3>

<p>上面我们说过想神经网络的输入层注入噪声可以作为数据增强，对于某些模型而言，想输入添加方差极小的噪声等价于对权重事假范数惩罚。在一般情况下，注入噪声远比简单地收缩参数强大，特别是噪声被添加到隐藏单元时会更加强大。向隐藏单元添加噪声是值得讨论的话题，后面所述 Dropout 算法是这种做法的主要发展方向。</p>

<p>另一种正则化模型的噪声使用方式是将其加到权重。这项技术主要用于循环神经网络（Jim et al., 1996; Graves, 2011）。在某些假设下，施加于权重的噪音可以被解释为与更传统的正则化形式等同，鼓励要学习的函数保持稳定。在回归的情形下，训练将一组特征 \(\mathbf x\) 映射成一个标量的函数 \(\hat y(x)\)，并使用最小二乘代价函数衡量模型预测值 \(\hat y(x)\) 与真实值 \(y\) 的误差<br/>
\[<br/>
J = \mathbb E_{p(x,y)}[\hat y(\mathbb x) - y)^2]<br/>
\]</p>

<p>训练集包含 \(m\) 对标注样例 \(\{(\mathbf x^{(1)},y^{(1)}),...,(\mathbf x^{(m)},y^{(m)})\}\)。</p>

<p>现在我们假设对每个输入表示，网络权重添加随机扰动 \(\epsilon_w\sim \mathcal N(\epsilon;9,\eta I)\)。想象一下我们有个一个标准的 \(l\) 层 MLP。我们将扰动模型记为 \(\hat y_{\epsilon w}(x)\)。尽管有噪声注入，我们仍然希望减少网络输出误差的平方。因此目标函数变为：<br/>
\[<br/>
\begin{align*}<br/>
\widetilde J_{\mathbf w} &amp;= \mathbb E_{p(x,y,\epsilon \mathbf w)}[(\hat y_{\epsilon \mathbf w}(x) - y)^2]\\<br/>
&amp;= \mathbb E_{p(x,y,\epsilon \mathbf w)}[\hat y^2_{\epsilon \mathbf w}(x) - 2y\hat y_{\epsilon \mathbf w}(x) + y^2]\\<br/>
\end{align*}<br/>
\]</p>

<p>对于小的 \(\eta\)，最小化带权重噪声（方差为 \(\eta I\)）的 \(J\) 等同于最小化附加正则化项: \(\eta \mathbb E_{p(x,y)}[||\nabla_{\mathbf W} \hat y(x)||^2]\) 的 \(J\)。这种形式的正则化鼓励参数进入权重小扰动对输出相对影响较小的参数空间区域。换句话说，它推动模型进入对权重小的变化相对不敏感的区域，找到的点不只是极小点，还是由平坦区域所包围的极小点 (Hochreiter and Schmidhuber, 1995)。在简化的线性回归中（例如，\(\hat y(x) = w^T x + b）\)，正则项退化为 \(\eta \mathbb E_{p(x)}[||x||^2]\)，这与函数的参数无关，因此不会对 \(\widetilde J_w\) 关于模型参数的梯度有影响。 </p>

<h4 id="toc_6">向输出目标注入噪声</h4>

<p>大多数数据集的 \(y\) 标签都有一定错误。错误的 \(y\) 不利于最大化 \(\log p(y | \mathbf x)\)。避免这种情况的一种方法是显式地对标签上的噪声进行建模。例如，我们可以假设，对于一些小常数 \(\epsilon\)，训练集标记 \(y\) 是正确的概率是 \(1 − \epsilon\)，(以 \(\epsilon\) 的概率)任何其他可能的标签也可能是正确的。这个假设很容易就能解析地与代价函数结合，而不用显式地抽取噪声样本。例如，<strong>标签平滑</strong>(label smoothing)通过把确切分类目标从 0 和 1 替换成 \(\epsilon\) 和 \(1 − \epsilon\)，正则化具有 \(k\) 个输出的 softmax 函数 的模型。标准交叉熵 \(k−1\) 损失可以用在这些非确切目标的输出上。使用 softmax 函数和明确目标的最大似然学习可能永远不会收敛——softmax 函数永远无法真正预测 0 概率或 1 概率，因此它会继续学习越来越大的权重，使预测更极端。使用如权重衰减等其他正则化策略能够防止这种情况。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。这种策略自 20 世纪 80 年代就已经被使用，并在现代神经网络继续保持显著特色 (Szegedy et al., 2015)。</p>

<h3 id="toc_7">提前终止</h3>

<p>当训练有足够的表示能力甚至会过拟合的大模型时，我们经常观察到，训练误差会随着时间的推移逐渐降低但验证集的误差会再次上升。</p>

<p>这意味着我们只要返回使验证集误差最低的参数设置，就可以获得验证集误差更低的模型(并且因此有希望获得更好的测试误差)。在每次验证集误差有所改善后，我们存储模型参数的副本。当训练算法终止时，我们返回这些参数而不是最新的参数。当验证集上的误差在事先指定的循环次数内没有进一步改善时，算法就会终止。</p>

<p>这种策略被称为<strong>提前终止（early stopping）</strong>。这可能是深度学习中最常用的正则化形式。它的流行主要是因为有效性和简单性。</p>

<div align="center">
    <img width="550" src="media/15272663505619/15472738666106.jpg" />
</div>

<p>我们可以认为提前终止是非常高效的超参数选择算法。按照这种观点，训练步数仅是另一个超参数。这个超参数在验证集上具有 U 型性能曲线。很多控制模型容量的超参数在验证集都是这样的 U 型性能曲线，在提前终止的情况下，我们通过控制拟合训练集的步数来控制模型的有效容量。大多数超参数的选择必须使用高代价的猜测和检查过程，我们需要再训练开始时超参数的选择必须使用高代价的猜测和检查过程，我们需要再训练开始时猜测一个超参数，然后运行几个步骤检查它的训练效果。“训练时间”是唯一只要跑一次训练就能尝试很多值得超参数。通过提前终止自动选择超参数的唯一显著代价是训练期间要定期评估验证集。在理想情况下，这可以并行在与主训练过程分离的机器上，或独立的CPU，或独立的 GPU 上完成。如果没有这些额外的资源，可以使用比训练集小的验证集或较不频繁地评估验证集来减小评估代价，较粗略地估算取得最佳的训练时间。</p>

<p>另一个提前终止的额外代价是需要保持最佳的参数副本。这种代价一般是可忽略的，因为可以将它储存在较慢较大的存储器上(例如，在 GPU 内存中训练，但将最佳参数存储在主存储器或磁盘驱动器上)。由于最佳参数的写入很少发生而且从不在训练过程中读取，这些偶发的慢写入对总训练时间的影响不大。</p>

<p>提前终止是一个非常不显眼的正则化形式，它几乎不需要改变基本训练过程、目标函数或一组允许的参数值。这意味着，无需破坏学习动态就能很容易地使用提前终止。相对于权重衰减，必须小心不能使用太多的权重衰减，以防网络陷入不良局部极小点（对应于病态的小权重）。</p>

<p>提前终止可单独使用或与其他正则化策略结合使用。即使为鼓励更好泛化，使用正则化策略改进目标函数，在训练目标的局部极小点达到最好泛化也是罕见的。</p>

<p>提前终止需要验证集，这意味着某些训练数据不能被馈送到模型。为了更好地利用这一额外的数据，我们可以在完成提前终止的首次训练之后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的训练数据都被包括在内。有两个基本的策略都可以用于第二轮训练过程。</p>

<p>一个策略是再次初始化模型，然后使用所有数据再次训练。在这个第二轮训练过程中，我们使用第一轮提前终止训练确定的最佳步数。此过程有一些细微之处。例如，我们没有办法知道重新训练时，对参数进行相同次数的更新和对数据集进行相同次数的遍历哪一个更好。由于训练集变大了，在第二轮训练时，每一次遍历数据集将会更多次地更新参数。</p>

<div align="center">
    <img width="550" src="media/15272663505619/15472789633704.jpg" />
</div>

<p>另一个策略是保持从第一轮训练获得的参数，然后使用全部的数据继续训练。在这个阶段，已经没有验证集指导我们需要在训练多少步后终止。取而代之，我们 可以监控验证集的平均损失函数，并继续训练，直到它低于提前终止过程终止时的目标值。此策略避免了重新训练模型的高成本，但表现并没有那么好。例如，验证集的目标不一定能达到之前的目标值，所以这种策略甚至不能保证终止。</p>

<div align="center">
    <img width="550" src="media/15272663505619/15472790713118.jpg" />
</div>

<hr/>

<p><a href="">deep learn</a><br/>
<a href="https://www.jianshu.com/p/36312582478f">神经网络性能曲面与最优点</a><br/>
<a href="https://blog.csdn.net/zouxy09/article/details/24971995">机器学习中的范数规则化之（一）L0、L1与L2范数</a></p>

			</div>

		
	  
		<footer>
		 <p class="meta">

			<strong>Categories:</strong>&nbsp; 
			<span class="categories">
			
			    <a class='category' href='%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html'>神经网络</a>&nbsp;
			 
			</span>
		    </p>
		    <p class="meta">
		      
		 </p>
	    
		<div class="sharing">
		  
          

          

		</div>

	    <p class="meta">
	    
	        <a class="basic-alignment left" href="15279303888267.html" 
	        title="Previous Post: 随机梯度下降法的变化形式">&laquo; 随机梯度下降法的变化形式</a>
	    
	    
	        <a class="basic-alignment right" href="15264839728559.html" 
	        title="Next Post: 神经网络的代价函数">神经网络的代价函数 &raquo;</a>
	    
	    </p>
	  </footer>
	</article>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>