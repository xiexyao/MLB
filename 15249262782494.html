
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  神经网络 - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div>
	<article class="hentry" role="article">
	<header>
			  	<h1 class="entry-title">神经网络</h1>
				<p class="meta"><time datetime="2018-04-28T22:37:58+08:00" pubdate data-updated="true">2018/4/28</time></p>
			 </header>
		  	<div class="entry-content">
			  	<p>在介绍神经网络之前，我们先来介绍一些S型神经元。</p>

<h3 id="toc_0">S型神经元</h3>

<p>我们⽤描绘感知器的相同⽅式来描绘 S 型神经元：</p>

<div align="center">
    <img width="300" src="media/15249262782494/15368696909674.jpg" />
</div>

<p>正如一个感知器，S 型神经元有多个输入，\(x_1,x_2,...\)。但是这些输入可以取 0 和 1 中的任意值，不仅仅是 0 或 1。例如，0.638...是一个 S 型神经元的有效输入。同样，S 型神经元对每个输入有权重，\(w1,w2,...\)，和一个总的偏置，\(b\)。S 型神经元采用的激活函数是 S 型函数，定义为<br/>
\[<br/>
\sigma(z) = \frac{1}{1 + e^{-z}}<br/>
\]</p>

<p>把它们放在一起可以更清楚的说明，一个具有输入 \(x_1,x_2,...\)，权重 \(w_1,w_2,...\) 和偏置 \(b\) 的 S 型神经元的输出是<br/>
\[<br/>
\frac{1}{1 + \exp(-\sum_{j} w_j x_j - b)}<br/>
\]</p>

<p>初看上去，S 型神经元和感知器有很大区别，但是主要区别还是在 S 型函数上，我们可以看看 S 型函数的几何意义。假设 \(z=\mathbf w \mathbf x + b\) 是一个很大的正数，S 型神经元的输出近似为1；当 \(z=\mathbf w \mathbf x + b\) 是一个较大的负数，S 型神经元的输出近似为0；\(\sigma\) 的函数形状如下图：</p>

<div align="center">
    <img width=400 src="media/15249262782494/15368701242065.jpg" />
</div>

<p>很明显，感知器和 S 型神经元之间的一个很大的不同是 S 型神经元不仅仅输出 0 或 1。它可以是 0 到 1 之间的任何实数，比如 0.1773 等。但是有时候在一些分类问题上，输出为 0 或 1 是最简单的，但是如果输出的是一个浮点数，将没法判断，因此我们需要设定一个约定来解决这个问题，例如，约定任何至少为 0.5 的输出为表示真，小于 0.5 表示假。</p>

<h3 id="toc_1">神经网络</h3>

<p>假设我们有如下的网络结构</p>

<div align="center">
    <img width=500 src="media/15249262782494/15368703446770.jpg" />
</div>

<p>前面提过，这个网络中最左边的称为<strong>输入层</strong>，其中的神经元称为输入神经元。最右边的为<strong>输出层</strong>，包含有输出神经元。在本例中，输出层只有一个神经元。中间层，这层中的神经元既不是输入也不是输出，也被称为<strong>隐藏层</strong>。“隐藏”这个术语也许听上去有些神秘，但它实际上仅仅意味着“既非输入也非输出”。上面的网络仅有一个隐藏层，但有些网络有多个隐藏层。</p>

<p>由于历史的原因，尽管神经网络是由 S 型神经元而不是感知器构成，但是多层神经网络有时候会被称为<strong>多层感知器</strong>或者<strong>MLP</strong>。</p>

<p>神经网络的输入输出层通常是比较直接的。例如，假设我们尝试确定一张手写数字的图像上是否写的是“9”。很自然地，我们可以将图片像素的强度进行编码作为神经网络的输入。如果图像是一个 64 \(\times\) 64 的灰度图像，那么我们会需要 64 \(\times\) 64 = 4096 个输入神经元，每一个强度取 0 和 1 之间合适的值。输出层只需要包含一个神经元，当输出值小于 0.5时表示“输入图像不是9”，大于 0.5 时表示“输入图像是9”。</p>

<p>相比神经网络中输入输出层的直观设计，隐藏层的设计尤为重要，隐藏层层数和数量等选择的不同，直接会影响算法的效率。目前为止，我们讨论的神经网络都是以上一层的输出作为下一层的输入。这种网络被称为“<strong>前馈</strong>神经网络**。这意味着网络中是没有回路的，信息总是向前进行反馈。</p>

<p>然而后面我们也会学习一些神经网络，反馈环路是可行的，这被称之为递归神经网络。关于递归神经网络暂且不说。</p>

<h4 id="toc_2">神经网络的输出</h4>

<p>神经网络实际上就是一个输入向量 \(\vec x\) 到输出向量 \(\vec y\) 的函数，即<br/>
\[<br/>
\vec y = f_{network}(\vec x)<br/>
\]</p>

<p>接下来用一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。</p>

<div align="center">
    <img width="400" src="media/15249262782494/15368749617164.jpg" />
</div>

<p>如上图，输入层有三个节点，隐藏层有4个节点，输出层有2个节点。因为我们这个神经网络是全连接网络，所以每一层都与上一层的所有节点两两相连。假设隐藏节点4与输入层三个节点1、2、3之间都有连接，其连接上的权重分别为 \(w_{41}\)，\(w_{42}\)，\(w_{43}\)。现在来计算节点4的输出值。节点1、2、3是输入层，他们的输出值就是输入向量 \(\vec x\) 本身。假设节点1、2、3的输出值为 \(x_1,x_2,x_3\)。我们要求<strong>输入向量的维度与输入层神经元个数相同</strong>，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。</p>

<p>一旦我们有了节点1、2、3的输出值，我们就可以计算节点4的输出值 \(a_4\)：<br/>
\[<br/>
\begin{align*}<br/>
a_4 &amp;= \text{sigmoid}({\vec w}^T \vec x)\\<br/>
&amp;= \text{sigmoid}(w_{41} x_1 + w_{42} x_2 + w_{43} x_3 + b_4)<br/>
\end{align*}<br/>
\]</p>

<p>上式中 \(b_4\) 代表节点4的偏置项。同理我们可以计算节点5、6、7的输出值 \(a_5,a_6,a_7\)。这样，隐藏层4个节点的输出值就计算完成了，可以计算输出层的节点8的输出值 \(y_1\)：<br/>
\[<br/>
\begin{align*}<br/>
y_1 &amp;= \text{sigmoid}({\vec w}^T \vec a)\\<br/>
&amp;= \text{sigmoid}(w_{84} a_4 + w_{85} a_5 + w_{86} a_6 + w_{87} a_7 + b_8)\\<br/>
\end{align*}<br/>
\]</p>

<p>同理，还可以计算出 \(y_2\) 的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量 \(\vec x\) 为 \(\left [\begin{array}\\x_1\\x_2\\x_3\\\end{array}\right ]\) 时，神经网络的输出向量 \(\vec y = \left [ \begin{array}\\ y_1 \\y_2 \\\end{array}\right ]\)。显然，输出向量的维度等于输出神经元的个数。</p>

<h4 id="toc_3">神经网络的矩阵表示</h4>

<p>神经网络如果用矩阵表示会很方便，首先我们看隐藏层4个节点的计算（偏置项看成输入为1的权重，\(b_4\) 写成 \(w_{40}\)）：<br/>
\[<br/>
\begin{align*}<br/>
a_4 &amp;= \text{sigmoid}(w_{40}+w_{41} x_1 + w_{42} x_2 + w_{43} x_3 )\\<br/>
a_5 &amp;= \text{sigmoid}(w_{50}+w_{51} x_1 + w_{52} x_2 + w_{53} x_3 )\\<br/>
a_6 &amp;= \text{sigmoid}(w_{60}+w_{61} x_1 + w_{62} x_2 + w_{63} x_3 )\\<br/>
a_7 &amp;= \text{sigmoid}(w_{70}+w_{71} x_1 + w_{72} x_2 + w_{73} x_3 )\\<br/>
\end{align*}<br/>
\]</p>

<p>接着，输入层 \(\vec x\) 和隐藏层每个节点权重向量 \(\vec w_j\)。令<br/>
\[<br/>
\begin{align*}<br/>
\vec x &amp;= \left [ \begin{array}\\ 1\\x_1 \\x_2\\x_3\\\end{array} \right ]\\<br/>
\vec w_4 &amp;= [w_{40},w_{41},w_{42},w_{43}]\\<br/>
\vec w_5 &amp;= [w_{50},w_{51},w_{52},w_{53}]\\<br/>
\vec w_6 &amp;= [w_{60},w_{61},w_{62},w_{63}]\\<br/>
\vec w_7 &amp;= [w_{70},w_{71},w_{72},w_{73}]\\<br/>
\end{align*}<br/>
\]</p>

<p>令 \(f\) 表示 S 型 sigmoid 函数，代入前面式子得：<br/>
\[<br/>
a_4 = f(\vec w_4\cdot \vec x)\\<br/>
a_5 = f(\vec w_5\cdot \vec x)\\<br/>
a_6 = f(\vec w_6\cdot \vec x)\\<br/>
a_7 = f(\vec w_7\cdot \vec x)\\<br/>
\]</p>

<p>将 \(a_4,a_5,a_6,a_7\) 表示成矩阵，并将 \(\vec w_4,\vec w_5,\vec w_6,\vec w_7\) 表示成一个矩阵的形式，令<br/>
\[<br/>
\vec a = \left [\begin{array}\\a_4\\a_5\\a_6\\a_7\\\end{array}\right ],\quad W = \left [ \begin{array}\\\vec w_4\\\vec w_5\\\vec w_6\\\vec w_7\\\end{array}\right ] = \left [\begin{array}\\w_{40},w_{41},w_{42},w_{43}\\w_{50},w_{51},w_{52},w_{53}\\w_{60},w_{61},w_{62},w_{63}\\w_{70},w_{71},w_{72},w_{73}\\\end{array} \right ],\quad f\left(\begin{array}\\x_1\\x_2\\x_3\\\vdots\end{array}\right) = \left [\begin{array} \\f(x_1)\\f(x_2)\\f(x_3)\\\vdots\end{array} \right ]<br/>
\]</p>

<p>所以隐藏层计算可以写成<br/>
\[<br/>
\vec a = f(W \cdot \vec x)<br/>
\]</p>

<p>每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为 \(W_1,W_2,W_3,W_4\)，每个隐藏层的输出分别是 \(\vec a_1,\vec a_2,\vec a_3\)，神经网络的输入为 \(\vec x\)，神经网络的输入为 \(\vec y\)，如下图所示：</p>

<div align="center">
    <img width="500" src="media/15249262782494/15368748602021.jpg" />
</div>

<p>则每一层的输出向量的计算可以表示为<br/>
\[<br/>
\begin{align*}\\<br/>
\vec a_1 &amp;= f(W_1\cdot \vec x)\\<br/>
\vec a_2 &amp;= f(W_2\cdot \vec a_1)\\<br/>
\vec a_3 &amp;= f(W_3\cdot \vec a_2)\\<br/>
\vec y &amp;= f(W_4\cdot \vec a_3)<br/>
\end{align*}<br/>
\]</p>

<p>这就是神经网络输出值的计算方法。</p>

<h3 id="toc_4">神经网络超参数的估计</h3>

<p>以手写数字识别为例，首先需要确定网络的层数和每层的节点数。实际并没有什么理论指导，一般靠经验获取，或者可以多试几个值，训练不同层数的神经网络，看哪个结果最好。我们知道网络层数越多越好，也知道层数越多训练难度越大。对于全连接网络，隐藏层最好<strong>不要超过三层</strong>。</p>

<p>输入层节点数是确定的。因为手写数字数据集每个训练数据是28*28的图片，共784个像素，因此，输入层节点数应该是784，每个像素对应一个输入节点。</p>

<p>输出层节点数也是确定的。因为是10分类，我们可以用10个节点，每个节点对应一个分类。输出层10个节点中，输出最大值的那个节点对应的分类，就是模型的预测结果。</p>

<p>隐藏层节点数量是不好确定的，从1到100万都可以。下面有几个经验公式：<br/>
\[<br/>
\begin{align*}<br/>
m &amp;= \sqrt{n + l} + \alpha\\<br/>
m &amp;= \log_2 n\\<br/>
m &amp;= \sqrt{nl}\\<br/>
\end{align*}<br/>
\]</p>

<p>其中 \(m\) 表是隐藏层节点数，\(n\) 表示输入层节点数，\(l\) 表示输出层节点数，\(\alpha\) 表示1到10自己的常数。</p>

<p>因此，我们可以先根据上面的公式设置一个隐藏层节点数。如果有时间，我们可以设置不同的节点数，分别训练，看看哪个效果最好就用哪个。</p>

<p>假设设隐藏节点数为300，则对于3层 \(784\times 300 \times 10\) 的全连接网络，总共有 \(300\times(784+1) + 10 \times (300+1) = 238510 \) 个参数！神经网络之所以强大，是它提供了一种非常简单的方法去实现大量的参数。</p>

<hr/>

<p><a href="https://www.zybuluo.com/hanbingtao/note/476663">零基础入门深度学习(3) - 神经网络和反向传播算法</a></p>

			</div>

		
	  
		<footer>
		 <p class="meta">

			<strong>Categories:</strong>&nbsp; 
			<span class="categories">
			
			    <a class='category' href='%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html'>神经网络</a>&nbsp;
			 
			</span>
		    </p>
		    <p class="meta">
		      
		 </p>
	    
		<div class="sharing">
		  
          

          

		</div>

	    <p class="meta">
	    
	        <a class="basic-alignment left" href="15254702833482.html" 
	        title="Previous Post: 反向传播">&laquo; 反向传播</a>
	    
	    
	        <a class="basic-alignment right" href="15239854483603.html" 
	        title="Next Post: 感知器">感知器 &raquo;</a>
	    
	    </p>
	  </footer>
	</article>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>