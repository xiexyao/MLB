
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  矩阵求导 - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div>
	<article class="hentry" role="article">
	<header>
			  	<h1 class="entry-title">矩阵求导</h1>
				<p class="meta"><time datetime="2018-10-16T00:28:23+08:00" pubdate data-updated="true">2018/10/16</time></p>
			 </header>
		  	<div class="entry-content">
			  	<p>矩阵求导在机器学习中占据了相当的作用，而矩阵求导本身也难以理解。这里我们了解一下矩阵求导与迹的相关知识。</p>

<h4 id="toc_0">基础矩阵知识-迹</h4>

<p>在线性代数中，\(n\times n\) 方阵 \(A\) 的迹，是指 \(A\) 的主对角线各元素的总和（从左上方至右下方的对角线），例如：<br/>
\[<br/>
Tr(A) = A_{11} + A_{22} + \cdots + A_{nn} = \sum_{i=1}^n A_{ii}<br/>
\]</p>

<p>其中 \(A_{ij}\) 代表在 \(i\) 行 \(j\) 栏中的数值。同样的，元素的迹是其特征值的总和，使其不变量根据选择的基本准则而定。</p>

<p>矩阵迹的性质：</p>

<ol>
<li>常数的迹：\(Tr(a) = a\)</li>
<li>加减法：\(Tr(A\pm B) = Tr(A) \pm Tr(B)\)</li>
<li>转置：\(Tr(A^T) = Tr(A)\)</li>
<li>标量乘法：\(Tr(\alpha A) = \alpha Tr(A)\)</li>
<li>向量乘法：\(Tr(AB) = Tr(BA)\)</li>
<li><p>矩阵迹的求导：<br/>
\[<br/>
\nabla_A Tr(AB) = B^T<br/>
\]</p>

<p>证明：假设矩阵 \(A\) 的大小为 \(m\times n\)，矩阵 \(B\) 的大小为 \(n\times m\)，则<br/>
\[<br/>
\begin{align*}<br/>
\because Tr(AB) &amp;= \sum_{i=1}^m\sum_{j=1}^n A_{ij} B_{ji}\\<br/>
\therefore \frac{\partial Tr(AB)}{\partial A} &amp;= \left [ \begin{array}{cccc} \frac{\partial Tr(AB)}{\partial A_{11}} &amp; \frac{\partial Tr(AB)}{\partial A_{12}} &amp; \cdots &amp; \frac{\partial Tr(AB)}{\partial A_{1n}} \\ \frac{\partial Tr(AB)}{\partial A_{21}} &amp; \frac{\partial Tr(AB)}{\partial A_{22}} &amp; \cdots &amp; \frac{\partial Tr(AB)}{\partial A_{2n}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\  \frac{\partial Tr(AB)}{\partial A_{m1}} &amp; \frac{\partial Tr(AB)}{\partial A_{m2}} &amp; \cdots &amp; \frac{\partial Tr(AB)}{\partial A_{mn}}\end{array} \right ]\\<br/>
&amp;= \left [ \begin{array}{cccc} B_{11} &amp; B_{21} &amp; \cdots &amp; B_{n1}\\ B_{12} &amp; B_{22} &amp;\cdots &amp; B_{n2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ B_{1m} &amp; B_{2m} &amp; \cdots &amp; B_{nm} \end{array} \right ]\\<br/>
&amp;= B^T<br/>
\end{align*}<br/>
\]</p>

<p>得证。</p></li>
<li><p>\(\nabla_{A^T} f(A) = \big(\nabla_A f(X) \big)^T\)</p></li>
<li><p>\(\nabla_A Tr(ABA^T C) = CAB + C^TAB^T\)<br/>
证明：令 \(u(A) = AB\)，\(v(A^T) = A^T C\)，则<br/>
\[<br/>
\begin{align*}<br/>
\nabla_A Tr(ABA^T C) &amp;= \nabla_A Tr\big(u(A) v(A^T)\big) \\<br/>
&amp;= \nabla_{A:u(A)} Tr\big(u(A) v(A^T)\big) + \nabla_{A:v(A^T)} Tr\big(u(A) v(A^T) \big)\\<br/>
&amp;= \nabla_{A:u(A)} Tr\big(u(A) v(A^T)\big) + \Big(\nabla_{A^T:v(A^T)} Tr\big(u(A) v(A^T) \big)\Big)^T\\<br/>
&amp;= \big(v(A^T)\big)^T \nabla_{A}u(A) + \Big(\big(u(A)\big)^T \nabla_{A^T} v(A^T)\Big)^T\\<br/>
&amp;= C^TAB^T + \big(B^TA^T C^T\big)^T\\<br/>
&amp;= C^TAB^T + CAB<br/>
\end{align*}<br/>
\]</p></li>
<li><p>矩阵逐元素乘法：\(Tr\Big(A^T(B\odot C)\Big) = Tr\Big((A\odot B)^T C\Big)\)</p></li>
</ol>

<h4 id="toc_1">基础矩阵知识-内积</h4>

<p>向量 \(\mathbf x,\mathbf y \in \mathbb R^n\) 的内积定义为<br/>
\[<br/>
&lt;\mathbf x,\mathbf y&gt; = \mathbf x^T \mathbf y = \sum_{i=1}^n x_i y_i = Tr(\mathbf x^T \mathbf y) = Tr(\mathbf y\mathbf x^T)<br/>
\]</p>

<p>矩阵的 \(\mathbf X,\mathbf Y \in \mathbb R^{m\times n}\) 内积定义为<br/>
\[<br/>
&lt;\mathbf X,\mathbf Y&gt; = Tr(\mathbf X^T\mathbf Y) = \sum_{i=1}^m \sum_{j=1}^n X_{ij} Y_{ij}<br/>
\]</p>

<p>利用 Tr 的性质 \(Tr(\mathbf A\mathbf B) = Tr(\mathbf B\mathbf A)\) 和内积的定义，可以知道<br/>
\[<br/>
&lt;\mathbf A\mathbf X\mathbf B,\mathbf C&gt; = &lt;\mathbf X,\mathbf A^T\mathbf C\mathbf B^T&gt;<br/>
\]</p>

<p>证明：<br/>
\[<br/>
\begin{align*}<br/>
&lt;\mathbf A\mathbf X\mathbf B,\mathbf C&gt; &amp;= Tr\Big((\mathbf A\mathbf X\mathbf B)^T\mathbf C\Big) \\<br/>
&amp;= Tr(\mathbf B^T\mathbf X^T \mathbf A^T \mathbf C)\\<br/>
&amp;= Tr(\mathbf X^T \mathbf A^T \mathbf C \mathbf B^T)\\<br/>
&amp;= &lt;\mathbf X,\mathbf A^T\mathbf C\mathbf B^T&gt;\\<br/>
\end{align*}<br/>
\]</p>

<h4 id="toc_2">导数与微分的关系</h4>

<p>我们首先来看一下一元函数的导数与微分的联系:\(df = f&#39;(x) dx\)；多元微积分中的梯度（标量对向量的导数）也与微分有联系:<br/>
\[<br/>
df = \sum_{i=1}^n \frac{\partial f}{\partial x_i} dx_i = \frac{\partial f^T}{\partial \mathbf x} d\mathbf x<br/>
\]</p>

<p>这里第一个等号是全微分公式，第二个等号表达了梯度与微分的联系；全微分 \(df\) 是梯度向量 \(\frac{\partial f}{\partial \mathbf x}\quad (n\times 1)\) 与微分向量 \(d\mathbf x\quad (n\times 1)\) 的内积；受此启发，我们将矩阵导数与微分建立联系：<br/>
\[<br/>
df = \sum_{i=1}^m \sum_{j=1}^n \frac{\partial f}{\partial X_{ij}} dX_{ij} = Tr\Big( \frac{\partial f^T}{\partial X} dX\Big)<br/>
\]</p>

<p>其中 \(Tr\) 表示迹（trace）是方阵对角线元素之和，满足性质：对尺寸相同的矩阵A，B，\(Tr(A^TB) = \sum_{i,j} A_{ij} B_{ij}\)，，即 \(\text{tr}(A^TB)\) 是矩阵 \(A,B\) 的内积。与梯度相似，这里第一个等号是全微分公式，第二个等号表达了矩阵导数与微分的联系：全微分 \(df\) 是导数 \(\frac{\partial f}{\partial X}(m\times n)\) 与微分矩阵 \(dX(m\times n)\) 的内积。</p>

<p>我们来创立常用的矩阵微分的运算法则：</p>

<ol>
<li>加减法：\(d(X\pm Y) = dX \pm dY\)；矩阵乘法：\(d(XY) = (dX)Y + X dY\) ；转置：\(d(X^T) = (dX)^T\)；迹：\(d\text{Tr}(X) = \text{Tr}(dX)\)。</li>
<li><p>逆：\(dX^{-1} = -X^{-1}dX X^{-1}\)。此式可在 \(XX^{-1}=I\) 两侧求微分来证明。<br/>
证明：<br/>
\[<br/>
\begin{align*}<br/>
&amp;\because \quad XX^{-1} = I \\<br/>
&amp;\therefore \quad d(XX^{-1}) = dXX^{-1} + XdX^{-1} = 0\\<br/>
&amp;\Rightarrow \quad XdX^{-1} = -dXX^{-1} \\<br/>
&amp;\Rightarrow \quad dX^{-1} = -X^{-1}dXX^{-1}\\<br/>
\end{align*}<br/>
\]</p></li>
<li><p>行列式：\(d|X| = \text{tr}(X^{\#}dX)\) ，其中 \(X^{\#}\) 表示 \(X\) 的伴随矩阵，在 \(X\) 可逆时又可以写作 \(d|X|= |X|\text{tr}(X^{-1}dX)\)。</p></li>
<li><p>逐元素乘法：\(d(X\odot Y) = dX\odot Y + X\odot dY\)，\(\odot\) 表示尺寸相同的矩阵 \(X,Y\) 逐元素相乘。</p></li>
<li><p>逐元素函数：\(d\sigma(X) = \sigma&#39;(X)\odot dX\) ，\(\sigma(X) = \left[\sigma(X_{ij})\right]\) 是逐元素标量函数运算， \(\sigma&#39;(X)=[\sigma&#39;(X_{ij})]\) 是逐元素求导数。举个例子，<br/>
\[<br/>
X=\left[\begin{matrix}x_{11} &amp; x_{12} \\ x_{21} &amp; x_{22}\end{matrix}\right], d \sin(X) = \left[\begin{matrix}\cos x_{11} dx_{11} &amp; \cos x_{12} d x_{12}\\ \cos x_{21} d x_{21}&amp; \cos x_{22} dx_{22}\end{matrix}\right] = \cos(X)\odot dX<br/>
\]</p></li>
</ol>

<p>在建立法则的最后，来谈一谈复合：假设已求得 \(\frac{\partial f}{\partial Y}\) ，而 \(Y\) 是 \(X\) 的函数，如何求 \(\frac{\partial f}{\partial X}\) 呢？在微积分中有标量求导的链式法则 \(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial y}{\partial x}\)，但这里我们不能沿用链式法则，因为矩阵对矩阵的导数 \(\frac{\partial Y}{\partial X}\) 截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出 \(df = \text{tr}\left(\frac{\partial f}{\partial Y}^T dY\right)\)，再将 \(dY\) 用 \(dX\) 表示出来代入，并使用迹技巧将其他项交换至 \(dX\) 左侧，即可得到 \(\frac{\partial f}{\partial X}\)。</p>

<p>接下来演示一些算例。特别提醒要依据已经建立的运算法则来计算，不能随意套用微积分中标量导数的结论，比如认为 \(AX\) 对 \(X\) 的导数为 \(A\)，这是没有根据、意义不明的。</p>

<p>例1：\(f = \boldsymbol{a}^T X\boldsymbol{b}\)，求 \(\frac{\partial f}{\partial X}\)。其中 \(\boldsymbol{a}\) 是 \(m\times 1\) 列向量，\(X\) 是 \(m\times n\) 矩阵，\(\boldsymbol{b}\) 是 \(n\times 1\) 列向量，\(f\) 是标量。</p>

<p>解：先使用矩阵乘法法则求微分，这里的 \(\boldsymbol{a}\), \(\boldsymbol{b}\) 是常量，\(d\boldsymbol{a} = \boldsymbol{0}\), \(d\boldsymbol{b} = \boldsymbol{0}\)，得到：\(df = \boldsymbol{a}^T dX\boldsymbol{b}\)，再套上迹并做矩阵乘法交换：<br/>
\[<br/>
df = \text{tr}(\boldsymbol{a}^TdX\boldsymbol{b}) = \text{tr}(\boldsymbol{b}\boldsymbol{a}^TdX)<br/>
\]</p>

<p>对照导数与微分的联系 \(df = \text{tr}\left(\frac{\partial f}{\partial X}^T dX\right)\)，得到 \(\frac{\partial f}{\partial X} = (\boldsymbol{b}\boldsymbol{a}^T)^T= \boldsymbol{a}\boldsymbol{b}^T\)。</p>

<p>例2：\(f = \boldsymbol{a}^T \exp(X\boldsymbol{b})\)，求 \(\frac{\partial f}{\partial X}\)。其中\(\boldsymbol{a}\) 是 \(m\times 1\) 列向量，\(X\) 是 \(m\times n\) 矩阵，\(\boldsymbol{b}\) 是 \(n\times 1\) 列向量，\(f\) 是标量。</p>

<p>解：先使用矩阵乘法、逐元素函数法则求微分：\(df = \boldsymbol{a}^T(\exp(X\boldsymbol{b})\odot (dX\boldsymbol{b}))\)，再套上迹并做交换：<br/>
\[<br/>
\begin{align*}<br/>
df &amp;= \text{tr}( \boldsymbol{a}^T(\exp(X\boldsymbol{b})\odot (dX\boldsymbol{b}))) \\<br/>
&amp;=\text{tr}((\boldsymbol{a}\odot \exp(X\boldsymbol{b}))^TdX \boldsymbol{b}) \\<br/>
&amp;= \text{tr}(\boldsymbol{b}(\boldsymbol{a}\odot \exp(X\boldsymbol{b}))^TdX)<br/>
\end{align*}<br/>
\]</p>

<p>对照导数与微分的联系 \(df = \text{tr}\left(\frac{\partial f}{\partial X}^T dX\right)\)，得到 <br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial X} &amp;= (\boldsymbol{b}(\boldsymbol{a}\odot \exp(X\boldsymbol{b}))^T)^T \\<br/>
&amp;= (\boldsymbol{a}\odot \exp(X\boldsymbol{b}))\boldsymbol{b}^T\\<br/>
\end{align*}<br/>
\]</p>

<p>例3：\(f = \text{Tr}(Y^T M Y)\), \(Y = \sigma(WX)\)，求 \(\frac{\partial f}{\partial X}\)。其中 \(W\) 是 \(l\times m\) 矩阵，\(X\) 是 \(m\times n\) 矩阵，\(Y\) 是 \(l\times n\) 矩阵，\(M\) 是 \(l\times l\) 对称矩阵，\(\sigma\) 是逐元素函数，\(f\) 是标量。<br/>
解：先求 \(\frac{\partial y}{\partial Y}\)，用矩阵乘法和转置法则：<br/>
\[<br/>
\begin{align*}<br/>
\because f &amp;= \text{Tr}(Y^T M Y)\\<br/>
\Rightarrow df &amp;= d\text{Tr}(Y^T M Y) = \text{Tr}(d(Y^TMY))\\<br/>
&amp;= \text{Tr}((dY)^TMY) + \text{Tr}(Y^TMdY)\\<br/>
&amp;= \text{Tr}(Y^TM^TdY) + \text{Tr}(Y^TMdY)\qquad \because \text{M是对称矩阵}\\<br/>
&amp;= 2\text{Tr}(Y^TMdY)<br/>
\end{align*}<br/>
\]</p>

<p>对照微分和导数的关系 \(df = \text{Tr}\Big (\frac{\partial f^T}{\partial Y} dY\Big)\)，所以有：<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial Y} = (2 Y^T M)^T = 2MY<br/>
\end{align*}<br/>
\]</p>

<p>现在再来看 \(Y = \sigma(WX)\)，可知：\(dY = \sigma&#39;(WX)\odot (WdX)\)，代入 \(df\)，得：<br/>
\[<br/>
\begin{align*}<br/>
df &amp;= \text{Tr}\Big(\frac{\partial f^T}{\partial Y}\big(\sigma&#39;(WX)\odot (WdX)\big)\Big)\\<br/>
&amp;= \text{Tr}\Big(\big(\frac{\partial f}{\partial Y} \odot \sigma&#39;(WX)\big )^T WdX \Big) \\<br/>
\end{align*}<br/>
\]</p>

<p>对照导数与微分的联系，得到<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial X} &amp;= \Big(\big(\frac{\partial f}{\partial Y} \odot \sigma&#39;(WX)\big )^T W \Big)^T\\<br/>
&amp;= W^T \big(\frac{\partial f}{\partial Y} \odot \sigma&#39;(WX)\big )\\<br/>
&amp;= W^T \big(2MY\odot \sigma&#39;(WX)\big)\\<br/>
&amp;= W^T \big(2M\sigma(WX)\odot \sigma&#39;(WX)\big) \\<br/>
\end{align*}<br/>
\]</p>

<p>例4【线性回归】：\(l = \|X\boldsymbol{w}- \boldsymbol{y}\|^2\)， 求\(\boldsymbol{w}\) 的最小二乘估计，即求 \(\frac{\partial l}{\partial \boldsymbol{w}}\) 的零点。其中 \(\boldsymbol{y}\) 是 \(m\times 1\) 列向量，\(X\) 是 \(m\times n\) 矩阵，\(\boldsymbol{w}\) 是 \(n×1\) 列向量，\(l\) 是标量。<br/>
解：关于 \(l\) 的模的平方可以写为向量与自身的内积，即<br/>
\[<br/>
l = (X\boldsymbol{w} - \boldsymbol{y})^T(X\boldsymbol{w} - \boldsymbol{y})<br/>
\]</p>

<p>所以，它的微分为<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \big(d(X\boldsymbol{w} - \boldsymbol{y})^T\big)(X\boldsymbol{w} - \boldsymbol{y})+ (X\boldsymbol w - \boldsymbol y)^T d(X\boldsymbol w - \boldsymbol x)\\<br/>
&amp;= (Xd\boldsymbol w)^T(X\boldsymbol{w} - \boldsymbol{y}) + (X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w)\\<br/>
&amp;= \big((X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w)\big)^T + (X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w)\\<br/>
\because \quad &amp; (X\boldsymbol w - \boldsymbol x) \in \mathcal A_{m\times 1}\qquad (Xd\boldsymbol w)\in \mathcal A_{m\times 1}\\<br/>
\therefore \quad &amp; (X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w) \in \mathcal A_{1\times 1}\\<br/>
dl &amp;= 2(X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w)<br/>
\end{align*}<br/>
\]</p>

<p>按照微分和导数的关系：<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= Tr\big(\frac{\partial f^T}{\partial \boldsymbol w} d\boldsymbol w\big)\\<br/>
\end{align*}<br/>
\]</p>

<p>可知：<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial \boldsymbol w} &amp; = \big( 2(X\boldsymbol w - \boldsymbol y)^T X\big)^T\\<br/>
&amp;= 2X^T(X\boldsymbol w - \boldsymbol y)<br/>
\end{align*}<br/>
\]</p>

<p>\(\frac{\partial l}{\partial \boldsymbol{w}}\) 的零点即\(\boldsymbol{w}\) 的最小二乘估计为 \(\boldsymbol{w} = (X^TX)^{-1}X^T\boldsymbol{y}\)</p>

<p>例5【方差的最大似然估计】：样本 \(\boldsymbol{x}_1,\dots, \boldsymbol{x}_N \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)\)，求方差 \(\Sigma\) 的最大似然估计。写成数学式是：\(l = \log|\Sigma|+\frac{1}{N}\sum_{i=1}^N(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T\Sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\bar{x}})\)，求 \(\frac{\partial l }{\partial \Sigma}\) 的零点。其中 \(\boldsymbol{x}_i\) 是 \(m\times 1\) 列向量，\(\bar{\boldsymbol{x}}=\frac{1}{N}\sum_{i=1}^N \boldsymbol{x}_i\) 是样本均值，\(\Sigma\) 是 \(m\times m\) 对称正定矩阵，\(l\) 是标量，\(<br/>
\log\)表示自然对数。<br/>
解：首先求微分，使用矩阵乘法、行列式、逆等运算法则，第一项是：<br/>
\[<br/>
\begin{align*}<br/>
d\log|\Sigma| &amp;= |\Sigma|^{-1} \odot d|\Sigma|\\<br/>
&amp;= |\Sigma|^{-1} \odot \Big(|\Sigma|\text{Tr}(\Sigma^{-1} d\Sigma)\Big)\\<br/>
&amp;= \text{Tr}(\Sigma^{-1} d\Sigma) \qquad \because \text{Tr(*) is Scalar}<br/>
\end{align*}<br/>
\]</p>

<p>再来看第二项，<br/>
\[<br/>
\begin{align*}<br/>
d \Big(\frac{1}{N}\sum_{i=1}^N(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T\Sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\bar{x}})\Big) &amp;= \frac 1 N \sum_{i=1}^N(\boldsymbol x_i - \boldsymbol{\bar{x}})^T d(\Sigma^{-1}) (\boldsymbol x_i - \boldsymbol{\bar{x}})\\<br/>
&amp;= -\frac 1 N \sum_{i=1}^N(\boldsymbol x_i - \boldsymbol{\bar{x}})^T \Sigma^{-1} d\Sigma \Sigma^{-1}) (\boldsymbol x_i - \boldsymbol{\bar{x}})\\<br/>
\end{align*}<br/>
\]</p>

<p>给第二项套上迹然后做交换：<br/>
\[<br/>
\begin{align*}<br/>
\text{Tr}\Big(-\frac 1 N \sum_{i=1}^N(\boldsymbol x_i - \boldsymbol{\bar{x}})^T \Sigma^{-1} d\Sigma \Sigma^{-1}) (\boldsymbol x_i - \boldsymbol{\bar{x}})\Big) &amp;= -\frac 1 N \sum_{i=1}^N \text{Tr}\Big((\boldsymbol x_i - \boldsymbol{\bar{x}})^T \Sigma^{-1}d\Sigma\Sigma^{-1} (\boldsymbol x_i - \boldsymbol{\bar{x}})\Big)\\<br/>
&amp;= -\frac 1 N \sum_{i=1}^N \text{Tr}\Big(\Sigma^{-1} (\boldsymbol x_i - \boldsymbol{\bar{x}})(\boldsymbol x_i - \boldsymbol{\bar{x}})^T \Sigma^{-1}d\Sigma\Big)\\<br/>
\end{align*}<br/>
\]</p>

<p>令 \(S = \frac1 N \sum{i=1}^N (\boldsymbol x_i - \boldsymbol{\bar{x}})(\boldsymbol x_i - \boldsymbol{\bar{x}})^T\)，所以第二项可以写为：\(-\text{Tr}\Big(\Sigma^{-1} S \Sigma^{-1}d\Sigma\Big)\)，结合第一项得到：<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \text{Tr}(\Sigma^{-1} d\Sigma) - \text{Tr}\big(\Sigma^{-1} S \Sigma^{-1}d\Sigma\big) \\<br/>
&amp;= \text{Tr}\Big(\big(\Sigma^{-1} - \Sigma^{-1} S \Sigma^{-1} \big)d\Sigma\Big)\\<br/>
\end{align*}<br/>
\]</p>

<p>对照微分与导数的关系：<br/>
\[<br/>
dl = \text{Tr}\Big(\frac{\partial l^T}{\partial \Sigma} d\Sigma \Big)<br/>
\]</p>

<p>可知<br/>
\[<br/>
\frac{\partial l}{\partial \Sigma} = \big(\Sigma^{-1} - \Sigma^{-1} S \Sigma^{-1} \big)^T<br/>
\]</p>

<p>其零点也就是 \(S = \Sigma\) 的时候。</p>

<p>例6【多元logistic回归】：\(l = -\boldsymbol{y}^T\log\text{softmax}(W\boldsymbol{x})\)，求 \(\frac{\partial l}{\partial W}\)。其中\(\boldsymbol{y}\) 是除一个元素为1外其它元素为0的 \(m\times 1\) 列向量，\(W\) 是 \(m\times n\) 矩阵，\(\boldsymbol{x}\) 是 \(n\times 1\) 列向量，\(l\) 是标量；\(\log\) 表示自然对数，\(\text{softmax}(\boldsymbol{a}) = \frac{\exp(\boldsymbol{a})}{\boldsymbol{1}^T\exp(\boldsymbol{a})}\)，其中 \(\exp(\boldsymbol{a})\) 表示逐元素求指数，\(\boldsymbol{1}\) 代表全1向量。<br/>
解1：首先将 softmax 代入 \(l\) 中，得：<br/>
\[<br/>
\begin{align*}<br/>
l &amp;= -\boldsymbol{y}^T\log \frac{\exp(W\boldsymbol{x})}{\boldsymbol{1}^T\exp(W\boldsymbol{x})}\\<br/>
&amp;= -\boldsymbol{y}^T \Big(\log\exp(W\boldsymbol{x}) -\boldsymbol{1} \log\big(\boldsymbol{1}^T \exp(W\boldsymbol{x})\big)\Big) \\<br/>
&amp;= -\boldsymbol{y}^TW\boldsymbol{x} + \log\big(\boldsymbol{1}^T \exp(W\boldsymbol{x})\big)<br/>
\end{align*}<br/>
\]</p>

<p>这里需要注意两点，一个是\(\log{\boldsymbol{x}/y} = \log \boldsymbol{x} - \boldsymbol{1} \log y\)，另一个是 \(\boldsymbol{y}^T \boldsymbol{1} = 1\)。现在求微分，利用矩阵乘法和逐乘法得<br/>
\[<br/>
dl = -\boldsymbol{y}^T dW\boldsymbol x + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \Big(\boldsymbol{1}^T \big(\exp(W\boldsymbol x)\odot (dW\boldsymbol x)\big)\Big)\\<br/>
\]</p>

<p>再套上迹作交换<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \text{Tr}\bigg( -\boldsymbol{y}^T dW\boldsymbol x + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \Big(\boldsymbol{1}^T \big(\exp(W\boldsymbol x)\odot (dW\boldsymbol x)\big)\Big) \bigg)\\<br/>
&amp;= \text{Tr}\bigg( -\boldsymbol{y}^T dW\boldsymbol x + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \Big(\big(\boldsymbol{1}\odot\exp(W\boldsymbol x)\big)^T (dW\boldsymbol x)\Big) \bigg)\\<br/>
&amp;= \text{Tr}\bigg( -\boldsymbol{y}^T dW\boldsymbol x + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \Big(\exp(W\boldsymbol x)^T (dW\boldsymbol x)\Big) \bigg)\\<br/>
&amp;= \text{Tr}\bigg( -\boldsymbol x \big( \boldsymbol y^T + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \exp(W\boldsymbol x)^T  \big) dW\bigg)<br/>
\end{align*}<br/>
\]</p>

<p>最后一步是利用迹的可交换性得到，此时按照微分和导数的关系：<br/>
\[<br/>
dl = \text{Tr}(\frac{\partial l^T}{\partial W} dW)^T<br/>
\]</p>

<p>所以有<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial l}{\partial W} &amp;= \bigg( -\boldsymbol x \big( \boldsymbol y^T - \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \exp(W\boldsymbol x)^T  \big)\bigg)^T\\<br/>
&amp;= \bigg( -\boldsymbol x \big( \boldsymbol y^T - \text{softmax}(W\boldsymbol x)^T  \big)\bigg)^T\\<br/>
&amp;= \big(\text{softmax}(W\boldsymbol x) - \boldsymbol y\big) \boldsymbol x^T<br/>
\end{align*}<br/>
\]</p>

<p>解2：定义 \(\boldsymbol{a} = W\boldsymbol{x}\)，则 \(l = -\boldsymbol{y}^T\log\text{softmax}(\boldsymbol{a})\) ，先同上求出 \(\frac{\partial l}{\partial \boldsymbol{a}} = \text{softmax}(\boldsymbol{a})-\boldsymbol{y}\)，再利用复合法则：\(dl = \text{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}}^Td\boldsymbol{a}\right) = \text{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}}^TdW \boldsymbol{x}\right) = \text{tr}\left(\boldsymbol{x}\frac{\partial l}{\partial \boldsymbol{a}}^TdW\right)\)，得到\(\frac{\partial l}{\partial W}= \frac{\partial l}{\partial\boldsymbol{a}}\boldsymbol{x}^T\)。</p>

<p>例7【二层神经网络】：\(l = -\boldsymbol{y}^T\log\text{softmax}(W_2\sigma(W_1\boldsymbol{x}))\) ，求 \(\frac{\partial l}{\partial W_1}和\frac{\partial l}{\partial W_2}\)。其中 \(\boldsymbol{y}\) 是除一个元素为1外其它元素为0的的 \(m\times 1\) 列向量，\(W_2\) 是 \(m\times p\) 矩阵，\(W_1\) 是 \(p \times n\) 矩阵，\(\boldsymbol{x}\) 是 \(n\times 1\) 列向量，l是标量；\(\log\) 表示自然对数，\(\text{softmax}(\boldsymbol{a}) = \frac{\exp(\boldsymbol{a})}{\boldsymbol{1}^T\exp(\boldsymbol{a})}\) 。同上，\(\sigma\) 是逐元素 \(\text{sigmoid}\) 函数 \(\sigma(a) = \frac{1}{1+\exp(-a)}\)。</p>

<p>解：定义 \(\boldsymbol{a}_1=W_1\boldsymbol{x}\)，\(\boldsymbol{h}_1 = \sigma(\boldsymbol{a}_1)\)，\(\boldsymbol{a}_2 = W_2 \boldsymbol{h}_1\)，则 \(l =-\boldsymbol{y}^T\log\text{softmax}(\boldsymbol{a}_2)\)。上例中已经求出 \(\frac{\partial l}{\partial \boldsymbol{a}_2} = \text{softmax}(\boldsymbol{a}_2)-\boldsymbol{y}\)。使用复合法则 <br/>
\[<br/>
dl = \text{Tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^Td\boldsymbol{a}_2\right) = \text{Tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^TdW_2 \boldsymbol{h}_1\right) + \underbrace{ \text{Tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^TW_2 d\boldsymbol{h}_1\right)}_{dl_2}<br/>
\]</p>

<p>使用矩阵乘法交换的迹技巧从第一项得到 <br/>
\[<br/>
\frac{\partial l}{\partial W_2}= \frac{\partial l}{\partial\boldsymbol{a}_2}\boldsymbol{h}_1^T<br/>
\]</p>

<blockquote>
<p>因为 \(\boldsymbol a_2 = W_2 \boldsymbol h_1\)，所以 \(d\boldsymbol a_2 = (dW_2) \boldsymbol h_1\)，所以 \(dl = \text{Tr}(\frac{l}{\partial \boldsymbol a_2}^T d\boldsymbol a_2)\) ，将 \(d\boldsymbol a_2\) 代入可得 <br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \text{Tr}\Big(\frac{l}{\partial \boldsymbol a_2}^T (dW_2)\boldsymbol h_1\Big)^T\\<br/>
&amp;= \text{Tr}\Big(\boldsymbol h_1 \frac{l}{\partial \boldsymbol a_2}^T dW_2\Big) \\<br/>
\end{align*}<br/>
\]</p>

<p>对比微分和导数的关系 \(dl = \text{Tr}(\frac{l}{\boldsymbol W_2}^T dW_2)\)，可得<br/>
\[<br/>
\frac{l}{\boldsymbol W_2} = \Big(\boldsymbol h_1 \frac{l}{\partial \boldsymbol a_2}^T \Big)^T = \frac{l}{\partial \boldsymbol a_2} \boldsymbol h_1^T<br/>
\]</p>
</blockquote>

<p>从第二项得到 \(\frac{\partial l}{\partial \boldsymbol{h}_1}= W_2^T\frac{\partial l}{\partial\boldsymbol{a}_2}\)。接下来对第二项继续使用复合法则来求 \(\frac{\partial l}{\partial \boldsymbol{a}_1}\)，并利用矩阵乘法和逐元素乘法交换的迹技巧：<br/>
\[<br/>
\begin{align*}<br/>
dl_2 = \text{Tr}\left(\frac{\partial l}{\partial\boldsymbol{h}_1}^Td\boldsymbol{h}_1\right) \\<br/>
&amp;= \text{Tr}\left(\frac{\partial l}{\partial\boldsymbol{h}_1}^T(\sigma&#39;(\boldsymbol{a}_1)\odot d\boldsymbol{a}_1)\right) \\<br/>
&amp;= \text{Tr}\left(\left(\frac{\partial l}{\partial\boldsymbol{h}_1}\odot \sigma&#39;(\boldsymbol{a}_1)\right)^Td\boldsymbol{a}_1\right)\\<br/>
\end{align*}<br/>
\]</p>

<p>得到 \(\frac{\partial l}{\partial \boldsymbol{a}_1}= \frac{\partial l}{\partial\boldsymbol{h}_1}\odot\sigma&#39;(\boldsymbol{a}_1)\)。为求 \(\frac{\partial l}{\partial W_1}\)，再用一次复合法则：<br/>
\[<br/>
\begin{align*}<br/>
dl_2 &amp;= \text{Tr}\left(\frac{\partial l}{\partial\boldsymbol{a}_1}^Td\boldsymbol{a}_1\right) \\<br/>
&amp;= \text{tr}\left(\frac{\partial l}{\partial\boldsymbol{a}_1}^TdW_1\boldsymbol{x}\right) \\<br/>
&amp;= \text{tr}\left(\boldsymbol{x}\frac{\partial l}{\partial\boldsymbol{a}_1}^TdW_1\right)<br/>
\end{align*}<br/>
\]</p>

<p>得到 \(\frac{\partial l}{\partial W_1}= \frac{\partial l}{\partial\boldsymbol{a}_1}\boldsymbol{x}^T\)。</p>

<p>推广：样本 \((\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_N,y_N)\)，\(l = -\sum_{i=1}^N \boldsymbol{y}_i^T\log\text{softmax}(W_2\sigma(W_1\boldsymbol{x}_i + \boldsymbol{b}_1) + \boldsymbol{b}_2)\)，其中 \(\boldsymbol{b}_1\) 是 \(p \times 1\) 列向量，\(\boldsymbol{b}_2\) 是 \(m\times 1\) 列向量，其余定义同上。</p>

<p>解1：定义 \(\boldsymbol{a}_{1,i} = W_1 \boldsymbol{x}_i + \boldsymbol{b}_1\)，\(\boldsymbol{h}_{1,i} = \sigma(\boldsymbol{a}_{1,i})\)，\(\boldsymbol{a}_{2,i} = W_2\boldsymbol{h}_{1,i} + \boldsymbol{b}_2\)，则 \(l = -\sum_{i=1}^N \boldsymbol{y}_i^T \log \text{softmax}(\boldsymbol{a}_{2,i})\)。先同上可求出 \(\frac{\partial l}{\partial \boldsymbol{a}_{2,i}} = \text{softmax}(\boldsymbol{a}_{2,i})-\boldsymbol{y}_i\) 。使用复合法则，<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \text{tr}\left(\sum_{i=1}^N\frac{\partial l}{\partial \boldsymbol{a}_{2,i}}^T d \boldsymbol{a}_{2,i}\right) \\<br/>
&amp;= \text{tr}\left( \sum_{i=1}^N \frac{\partial l}{\partial \boldsymbol{a}_{2,i}}^T dW_2 \boldsymbol{h}_{1,i}\right) + \underbrace{\text{tr}\left( \sum_{i=1}^N \frac{\partial l}{\partial \boldsymbol{a}_{2,i}}^T W_2 d\boldsymbol{h}_{1,i}\right)}_{dl_2} + \text{tr}\left( \sum_{i=1}^N \frac{\partial l}{\partial \boldsymbol{a}_{2,i}}^T d \boldsymbol{b}_2\right)，<br/>
\end{align*}<br/>
\]</p>

<p>从第一项得到得到 \(\frac{\partial l}{\partial W_2}= \sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{2,i}}\boldsymbol{h}_{1,i}^T\)，从第二项得到 \(\frac{\partial l}{\partial \boldsymbol{h}_{1,i}}= W_2^T\frac{\partial l}{\partial\boldsymbol{a}_{2,i}}\)，从第三项得到到 \(\frac{\partial l}{\partial \boldsymbol{b}_2}= \sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{2,i}}\)。接下来对第二项继续使用复合法则，得到 \(\frac{\partial l}{\partial \boldsymbol{a}_{1,i}}= \frac{\partial l}{\partial\boldsymbol{h}_{1,i}}\odot\sigma&#39;(\boldsymbol{a}_{1,i})\)。为求 \(\frac{\partial l}{\partial W_1}, \frac{\partial l}{\partial \boldsymbol{b}_1}\)，再用一次复合法则：\(dl_2 = \text{tr}\left(\sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}^Td\boldsymbol{a}_{1,i}\right) = \text{tr}\left(\sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}^TdW_1\boldsymbol{x}_i\right) + \text{tr}\left(\sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}^Td\boldsymbol{b}_1\right)\)，得到 \(\frac{\partial l}{\partial W_1}= \sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}\boldsymbol{x}_i^T，\frac{\partial l}{\partial \boldsymbol{b}_1}= \sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}\)。</p>

<p>解2：可以用矩阵来表示 \(N\) 个样本，以简化形式。定义 \(X = [\boldsymbol{x}_1, \cdots, \boldsymbol{x}_N]，A_1 = [\boldsymbol{a}_{1,1},\cdots,\boldsymbol{a}_{1,N}] =W_1 X + \boldsymbol{b}_1 \boldsymbol{1}^T，H_1 = [\boldsymbol{h}_{1,1}, \cdots, \boldsymbol{h}_{1,N}] = \sigma(A_1)，A_2 = [\boldsymbol{a}_{2,1},\cdots,\boldsymbol{a}_{2,N}] = W_2 H_1 + \boldsymbol{b}_2 \boldsymbol{1}^T\)，注意这里使用全1向量来扩展维度。先同上求出 \(\frac{\partial l}{\partial A_2} = [\text{softmax}(\boldsymbol{a}_{2,1})-\boldsymbol{y}_1, \cdots, \text{softmax}(\boldsymbol{a}_{2,N})-\boldsymbol{y}_N]\) 。使用复合法则，\(dl = \text{tr}\left(\frac{\partial l}{\partial A_2}^T d A_2\right) = \text{tr}\left( \frac{\partial l}{\partial A_2}^T dW_2 H_1 \right) + \underbrace{\text{tr}\left(\frac{\partial l}{\partial A_2}^T W_2 d H_1\right)}_{dl_2} + \text{tr}\left(\frac{\partial l}{\partial A_2}^T d \boldsymbol{b}_2 \boldsymbol{1}^T\right)\) ，从第一项得到 \(\frac{\partial l}{\partial W_2}= \frac{\partial l}{\partial A_2}H_1^T\)，从第二项得到 \(\frac{\partial l}{\partial H_1}= W_2^T\frac{\partial l}{\partial A_{2}}\)，从第三项得到到 \(\frac{\partial l}{\partial \boldsymbol{b}_2}= \frac{\partial l}{\partial A_2}\boldsymbol{1}\)。接下来对第二项继续使用复合法则，得到 \(\frac{\partial l}{\partial A_1}= \frac{\partial l}{\partial H_1}\odot\sigma&#39;(A_1)。为求\frac{\partial l}{\partial W_1}, \frac{\partial l}{\partial \boldsymbol{b}_1}\)，再用一次复合法则：\(dl_2 = \text{tr}\left(\frac{\partial l}{\partial A_1}^TdA_1\right) = \text{tr}\left(\frac{\partial l}{\partial A_1}^TdW_1X\right) + \text{tr}\left( \frac{\partial l}{\partial A_1}^Td\boldsymbol{b}_1 \boldsymbol{1}^T\right)\)，得到 \(\frac{\partial l}{\partial W_1}=  \frac{\partial l}{\partial A_1}X^T，\frac{\partial l}{\partial \boldsymbol{b}_1}= \frac{\partial l}{\partial A_1}\boldsymbol{1}\)。</p>

<h4 id="toc_3">矩阵对矩阵求导</h4>

<p>我们先定义向量 \(\mathbf f(p\times 1)\) 对向量 \(\mathbf x(m\times 1)\) 的导数 \(\frac{\partial \mathbf f}{\partial \mathbf x} = \left [\begin{array}{cccc} \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_1} \\ \frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_1}{\partial x_m} &amp; \frac{\partial f_2}{\partial x_m} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_m} \\ \end{array} \right ]\)\((m\times p)\)，有 \(d\mathbf f = \frac{\partial \mathbf f}{\partial \mathbf x}^T d\mathbf x\)；再定义矩阵的向量化（按列优先）：\(\text{vec}(X) = [X_{11},\cdots,X_{m1},X_{12},\cdots,X_{m2},\cdots,X_{1n},\cdots,X_{mn}]^T\)\((mn\times 1)\)，并定义矩阵 \(F(p\times q)\) 对矩阵 \(X(m\times n)\) 的导数 \(\frac{\partial F}{\partial X} = \frac{\text{vec}(F)}{\text{vec}(X)}\)\((mn\times pq)\)。导数与微分有联系 \(\text{vec}(dF) = \frac{\partial F}{\partial X}^T \text{vec}(dX)\)。几点说明如下：</p>

<ol>
<li>按此定义，标量 \(f\) 对矩阵 \(X(m\times n)\) 的导数 \(\frac{\partial f}{\partial X}\) 是 \(mn\times 1\) 向量，与上面的定义不兼容，不过二者容易相互转换。为避免混淆，用记号 \(\nabla_X f\) 表示上篇定义的 \(m\times n\) 矩阵，则有 \(\frac{\partial f}{\partial X}=\mathrm{vec}(\nabla_X f)\)。虽然本篇的技术可以用于标量对矩阵求导这种特殊情况，但使用上篇中的技术更方便。</li>
<li>标量对矩阵的二阶导数，又称 Hessian 矩阵，定义为 \(\nabla^2_X f = \frac{\partial^2 f}{\partial X^2} = \frac{\partial \nabla_X f}{\partial X}\)\((mn\times mn)\)，是对称矩阵。对向量 \(\frac{\partial f}{\partial X}\) 或矩阵 \(\nabla_X f\) 求导都可以得到 Hessian 矩阵，但从矩阵 \(\nabla_X f\) 出发更方便。</li>
<li>\(\frac{\partial F}{\partial X} = \frac{\partial\mathrm{vec} (F)}{\partial X} = \frac{\partial F}{\partial \mathrm{vec}(X)} = \frac{\partial\mathrm{vec}(F)}{\partial \mathrm{vec}(X)}\)，求导时矩阵被向量化，弊端是这在一定程度破坏了矩阵的结构，会导致结果变得形式复杂；好处是多元微积分中关于梯度、Hessian矩阵的结论可以沿用过来，只需将矩阵向量化。例如优化问题中，牛顿法的更新 \(\Delta X\)，满足 \(\mathrm{vec}(\Delta X) = -(\nabla^2_X f)^{-1}\mathrm{vec}(\nabla_X f)\)。</li>
<li>在资料中，矩阵对矩阵的导数还有其它定义，比如 \(\frac{\partial F}{\partial X} = \left[\frac{\partial F_{kl}}{\partial X}\right](mp×nq)\)，它能兼容上篇中的标量对矩阵导数的定义，但微分与导数的联系（\(dF\) 等于 \(\frac{\partial F}{\partial X}\) 中每个 \(m\times n\) 子块分别与 \(dX\) 做内积）不够简明，不便于计算和应用。</li>
</ol>

<p>然后来建立运算法则。仍然要利用导数与微分的联系 \(\mathrm{vec}(dF) = \frac{\partial F}{\partial X}^T \mathrm{vec}(dX)\)，求微分的方法与上篇相同，而从微分得到导数需要一些向量化的技巧：</p>

<ol>
<li>线性：\(\mathrm{vec}(A+B) = \mathrm{vec}(A) + \mathrm{vec}(B)\)。</li>
<li>矩阵乘法：\(\mathrm{vec}(AXB) = (B^T \otimes A) \mathrm{vec}(X)\)，其中 \(\otimes\) 表示Kronecker积，\(A\)\((m\times n)\) 与 \(B\)\((p\times q)\) 的Kronecker积是 \(A\otimes B = [A_{ij}B]\)\((mp\times nq)\)。</li>
<li>转置：\(\mathrm{vec}(A^T) = K_{mn}\mathrm{vec}(A)\)，\(A\) 是 \(m\times n\) 矩阵，其中 \(K_{mn}\)\((mn\times mn)\) 是交换矩阵(commutation matrix)。</li>
<li>逐元素乘法：\(\mathrm{vec}(A\odot X) = \mathrm{diag}(A)\mathrm{vec}(X)\)，其中 \(\mathrm{diag}(A)\)\((mn\times mn)\) 是用 \(A\) 的元素（按列优先）排成的对角阵。</li>
</ol>

<p>观察一下可以断言，若矩阵函数 \(F\) 是矩阵 \(X\) 经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对 \(F\) 求微分，再做向量化并使用技巧将其它项交换至 \(\text{vec}(dX)\) 左侧，即能得到导数。</p>

<p>再谈一谈复合：假设已求得 \(\frac{\partial F}{\partial Y}\)，而 \(Y\) 是 \(X\) 的函数，如何求 \(\frac{\partial F}{\partial X}\) 呢？从导数与微分的联系入手，\(\mathrm{vec}(dF) = \frac{\partial F}{\partial Y}^T\mathrm{vec}(dY) = \frac{\partial F}{\partial Y}^T\frac{\partial Y}{\partial X}^T\mathrm{vec}(dX)\) ，可以推出链式法则 \(\frac{\partial F}{\partial X} = \frac{\partial Y}{\partial X}\frac{\partial F}{\partial Y}\)。</p>

<p>和标量对矩阵的导数相比，矩阵对矩阵的导数形式更加复杂，从不同角度出发常会得到形式不同的结果。有一些Kronecker积和交换矩阵相关的恒等式，可用来做等价变形：\((A\otimes B)^T = A^T \otimes B^T\)。\(\mathrm{vec}(\boldsymbol{ab}^T) = \boldsymbol{b}\otimes\boldsymbol{a}\)。\((A\otimes B)(C\otimes D) = (AC)\otimes (BD)\)。可以对 \(F = D^TB^TXAC\) 求导来证明，一方面，直接求导得到 \(\frac{\partial F}{\partial X} = (AC) \otimes (BD)\)；另一方面，引入 \(Y = B^T X A\)，有 \(\frac{\partial F}{\partial Y} = C \otimes D\), \(\frac{\partial Y}{\partial X} = A \otimes B\)，用链式法则得到 \(\frac{\partial F}{\partial X} = (A\otimes B)(C \otimes D)\)。\(K_{mn} = K_{nm}^T\), \(K_{mn}K_{nm} = I\)。\(K_{pm}(A\otimes B) K_{nq} = B\otimes A\)，\(A\) 是 \(m\times n\) 矩阵，\(B\) 是 \(p\times q\) 矩阵。可以对 \(AXB^T\) 做向量化来证明，一方面，\(\mathrm{vec}(AXB^T) = (B\otimes A)\mathrm{vec}(X)\)；另一方面，\(\mathrm{vec}(AXB^T) = K_{pm}\mathrm{vec}(BX^TA^T) = K_{pm}(A\otimes B)\mathrm{vec}(X^T) = K_{pm}(A\otimes B) K_{nq}\mathrm{vec}(X)\)。</p>

<p>接下来演示一些算例。</p>

<p>例1：\(F = AX\)，\(X\) 是 \(m\times n\) 矩阵，求 \(\frac{\partial F}{\partial X}\)。</p>

<p>解：先求微分：\(dF=AdX\)，再做向量化，使用矩阵乘法的技巧，注意在 \(dX\) 右侧添加单位阵：\(\mathrm{vec}(dF) = \mathrm{vec}(AdX) = (I_n\otimes A)\mathrm{vec}(dX)\)，对照导数与微分的联系得到 \(\frac{\partial F}{\partial X} = I_n\otimes A^T\)。</p>

<p>特例：如果 \(X\) 退化为向量，即 \(\boldsymbol{f} = A \boldsymbol{x}\)，则根据向量的导数与微分的关系 \(d\boldsymbol{f} = \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}^T d\boldsymbol{x}\)，得到 \(\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}} = A^T\)。</p>

<p>例2：\(f = \log |X|\) ，\(X\) 是 \(n\times n\) 矩阵，求 \(\nabla_X f\) 和 \(\nabla^2_X f\)。</p>

<p>解：使用上篇中的技术可求得 \(\nabla_X f = X^{-1T}\) 。为求 \(\nabla^2_X f\)，先求微分：\(d\nabla_X f = -(X^{-1}dXX^{-1})^T\)，再做向量化，使用转置和矩阵乘法的技巧 \(\mathrm{vec}(d\nabla_X f)= -K_{nn}\mathrm{vec}(X^{-1}dX X^{-1}) = -K_{nn}(X^{-1T}\otimes X^{-1})\mathrm{vec}(dX)\)，对照导数与微分的联系，得到 \(\nabla^2_X f = -K_{nn}(X^{-1T}\otimes X^{-1})\)，注意它是对称矩阵。在 \(X\) 是对称矩阵时，可简化为 \(\nabla^2_X f = -X^{-1}\otimes X^{-1}\)。</p>

<p>例3：\(F = A\exp(XB)\)，\(A\) 是 \(l\times m\) 矩阵，\(X\) 是 \(m\times n\) 矩阵，\(B\) 是 \(n\times p\) 矩阵，\(\exp\) 为逐元素函数，求\(\frac{\partial F}{\partial X}\)。</p>

<p>解：先求微分：\(dF = A(\exp(XB)\odot (dXB))\)，再做向量化，使用矩阵乘法的技巧：\(\mathrm{vec}(dF) = (I_p\otimes A)\mathrm{vec}(\exp(XB)\odot (dXB))\)，再用逐元素乘法的技巧：\(\mathrm{vec}(dF) = (I_p \otimes A) \mathrm{diag}(\exp(XB))\mathrm{vec}(dXB)\)，再用矩阵乘法的技巧：\(\mathrm{vec}(dF) = (I_p\otimes A)\mathrm{diag}(\exp(XB))(B^T\otimes I_m)\mathrm{vec}(dX)\)，对照导数与微分的联系得到 \(\frac{\partial F}{\partial X} = (B\otimes I_m)\mathrm{diag}(\exp(XB))(I_p\otimes A^T)\)。</p>

<p>例4【一元logistic回归】：\(l = -y \boldsymbol{x}^T \boldsymbol{w} + \log(1 + \exp(\boldsymbol{x}^T\boldsymbol{w}))\)，求 \(\nabla_\boldsymbol{w} l\) 和 \(\nabla^2_\boldsymbol{w} l\)。其中 \(y\) 是取值0或1的标量，\(\boldsymbol{x}\),\(\boldsymbol{w}\) 是 \(n\times 1\) 列向量。</p>

<p>解：使用上篇中的技术可求得 \(\nabla_\boldsymbol{w} l = \boldsymbol{x}(\sigma(\boldsymbol{x}^T\boldsymbol{w}) - y)\)，其中 \(\sigma(a) = \frac{\exp(a)}{1+\exp(a)}\) 为sigmoid函数。为求 \(\nabla^2_\boldsymbol{w} l\)，先求微分：\(d\nabla_\boldsymbol{w} l = \boldsymbol{x} \sigma&#39;(\boldsymbol{x}^T\boldsymbol{w})\boldsymbol{x}^T d\boldsymbol{w}\) ，其中 \(\sigma&#39;(a) = \frac{\exp(a)}{(1+\exp(a))^2}\) 为sigmoid函数的导数，对照导数与微分的联系，得到 \(\nabla_w^2 l = \boldsymbol{x}\sigma&#39;(\boldsymbol{x}^T\boldsymbol{w})\boldsymbol{x}^T\)。</p>

<p>推广：样本 \((\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_N,y_N)\)，\(l = \sum_{i=1}^N \left(-y_i \boldsymbol{x}_i^T\boldsymbol{w} + \log(1+\exp(\boldsymbol{x_i}^T\boldsymbol{w}))\right)\)，求 \(\nabla_w l\) 和 \(\nabla^2_w l\)。有两种方法，方法一：先对每个样本求导，然后相加；方法二：定义矩阵 \(X = \begin{bmatrix}\boldsymbol{x}_1^T \\ \vdots \\ \boldsymbol{x}_n^T \end{bmatrix}\)，向量 \(\boldsymbol{y} = \begin{bmatrix}y_1 \\ \vdots \\ y_n\end{bmatrix}\)，将 \(l\) 写成矩阵形式 \(l = -\boldsymbol{y}^T X\boldsymbol{w} + \boldsymbol{1}^T\log(\boldsymbol{1} + \exp(X\boldsymbol{w}))\)，进而可以求得 \(\nabla_\boldsymbol{w} l = X^T(\sigma(X\boldsymbol{w}) - \boldsymbol{y})，\nabla_w^2 l = X^T\text{diag}(\sigma&#39;(X\boldsymbol{w}))X\)。</p>

<p>例5【多元logistic回归】：\(l = -\boldsymbol{y}^T\log \text{softmax}(W\boldsymbol{x}) = -\boldsymbol{y}^TW\boldsymbol{x} + \log(\boldsymbol{1}^T\exp(W\boldsymbol{x}))\)，求 \(\nabla_W l\) 和 \(\nabla^2_W l\) 。其中其中 \(\boldsymbol{y}\) 是除一个元素为1外其它元素为0的 \(m\times 1\) 列向量，\(W\) 是 \(m\times n\) 矩阵，\(\boldsymbol{x}\) 是 \(n\times 1\) 列向量，l是标量。</p>

<p>解：上篇中已求得 \(\nabla_W l = (\text{softmax}(W\boldsymbol{x})-\boldsymbol{y})\boldsymbol{x}^T\)。为求 \(\nabla^2_W l\)，先求微分：定义 <br/>
\[<br/>
\begin{align*}<br/>
\boldsymbol{a} &amp;= W\boldsymbol{x}，d\text{softmax}(\boldsymbol{a}) \\<br/>
&amp;= \frac{\exp(\boldsymbol{a})\odot d\boldsymbol{a}}{\boldsymbol{1}^T\exp(\boldsymbol{a})} - \frac{\exp(\boldsymbol{a}) (\boldsymbol{1}^T(\exp(\boldsymbol{a})\odot d\boldsymbol{a}))}{(\boldsymbol{1}^T\exp(\boldsymbol{a}))^2}<br/>
\end{align*}<br/>
\]</p>

<p>这里需要化简去掉逐元素乘法，第一项中 \(\exp(\boldsymbol{a})\odot d\boldsymbol{a} = \text{diag}(\exp(\boldsymbol{a})) d\boldsymbol{a}\) ，第二项中 \(\boldsymbol{1}^T(\exp(\boldsymbol{a})\odot d\boldsymbol{a}) = \exp(\boldsymbol{a})^Td\boldsymbol{a}\)，故有 \(d\text{softmax}(\boldsymbol{a}) = D\text{softmax}(\boldsymbol{a})d\boldsymbol{a}\)，其中 \(D\text{softmax}(\boldsymbol{a}) = \frac{\text{diag}(\exp(\boldsymbol{a}))}{\boldsymbol{1}^T\exp(\boldsymbol{a})} - \frac{\exp(\boldsymbol{a})\exp(\boldsymbol{a})^T}{(\boldsymbol{1}^T\exp(\boldsymbol{a}))^2}\) ，代入有 \(d\nabla_W l =D \text{softmax}(\boldsymbol{a})d\boldsymbol{a}\boldsymbol{x}^T = D\text{softmax}(W\boldsymbol{x})dW \boldsymbol{x}\boldsymbol{x}^T\)，做向量化并使用矩阵乘法的技巧，得到 \(\nabla^2_W l = (\boldsymbol{x}\boldsymbol{x}^T) \otimes D\text{softmax}(W\boldsymbol{x})\)。</p>

<p>最后做个总结。我们发展了从整体出发的矩阵求导的技术，导数与微分的联系是计算的枢纽，标量对矩阵的导数与微分的联系是 \(df = \mathrm{tr}(\nabla_X^T f dX)\)，先对 \(f\) 求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是 \(df = \nabla^T_{\boldsymbol{x}}f d\boldsymbol{x}\)；矩阵对矩阵的导数与微分的联系是 \(\mathrm{vec}(dF) = \frac{\partial F}{\partial X}^T \mathrm{vec}(dX)\)，先对 \(F\) 求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是 \(d\boldsymbol{f} = \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}^Td\boldsymbol{x}\)。</p>

<hr/>

<p><a href="https://zhuanlan.zhihu.com/p/27523007">矩阵求导浅析（一）</a><br/>
<a href="https://zhuanlan.zhihu.com/p/24709748">矩阵求导术（上）</a><br/>
<a href="https://zhuanlan.zhihu.com/p/24863977">矩阵求导术（下）</a></p>

			</div>

		
	  
		<footer>
		 <p class="meta">

			<strong>Categories:</strong>&nbsp; 
			<span class="categories">
			
			    <a class='category' href='%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html'>数学基础</a>&nbsp;
			 
			</span>
		    </p>
		    <p class="meta">
		      
		 </p>
	    
		<div class="sharing">
		  
          

          

		</div>

	    <p class="meta">
	    
	        <a class="basic-alignment left" href="15406533448507.html" 
	        title="Previous Post: 人工神经网络-长短时记忆网络 GRU">&laquo; 人工神经网络-长短时记忆网络 GRU</a>
	    
	    
	        <a class="basic-alignment right" href="15385007008840.html" 
	        title="Next Post: 次梯度 subgradient">次梯度 subgradient &raquo;</a>
	    
	    </p>
	  </footer>
	</article>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>