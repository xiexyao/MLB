
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  人工神经网络-RBF径向基网络 - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div>
	<article class="hentry" role="article">
	<header>
			  	<h1 class="entry-title">人工神经网络-RBF径向基网络</h1>
				<p class="meta"><time datetime="2018-07-16T22:57:58+08:00" pubdate data-updated="true">2018/7/16</time></p>
			 </header>
		  	<div class="entry-content">
			  	<p>RBF径向基网络全称是 Radical basis function network；RBF network，是一种常见的人工神经网络；在径向基网络先介绍径向基函数（RBF）。</p>

<h3 id="toc_0">径向基函数RBF</h3>

<p>径向基函数是某种沿径向对称的标量函数。通常定义为空间中任一点 \(x\) 到某一中心 \(x_c\) 之间欧氏距离的单调函数，可记作 \(\phi(||x-x_c||)\)，其作用往往是局部的 , 即当 \(x\) 远离 \(x_c\) 时函数取值很小。径向基函数的一般形式为<br/>
\[<br/>
\phi = \exp\big(-(x-x_c)^2\big)<br/>
\]</p>

<p>其中 \(x_c\) 为核函数中心，最常用的径向基函数是高斯核函数，形式为 <br/>
\[<br/>
\phi(||x-x_c||)=\exp\bigg(-\frac{||x-x_c||^2}{2\sigma^2}\bigg) = RBF(x,x_c)<br/>
\] </p>

<p>其中 \(\sigma\) 为函数的宽度参数，控制了函数的径向作用范围。如果 \(x\) 和 \(x_c\) 很相近那么核函数值为1，如果 \(x\) 和 \(x_c\) 相差很大那么核函数值约等于0。由于这个函数类似于高斯分布，因此也称为高斯核函数。它能够把原始特征映射到无穷维。</p>

<div align="center">
    <img width="320" src="media/15317530788642/15382614421727.jpg" />
</div>

<p>上图形象说明离中心点越远，函数值越小；距离值超过一定值后，函数值几乎为0。</p>

<p>简单说明一下为什么采样RBF函数的神经网络学习收敛得比较快。当网络的一个或多个可调参数（权值或阈值）对任何一个输出都有影响时，这样的网络称为全局逼近网络。由于对于每次输入，网络上的每一个权值都要调整，从而导致全局逼近网络的学习速度很慢。BP网络就是一个典型的例子。</p>

<p>如果对于输入空间的某个局部区域只有少数几个连接权值影响输出，大多数连接权重由于和中心点距离远，影响几乎为0，则该网络称为局部逼近网络。常见的局部逼近网络有RBF网络、小脑模型（CMAC）网络、B样条网络等。</p>

<h3 id="toc_1">径向基神经网络</h3>

<p>RBF神经网络，属于前向神经网络类型，它能够以任意精度逼近任意连续函数，特别适合于解决分类问题。</p>

<p>RBF网络的结构与多层前向网络类似，它是一种三层前向网络。第一层为输入层，由信号源结点组成；第二层为隐含层，隐单元数视所描述问题的需要而定，隐单元的变换函数是RBF径向基函数，它是对中心点径向对称且衰减的非负非线性函数；第三层为输出层，它对输入模式的作用作出响应。从输人空间到隐含层空间的变换是非线性的，而从隐含层空间到输出层空间变换是线性的。</p>

<div align="center">
    <img width="400" src="media/15317530788642/15382636777583.jpg" />
</div>

<p>RBF网络的基本思想是：用RBF作为隐单元的“基”构成隐含层空间，这样就可将输入矢量直接（即不需要通过权连接）映射到隐空间。根据Cover定理，低维空间不可分的数据到了高维空间会更有可能变得可分。换句话来说，<strong>RBF网络的隐层的功能就是将低维空间的输入通过非线性函数映射到一个高维空间。然后再在这个高维空间进行曲线的拟合</strong>。它等价于在一个隐含的高维空间寻找一个能最佳拟合训练数据的表面。这点与普通的多层感知机MLP是不同的。</p>

<p>当RBF的中心点确定以后，这种映射关系也就确定了。而隐含层空间到输出空间的映射是线性的，即网络的输出是隐单元输出的线性加权和，此处的权即为网络可调参数。由此可见，从总体上看，网络由输入到输出的映射是非线性的，而网络输出对可调参数而言却又是线性的。这样网络的权就可由线性方程组直接解出，从而大大加快学习速度并具有全局逼近能力，从根本上解决了BP网络的局部最优问题。</p>

<blockquote>
<p>在理论上，RBF网络和BP网络一样能以任意精度逼近任何非线性函数。但由于它们使用的激励函数不同，其逼近性能也不相同。Poggio和Girosi已经证明，RBF网络是连续函数的最佳逼近，而BP网络不是。BP网络使用的Sigmoid函数具有全局特性，它在输入值的很大范围内每个节点都对输出值产生影响，并且激励函数在输入值的很大范围内相互重叠，因而相互影响，因此BP网络训练过程很长。此外，由于BP算法采用梯度下降法，训练时间常，容易陷入局部极小的问题不可能从根本上避免，并且BP网络隐层节点数目的确定依赖于经验和试凑，很难得到最优网络。采用局部激励函数的RBF网络在很大程度上克服了上述缺点，RBF不仅有良好的泛化能力，而且对于每个输入值，只有很少几个节点具有非零激励值，因此只需很少部分节点及权值改变。学习速度可以比通常的BP算法提高上千倍,容易适应新数据，其隐层节点的数目也在训练过程中确定，并且其收敛性也较BP网络易于保证，因此可以得到最优解。</p>
</blockquote>

<p>从另一个方面也可以这样理解，多层感知器（包括BP神经网络）的隐节点基函数采用线性函数，激活函数则采用Sigmoid函数或硬极限函数。而<strong>RBF网络的隐节点的基函数采用距离函数（如欧氏距离），并使用径向基函数（如Gaussian函数）作为激活函数</strong>。径向基函数关于n维空间的一个中心点具有径向对称性，而且神经元的输入离该中心点越远，神经元的激活程度就越低。隐节点的这一特性常被称为“局部特性”。</p>

<p>\[<br/>
g_{RBF}(x_j) = \text{Output}\bigg(\sum_{i=1}^{|h|} w_{ij} \exp\Big(-\frac{||x_i - x_{c_i}||} {2\sigma^2}\Big) + b_{i}\bigg),j=1,2,...,M<br/>
\]</p>

<p>其中 \(|h|\) 表示隐藏层个数，也代表选取中心点的个数，\(M\) 表示输出层个数，\(\sigma\) 表示方差，\(w\) 则是对应的权重。最外层的 \(\text{Output}\) 函数则是根据目的选择不同的函数，如果想做分类的话就可以使用 softmax 或sign等函数，如果打算做回归或函数逼近就不用 \(\text{Output}\) 函数了，RBF Network 可以逼近任意连续的函数。</p>

<p>RBF的设计主要包括两个方面，一个是结构设计，也就是说隐藏层含有几个节点合适。另一个就是参数设计，也就是对网络各参数进行求解。由上面的输入到输出的网络映射函数公式可以看到，网络的参数主要包括三种：径向基函数的中心、方差和隐含层到输出层的权值。到目前为止，出现了很多求解这三种参数的方法，主要可以分为以下两大类：</p>

<h3 id="toc_2">RBF神经网络中心选取方法</h3>

<p>对于RBF神经网络的学习算法，关键问题是隐藏层神经元中心参数的合理确定。常用的方法是从中心参数(或者其初始值)是从给定的训练样本集里按照某种方法直接选取，或者是采用聚类的方法确定。</p>

<h4 id="toc_3">Full RBF network</h4>

<p>Full RBF 网络是指所有的数据节点都作为中心。</p>

<p>\[<br/>
g_{RBF}(x) = \text{Ouput}\bigg(\sum_{m=1}^M \beta_m RBF(x,c_m)\bigg)<br/>
\]</p>

<p>即 \(M=N\)，\(c_m = x_m\)，也就是中心点的个数（隐藏单元的个数）等于输入数据的个数。这样的话，预测新的数据点，就需要计算该点和所有训练数据点的距离，也就是相似度，然后结合权重 \(\beta\) 进行线性组合，此过程就是将所有的训练数据点对预测点的影响聚集到一起，距离越近，影响越大，这样得到最终的结果。</p>

<p>假设有四个元素 \(x_1,x_2,x_3,x_4\) 使用RBF神经网络，这样隐藏点的个数也是4，中心点分别是 \(c_1 = x_1\)，\(c_2 = x_2\)，\(c_3=x_3\) 和 \(c_4 = x_4\)，网络图如下<br/>
<div align=center><br/>
    <img width=360 src="media/15317530788642/15383902217169.jpg" /><br/>
</div></p>

<p>比如用均匀影响做二分类的话，即 \(\beta_m=1\cdot y_m\)，即:<br/>
\[<br/>
g_{RBF}(x) = \text{sign}\bigg(\sum_{m=1}^N y_m \exp(-\gamma ||x-x_m||^2)\bigg )<br/>
\]</p>

<p>就相当于每个训练数据给新数据投票，通过RBF，可以使得距离远的票数大，近的票数小，同时 \(y_m\) 可以控制投票的类别，这样通过统计所有点的观点，就可以得到新数据的类别了。</p>

<p>但是很显然， 这种Full RBF Network是一种偷懒的方式，因为直接将所有的点作为了center，因此如果样本量很大的话，那么计算量就太大了，在实际中，很少使用。</p>

<p>如果我们通过回归的方法学习参数 \(\beta_m\)，full RBF 是如下的形式<br/>
\[<br/>
\begin{align}<br/>
g_{RBF}(x) = \sum_{m=1}^N \beta_m RBF(x,x_m)\label{grsn}<br/>
\end{align}<br/>
\]</p>

<p>假设考虑 \(\mathbf x_i\) 在 RBF 转换后采用线性回归<br/>
\[<br/>
z_i = [RBF(\mathbf x_i,x_1),RBF(\mathbf x_i,x_2),...,RBF(\mathbf x_i,x_N)]<br/>
\]</p>

<p>令 \(\mathbf Z = [z_1,z_2,...,z_n]^T\)，\(\beta = [\beta_1,\beta_2,...,\beta_n]^T\)，用向量化表示式 ( \ref{grsn} )，现在我们要最小化这个函数<br/>
\[<br/>
L = \frac 1 2 \Big(\beta^T \mathbf Z - y\Big)^T(\beta^T \mathbf Z - y)<br/>
\]</p>

<p>用 \(L\) 对 \(\beta\) 求得，令结果为0，解得<br/>
\[<br/>
\begin{align*}<br/>
\beta = (\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^Ty<br/>
\end{align*}<br/>
\]</p>

<p>矩阵 \(\mathbf Z\) 的大小是 \(N\times N\)，是一个方阵。而且，由于 \(\mathbf Z\) 中每个向量 \(z_i\) 表示该点与其它所有点的RBF 距离，所以从形式上来说，\(\mathbf Z\) 也是对称矩阵。如果所有的样本点 \(x_i\) 都不一样，则 \(\mathbf Z\)一定是可逆的。</p>

<p>如果 \(\mathbf Z\) 是对称矩阵<br/>
\[<br/>
\beta = (\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^Ty = (\mathbf Z^T\mathbf Z)^{-1}\mathbf Z^T\mathbf Z^{-1}y = \mathbf Z^{-1}y<br/>
\]</p>

<p>因为 \(\mathbf Z\) 是对称矩阵，所以 \(\mathbf Z^{-1}\) 也是对称矩阵，所以<br/>
\[<br/>
g_{RBF}(x) = \beta^T \mathbf Z = (\mathbf Z^{-1}y)^T \mathbf Z = \mathbf Z^{-1} \mathbf Z y = y<br/>
\]</p>

<p>这里模型的输出与原样本的标签一模一样，所以模型的误差为0，这很有可能会增加模型的复杂度和过拟合。为了解决过拟合，这里可以参考岭回归，使用 \(L^2\) 正则化<br/>
\[<br/>
\beta = (\mathbf Z^T\mathbf Z + \lambda I)\mathbf Z^T y<br/>
\]</p>

<h4 id="toc_4">K-Means 方法</h4>

<p>将上面的方法改变一下，现在我们从训练样本集中随机选择 \(M\)（\(M\lt N\)）个样本作为 \(M\)个径向基函数的中心。更好的办法是这 \(m\) 个样本使用KNN的方式来选取，在Full RBF中，我们计算所有的训练数据与新数据的距离，距离最小的与新数据的相似度最高，而且高斯函数衰减很快，距离新数据远的点对它的影响很小，因此我们可以忽略那些距离较远的点，只需要找到几个最靠近新数据的点，然后只计算它们的贡献即可，假设我们找最近的K个点的话，那就是K近邻算法。这种类型的算法在训练的时候，不用花力气，但是再做测试或者预测的时候，需要对比全部的数据，然后找到几个最近的，计算这几个的贡献得到最后结果，这个过程跟上面的Full RBF 一样，计算量很大。因此这种方式经常在样本数据较少的时候使用。</p>

<p>下面重新给出使用K-Means的RBF Net用来做预测回归的流程:</p>

<ol>
<li>运行 K-Means 获得 \(k=M\) 个中心点 \(c_m\)</li>
<li>使用 RBF 在 \(c_m\) 构造转换 \(\phi(x)\)
\[
\phi(x) = [RBF(x,c_1),RBF(x,c_2),...,RBF(x,c_m)]
\]</li>
<li>在 \(\{\phi(x_i),y_i\}\) 上建立线性模型，获取 \(\beta\)。</li>
<li>返回\[
g_{RBF}(x) = \text{LinearHypothesis}(\beta,\phi(x))
\]</li>
</ol>

<p>这个过程与上面唯一不同的地方就是，开始先使用K-Means得到了中心，然后再计算特征转换的 \(Z\) 矩阵，其余步骤完全相同。</p>

<h3 id="toc_5">RBF神经网络的方差的选择</h3>

<p>RBF神经网络的基函数为高斯函数，方差 \(\sigma_i\) 可由下式求解得出：<br/>
\[<br/>
\sigma_i=\frac{c_{max}}{2M},i=1,2,⋯,M<br/>
\]</p>

<p>其中 \(c_{max}\) 是所选取中心之间的最大距离。</p>

<hr/>

<p><a href="http://www.cnblogs.com/zhangchaoyang/articles/2591663.html">径向基（RBF）神经网络</a><br/>
<a href="https://blog.csdn.net/zouxy09/article/details/13297881">径向基网络（RBF network）之BP监督训练</a><br/>
<a href="https://blog.csdn.net/sunxinyu/article/details/76598446">RBF神经网络与BP神经网络优缺点比较</a><br/>
<a href="https://shomy.top/2017/02/26/rbf-network/">机器学习技法笔记(6)-RBF Network(径向基函数网络)</a><br/>
<a href="https://github.com/ShomyLiu/stat-learn/tree/master/RBFNet">Numpy版KMeans+RBFNet 与 TensorFlow版KMeans+RBFNet的完整代码</a><br/>
<a href="http://read.pudn.com/downloads110/sourcecode/others/454289/Paper/pdf/y9935500004.pdf">第3章 RBF神经网络的基本原理</a></p>

			</div>

		
	  
		<footer>
		 <p class="meta">

			<strong>Categories:</strong>&nbsp; 
			<span class="categories">
			
			    <a class='category' href='%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html'>神经网络</a>&nbsp;
			 
			</span>
		    </p>
		    <p class="meta">
		      
		 </p>
	    
		<div class="sharing">
		  
          

          

		</div>

	    <p class="meta">
	    
	        <a class="basic-alignment left" href="15327348574245.html" 
	        title="Previous Post: 人工神经网络-卷积神经网络">&laquo; 人工神经网络-卷积神经网络</a>
	    
	    
	        <a class="basic-alignment right" href="15309710071828.html" 
	        title="Next Post: 自动编码器">自动编码器 &raquo;</a>
	    
	    </p>
	  </footer>
	</article>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>