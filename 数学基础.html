
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  数学基础 - 邪逍遥
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="历经千重罪，练就不死心">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="邪逍遥" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">邪逍遥</a></h1>
  
    <h2>历经千重罪，练就不死心</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div class="blog-index">

	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15396209035459.html">矩阵求导</a></h1>
			<p class="meta"><time datetime="2018-10-16T00:28:23+08:00" 
			pubdate data-updated="true">2018/10/16</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>矩阵求导在机器学习中占据了相当的作用，而矩阵求导本身也难以理解。这里我们了解一下矩阵求导与迹的相关知识。</p>

<h4 id="toc_0">基础矩阵知识-迹</h4>

<p>在线性代数中，\(n\times n\) 方阵 \(A\) 的迹，是指 \(A\) 的主对角线各元素的总和（从左上方至右下方的对角线），例如：<br/>
\[<br/>
Tr(A) = A_{11} + A_{22} + \cdots + A_{nn} = \sum_{i=1}^n A_{ii}<br/>
\]</p>

<p>其中 \(A_{ij}\) 代表在 \(i\) 行 \(j\) 栏中的数值。同样的，元素的迹是其特征值的总和，使其不变量根据选择的基本准则而定。</p>

<p>矩阵迹的性质：</p>

<ol>
<li>常数的迹：\(Tr(a) = a\)</li>
<li>加减法：\(Tr(A\pm B) = Tr(A) \pm Tr(B)\)</li>
<li>转置：\(Tr(A^T) = Tr(A)\)</li>
<li>标量乘法：\(Tr(\alpha A) = \alpha Tr(A)\)</li>
<li>向量乘法：\(Tr(AB) = Tr(BA)\)</li>
<li><p>矩阵迹的求导：<br/>
\[<br/>
\nabla_A Tr(AB) = B^T<br/>
\]</p>

<p>证明：假设矩阵 \(A\) 的大小为 \(m\times n\)，矩阵 \(B\) 的大小为 \(n\times m\)，则<br/>
\[<br/>
\begin{align*}<br/>
\because Tr(AB) &amp;= \sum_{i=1}^m\sum_{j=1}^n A_{ij} B_{ji}\\<br/>
\therefore \frac{\partial Tr(AB)}{\partial A} &amp;= \left [ \begin{array}{cccc} \frac{\partial Tr(AB)}{\partial A_{11}} &amp; \frac{\partial Tr(AB)}{\partial A_{12}} &amp; \cdots &amp; \frac{\partial Tr(AB)}{\partial A_{1n}} \\ \frac{\partial Tr(AB)}{\partial A_{21}} &amp; \frac{\partial Tr(AB)}{\partial A_{22}} &amp; \cdots &amp; \frac{\partial Tr(AB)}{\partial A_{2n}} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\  \frac{\partial Tr(AB)}{\partial A_{m1}} &amp; \frac{\partial Tr(AB)}{\partial A_{m2}} &amp; \cdots &amp; \frac{\partial Tr(AB)}{\partial A_{mn}}\end{array} \right ]\\<br/>
&amp;= \left [ \begin{array}{cccc} B_{11} &amp; B_{21} &amp; \cdots &amp; B_{n1}\\ B_{12} &amp; B_{22} &amp;\cdots &amp; B_{n2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ B_{1m} &amp; B_{2m} &amp; \cdots &amp; B_{nm} \end{array} \right ]\\<br/>
&amp;= B^T<br/>
\end{align*}<br/>
\]</p>

<p>得证。</p></li>
<li><p>\(\nabla_{A^T} f(A) = \big(\nabla_A f(X) \big)^T\)</p></li>
<li><p>\(\nabla_A Tr(ABA^T C) = CAB + C^TAB^T\)<br/>
证明：令 \(u(A) = AB\)，\(v(A^T) = A^T C\)，则<br/>
\[<br/>
\begin{align*}<br/>
\nabla_A Tr(ABA^T C) &amp;= \nabla_A Tr\big(u(A) v(A^T)\big) \\<br/>
&amp;= \nabla_{A:u(A)} Tr\big(u(A) v(A^T)\big) + \nabla_{A:v(A^T)} Tr\big(u(A) v(A^T) \big)\\<br/>
&amp;= \nabla_{A:u(A)} Tr\big(u(A) v(A^T)\big) + \Big(\nabla_{A^T:v(A^T)} Tr\big(u(A) v(A^T) \big)\Big)^T\\<br/>
&amp;= \big(v(A^T)\big)^T \nabla_{A}u(A) + \Big(\big(u(A)\big)^T \nabla_{A^T} v(A^T)\Big)^T\\<br/>
&amp;= C^TAB^T + \big(B^TA^T C^T\big)^T\\<br/>
&amp;= C^TAB^T + CAB<br/>
\end{align*}<br/>
\]</p></li>
<li><p>矩阵逐元素乘法：\(Tr\Big(A^T(B\odot C)\Big) = Tr\Big((A\odot B)^T C\Big)\)</p></li>
</ol>

<h4 id="toc_1">基础矩阵知识-内积</h4>

<p>向量 \(\mathbf x,\mathbf y \in \mathbb R^n\) 的内积定义为<br/>
\[<br/>
&lt;\mathbf x,\mathbf y&gt; = \mathbf x^T \mathbf y = \sum_{i=1}^n x_i y_i = Tr(\mathbf x^T \mathbf y) = Tr(\mathbf y\mathbf x^T)<br/>
\]</p>

<p>矩阵的 \(\mathbf X,\mathbf Y \in \mathbb R^{m\times n}\) 内积定义为<br/>
\[<br/>
&lt;\mathbf X,\mathbf Y&gt; = Tr(\mathbf X^T\mathbf Y) = \sum_{i=1}^m \sum_{j=1}^n X_{ij} Y_{ij}<br/>
\]</p>

<p>利用 Tr 的性质 \(Tr(\mathbf A\mathbf B) = Tr(\mathbf B\mathbf A)\) 和内积的定义，可以知道<br/>
\[<br/>
&lt;\mathbf A\mathbf X\mathbf B,\mathbf C&gt; = &lt;\mathbf X,\mathbf A^T\mathbf C\mathbf B^T&gt;<br/>
\]</p>

<p>证明：<br/>
\[<br/>
\begin{align*}<br/>
&lt;\mathbf A\mathbf X\mathbf B,\mathbf C&gt; &amp;= Tr\Big((\mathbf A\mathbf X\mathbf B)^T\mathbf C\Big) \\<br/>
&amp;= Tr(\mathbf B^T\mathbf X^T \mathbf A^T \mathbf C)\\<br/>
&amp;= Tr(\mathbf X^T \mathbf A^T \mathbf C \mathbf B^T)\\<br/>
&amp;= &lt;\mathbf X,\mathbf A^T\mathbf C\mathbf B^T&gt;\\<br/>
\end{align*}<br/>
\]</p>

<h4 id="toc_2">导数与微分的关系</h4>

<p>我们首先来看一下一元函数的导数与微分的联系:\(df = f&#39;(x) dx\)；多元微积分中的梯度（标量对向量的导数）也与微分有联系:<br/>
\[<br/>
df = \sum_{i=1}^n \frac{\partial f}{\partial x_i} dx_i = \frac{\partial f^T}{\partial \mathbf x} d\mathbf x<br/>
\]</p>

<p>这里第一个等号是全微分公式，第二个等号表达了梯度与微分的联系；全微分 \(df\) 是梯度向量 \(\frac{\partial f}{\partial \mathbf x}\quad (n\times 1)\) 与微分向量 \(d\mathbf x\quad (n\times 1)\) 的内积；受此启发，我们将矩阵导数与微分建立联系：<br/>
\[<br/>
df = \sum_{i=1}^m \sum_{j=1}^n \frac{\partial f}{\partial X_{ij}} dX_{ij} = Tr\Big( \frac{\partial f^T}{\partial X} dX\Big)<br/>
\]</p>

<p>其中 \(Tr\) 表示迹（trace）是方阵对角线元素之和，满足性质：对尺寸相同的矩阵A，B，\(Tr(A^TB) = \sum_{i,j} A_{ij} B_{ij}\)，，即 \(\text{tr}(A^TB)\) 是矩阵 \(A,B\) 的内积。与梯度相似，这里第一个等号是全微分公式，第二个等号表达了矩阵导数与微分的联系：全微分 \(df\) 是导数 \(\frac{\partial f}{\partial X}(m\times n)\) 与微分矩阵 \(dX(m\times n)\) 的内积。</p>

<p>我们来创立常用的矩阵微分的运算法则：</p>

<ol>
<li>加减法：\(d(X\pm Y) = dX \pm dY\)；矩阵乘法：\(d(XY) = (dX)Y + X dY\) ；转置：\(d(X^T) = (dX)^T\)；迹：\(d\text{Tr}(X) = \text{Tr}(dX)\)。</li>
<li><p>逆：\(dX^{-1} = -X^{-1}dX X^{-1}\)。此式可在 \(XX^{-1}=I\) 两侧求微分来证明。<br/>
证明：<br/>
\[<br/>
\begin{align*}<br/>
&amp;\because \quad XX^{-1} = I \\<br/>
&amp;\therefore \quad d(XX^{-1}) = dXX^{-1} + XdX^{-1} = 0\\<br/>
&amp;\Rightarrow \quad XdX^{-1} = -dXX^{-1} \\<br/>
&amp;\Rightarrow \quad dX^{-1} = -X^{-1}dXX^{-1}\\<br/>
\end{align*}<br/>
\]</p></li>
<li><p>行列式：\(d|X| = \text{tr}(X^{\#}dX)\) ，其中 \(X^{\#}\) 表示 \(X\) 的伴随矩阵，在 \(X\) 可逆时又可以写作 \(d|X|= |X|\text{tr}(X^{-1}dX)\)。</p></li>
<li><p>逐元素乘法：\(d(X\odot Y) = dX\odot Y + X\odot dY\)，\(\odot\) 表示尺寸相同的矩阵 \(X,Y\) 逐元素相乘。</p></li>
<li><p>逐元素函数：\(d\sigma(X) = \sigma&#39;(X)\odot dX\) ，\(\sigma(X) = \left[\sigma(X_{ij})\right]\) 是逐元素标量函数运算， \(\sigma&#39;(X)=[\sigma&#39;(X_{ij})]\) 是逐元素求导数。举个例子，<br/>
\[<br/>
X=\left[\begin{matrix}x_{11} &amp; x_{12} \\ x_{21} &amp; x_{22}\end{matrix}\right], d \sin(X) = \left[\begin{matrix}\cos x_{11} dx_{11} &amp; \cos x_{12} d x_{12}\\ \cos x_{21} d x_{21}&amp; \cos x_{22} dx_{22}\end{matrix}\right] = \cos(X)\odot dX<br/>
\]</p></li>
</ol>

<p>在建立法则的最后，来谈一谈复合：假设已求得 \(\frac{\partial f}{\partial Y}\) ，而 \(Y\) 是 \(X\) 的函数，如何求 \(\frac{\partial f}{\partial X}\) 呢？在微积分中有标量求导的链式法则 \(\frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial y}{\partial x}\)，但这里我们不能沿用链式法则，因为矩阵对矩阵的导数 \(\frac{\partial Y}{\partial X}\) 截至目前仍是未定义的。于是我们继续追本溯源，链式法则是从何而来？源头仍然是微分。我们直接从微分入手建立复合法则：先写出 \(df = \text{tr}\left(\frac{\partial f}{\partial Y}^T dY\right)\)，再将 \(dY\) 用 \(dX\) 表示出来代入，并使用迹技巧将其他项交换至 \(dX\) 左侧，即可得到 \(\frac{\partial f}{\partial X}\)。</p>

<p>接下来演示一些算例。特别提醒要依据已经建立的运算法则来计算，不能随意套用微积分中标量导数的结论，比如认为 \(AX\) 对 \(X\) 的导数为 \(A\)，这是没有根据、意义不明的。</p>

<p>例1：\(f = \boldsymbol{a}^T X\boldsymbol{b}\)，求 \(\frac{\partial f}{\partial X}\)。其中 \(\boldsymbol{a}\) 是 \(m\times 1\) 列向量，\(X\) 是 \(m\times n\) 矩阵，\(\boldsymbol{b}\) 是 \(n\times 1\) 列向量，\(f\) 是标量。</p>

<p>解：先使用矩阵乘法法则求微分，这里的 \(\boldsymbol{a}\), \(\boldsymbol{b}\) 是常量，\(d\boldsymbol{a} = \boldsymbol{0}\), \(d\boldsymbol{b} = \boldsymbol{0}\)，得到：\(df = \boldsymbol{a}^T dX\boldsymbol{b}\)，再套上迹并做矩阵乘法交换：<br/>
\[<br/>
df = \text{tr}(\boldsymbol{a}^TdX\boldsymbol{b}) = \text{tr}(\boldsymbol{b}\boldsymbol{a}^TdX)<br/>
\]</p>

<p>对照导数与微分的联系 \(df = \text{tr}\left(\frac{\partial f}{\partial X}^T dX\right)\)，得到 \(\frac{\partial f}{\partial X} = (\boldsymbol{b}\boldsymbol{a}^T)^T= \boldsymbol{a}\boldsymbol{b}^T\)。</p>

<p>例2：\(f = \boldsymbol{a}^T \exp(X\boldsymbol{b})\)，求 \(\frac{\partial f}{\partial X}\)。其中\(\boldsymbol{a}\) 是 \(m\times 1\) 列向量，\(X\) 是 \(m\times n\) 矩阵，\(\boldsymbol{b}\) 是 \(n\times 1\) 列向量，\(f\) 是标量。</p>

<p>解：先使用矩阵乘法、逐元素函数法则求微分：\(df = \boldsymbol{a}^T(\exp(X\boldsymbol{b})\odot (dX\boldsymbol{b}))\)，再套上迹并做交换：<br/>
\[<br/>
\begin{align*}<br/>
df &amp;= \text{tr}( \boldsymbol{a}^T(\exp(X\boldsymbol{b})\odot (dX\boldsymbol{b}))) \\<br/>
&amp;=\text{tr}((\boldsymbol{a}\odot \exp(X\boldsymbol{b}))^TdX \boldsymbol{b}) \\<br/>
&amp;= \text{tr}(\boldsymbol{b}(\boldsymbol{a}\odot \exp(X\boldsymbol{b}))^TdX)<br/>
\end{align*}<br/>
\]</p>

<p>对照导数与微分的联系 \(df = \text{tr}\left(\frac{\partial f}{\partial X}^T dX\right)\)，得到 <br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial X} &amp;= (\boldsymbol{b}(\boldsymbol{a}\odot \exp(X\boldsymbol{b}))^T)^T \\<br/>
&amp;= (\boldsymbol{a}\odot \exp(X\boldsymbol{b}))\boldsymbol{b}^T\\<br/>
\end{align*}<br/>
\]</p>

<p>例3：\(f = \text{Tr}(Y^T M Y)\), \(Y = \sigma(WX)\)，求 \(\frac{\partial f}{\partial X}\)。其中 \(W\) 是 \(l\times m\) 矩阵，\(X\) 是 \(m\times n\) 矩阵，\(Y\) 是 \(l\times n\) 矩阵，\(M\) 是 \(l\times l\) 对称矩阵，\(\sigma\) 是逐元素函数，\(f\) 是标量。<br/>
解：先求 \(\frac{\partial y}{\partial Y}\)，用矩阵乘法和转置法则：<br/>
\[<br/>
\begin{align*}<br/>
\because f &amp;= \text{Tr}(Y^T M Y)\\<br/>
\Rightarrow df &amp;= d\text{Tr}(Y^T M Y) = \text{Tr}(d(Y^TMY))\\<br/>
&amp;= \text{Tr}((dY)^TMY) + \text{Tr}(Y^TMdY)\\<br/>
&amp;= \text{Tr}(Y^TM^TdY) + \text{Tr}(Y^TMdY)\qquad \because \text{M是对称矩阵}\\<br/>
&amp;= 2\text{Tr}(Y^TMdY)<br/>
\end{align*}<br/>
\]</p>

<p>对照微分和导数的关系 \(df = \text{Tr}\Big (\frac{\partial f^T}{\partial Y} dY\Big)\)，所以有：<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial Y} = (2 Y^T M)^T = 2MY<br/>
\end{align*}<br/>
\]</p>

<p>现在再来看 \(Y = \sigma(WX)\)，可知：\(dY = \sigma&#39;(WX)\odot (WdX)\)，代入 \(df\)，得：<br/>
\[<br/>
\begin{align*}<br/>
df &amp;= \text{Tr}\Big(\frac{\partial f^T}{\partial Y}\big(\sigma&#39;(WX)\odot (WdX)\big)\Big)\\<br/>
&amp;= \text{Tr}\Big(\big(\frac{\partial f}{\partial Y} \odot \sigma&#39;(WX)\big )^T WdX \Big) \\<br/>
\end{align*}<br/>
\]</p>

<p>对照导数与微分的联系，得到<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial X} &amp;= \Big(\big(\frac{\partial f}{\partial Y} \odot \sigma&#39;(WX)\big )^T W \Big)^T\\<br/>
&amp;= W^T \big(\frac{\partial f}{\partial Y} \odot \sigma&#39;(WX)\big )\\<br/>
&amp;= W^T \big(2MY\odot \sigma&#39;(WX)\big)\\<br/>
&amp;= W^T \big(2M\sigma(WX)\odot \sigma&#39;(WX)\big) \\<br/>
\end{align*}<br/>
\]</p>

<p>例4【线性回归】：\(l = \|X\boldsymbol{w}- \boldsymbol{y}\|^2\)， 求\(\boldsymbol{w}\) 的最小二乘估计，即求 \(\frac{\partial l}{\partial \boldsymbol{w}}\) 的零点。其中 \(\boldsymbol{y}\) 是 \(m\times 1\) 列向量，\(X\) 是 \(m\times n\) 矩阵，\(\boldsymbol{w}\) 是 \(n×1\) 列向量，\(l\) 是标量。<br/>
解：关于 \(l\) 的模的平方可以写为向量与自身的内积，即<br/>
\[<br/>
l = (X\boldsymbol{w} - \boldsymbol{y})^T(X\boldsymbol{w} - \boldsymbol{y})<br/>
\]</p>

<p>所以，它的微分为<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \big(d(X\boldsymbol{w} - \boldsymbol{y})^T\big)(X\boldsymbol{w} - \boldsymbol{y})+ (X\boldsymbol w - \boldsymbol y)^T d(X\boldsymbol w - \boldsymbol x)\\<br/>
&amp;= (Xd\boldsymbol w)^T(X\boldsymbol{w} - \boldsymbol{y}) + (X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w)\\<br/>
&amp;= \big((X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w)\big)^T + (X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w)\\<br/>
\because \quad &amp; (X\boldsymbol w - \boldsymbol x) \in \mathcal A_{m\times 1}\qquad (Xd\boldsymbol w)\in \mathcal A_{m\times 1}\\<br/>
\therefore \quad &amp; (X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w) \in \mathcal A_{1\times 1}\\<br/>
dl &amp;= 2(X\boldsymbol w - \boldsymbol y)^T(Xd\boldsymbol w)<br/>
\end{align*}<br/>
\]</p>

<p>按照微分和导数的关系：<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= Tr\big(\frac{\partial f^T}{\partial \boldsymbol w} d\boldsymbol w\big)\\<br/>
\end{align*}<br/>
\]</p>

<p>可知：<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial \boldsymbol w} &amp; = \big( 2(X\boldsymbol w - \boldsymbol y)^T X\big)^T\\<br/>
&amp;= 2X^T(X\boldsymbol w - \boldsymbol y)<br/>
\end{align*}<br/>
\]</p>

<p>\(\frac{\partial l}{\partial \boldsymbol{w}}\) 的零点即\(\boldsymbol{w}\) 的最小二乘估计为 \(\boldsymbol{w} = (X^TX)^{-1}X^T\boldsymbol{y}\)</p>

<p>例5【方差的最大似然估计】：样本 \(\boldsymbol{x}_1,\dots, \boldsymbol{x}_N \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)\)，求方差 \(\Sigma\) 的最大似然估计。写成数学式是：\(l = \log|\Sigma|+\frac{1}{N}\sum_{i=1}^N(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T\Sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\bar{x}})\)，求 \(\frac{\partial l }{\partial \Sigma}\) 的零点。其中 \(\boldsymbol{x}_i\) 是 \(m\times 1\) 列向量，\(\bar{\boldsymbol{x}}=\frac{1}{N}\sum_{i=1}^N \boldsymbol{x}_i\) 是样本均值，\(\Sigma\) 是 \(m\times m\) 对称正定矩阵，\(l\) 是标量，\(<br/>
\log\)表示自然对数。<br/>
解：首先求微分，使用矩阵乘法、行列式、逆等运算法则，第一项是：<br/>
\[<br/>
\begin{align*}<br/>
d\log|\Sigma| &amp;= |\Sigma|^{-1} \odot d|\Sigma|\\<br/>
&amp;= |\Sigma|^{-1} \odot \Big(|\Sigma|\text{Tr}(\Sigma^{-1} d\Sigma)\Big)\\<br/>
&amp;= \text{Tr}(\Sigma^{-1} d\Sigma) \qquad \because \text{Tr(*) is Scalar}<br/>
\end{align*}<br/>
\]</p>

<p>再来看第二项，<br/>
\[<br/>
\begin{align*}<br/>
d \Big(\frac{1}{N}\sum_{i=1}^N(\boldsymbol{x}_i-\boldsymbol{\bar{x}})^T\Sigma^{-1}(\boldsymbol{x}_i-\boldsymbol{\bar{x}})\Big) &amp;= \frac 1 N \sum_{i=1}^N(\boldsymbol x_i - \boldsymbol{\bar{x}})^T d(\Sigma^{-1}) (\boldsymbol x_i - \boldsymbol{\bar{x}})\\<br/>
&amp;= -\frac 1 N \sum_{i=1}^N(\boldsymbol x_i - \boldsymbol{\bar{x}})^T \Sigma^{-1} d\Sigma \Sigma^{-1}) (\boldsymbol x_i - \boldsymbol{\bar{x}})\\<br/>
\end{align*}<br/>
\]</p>

<p>给第二项套上迹然后做交换：<br/>
\[<br/>
\begin{align*}<br/>
\text{Tr}\Big(-\frac 1 N \sum_{i=1}^N(\boldsymbol x_i - \boldsymbol{\bar{x}})^T \Sigma^{-1} d\Sigma \Sigma^{-1}) (\boldsymbol x_i - \boldsymbol{\bar{x}})\Big) &amp;= -\frac 1 N \sum_{i=1}^N \text{Tr}\Big((\boldsymbol x_i - \boldsymbol{\bar{x}})^T \Sigma^{-1}d\Sigma\Sigma^{-1} (\boldsymbol x_i - \boldsymbol{\bar{x}})\Big)\\<br/>
&amp;= -\frac 1 N \sum_{i=1}^N \text{Tr}\Big(\Sigma^{-1} (\boldsymbol x_i - \boldsymbol{\bar{x}})(\boldsymbol x_i - \boldsymbol{\bar{x}})^T \Sigma^{-1}d\Sigma\Big)\\<br/>
\end{align*}<br/>
\]</p>

<p>令 \(S = \frac1 N \sum{i=1}^N (\boldsymbol x_i - \boldsymbol{\bar{x}})(\boldsymbol x_i - \boldsymbol{\bar{x}})^T\)，所以第二项可以写为：\(-\text{Tr}\Big(\Sigma^{-1} S \Sigma^{-1}d\Sigma\Big)\)，结合第一项得到：<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \text{Tr}(\Sigma^{-1} d\Sigma) - \text{Tr}\big(\Sigma^{-1} S \Sigma^{-1}d\Sigma\big) \\<br/>
&amp;= \text{Tr}\Big(\big(\Sigma^{-1} - \Sigma^{-1} S \Sigma^{-1} \big)d\Sigma\Big)\\<br/>
\end{align*}<br/>
\]</p>

<p>对照微分与导数的关系：<br/>
\[<br/>
dl = \text{Tr}\Big(\frac{\partial l^T}{\partial \Sigma} d\Sigma \Big)<br/>
\]</p>

<p>可知<br/>
\[<br/>
\frac{\partial l}{\partial \Sigma} = \big(\Sigma^{-1} - \Sigma^{-1} S \Sigma^{-1} \big)^T<br/>
\]</p>

<p>其零点也就是 \(S = \Sigma\) 的时候。</p>

<p>例6【多元logistic回归】：\(l = -\boldsymbol{y}^T\log\text{softmax}(W\boldsymbol{x})\)，求 \(\frac{\partial l}{\partial W}\)。其中\(\boldsymbol{y}\) 是除一个元素为1外其它元素为0的 \(m\times 1\) 列向量，\(W\) 是 \(m\times n\) 矩阵，\(\boldsymbol{x}\) 是 \(n\times 1\) 列向量，\(l\) 是标量；\(\log\) 表示自然对数，\(\text{softmax}(\boldsymbol{a}) = \frac{\exp(\boldsymbol{a})}{\boldsymbol{1}^T\exp(\boldsymbol{a})}\)，其中 \(\exp(\boldsymbol{a})\) 表示逐元素求指数，\(\boldsymbol{1}\) 代表全1向量。<br/>
解1：首先将 softmax 代入 \(l\) 中，得：<br/>
\[<br/>
\begin{align*}<br/>
l &amp;= -\boldsymbol{y}^T\log \frac{\exp(W\boldsymbol{x})}{\boldsymbol{1}^T\exp(W\boldsymbol{x})}\\<br/>
&amp;= -\boldsymbol{y}^T \Big(\log\exp(W\boldsymbol{x}) -\boldsymbol{1} \log\big(\boldsymbol{1}^T \exp(W\boldsymbol{x})\big)\Big) \\<br/>
&amp;= -\boldsymbol{y}^TW\boldsymbol{x} + \log\big(\boldsymbol{1}^T \exp(W\boldsymbol{x})\big)<br/>
\end{align*}<br/>
\]</p>

<p>这里需要注意两点，一个是\(\log{\boldsymbol{x}/y} = \log \boldsymbol{x} - \boldsymbol{1} \log y\)，另一个是 \(\boldsymbol{y}^T \boldsymbol{1} = 1\)。现在求微分，利用矩阵乘法和逐乘法得<br/>
\[<br/>
dl = -\boldsymbol{y}^T dW\boldsymbol x + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \Big(\boldsymbol{1}^T \big(\exp(W\boldsymbol x)\odot (dW\boldsymbol x)\big)\Big)\\<br/>
\]</p>

<p>再套上迹作交换<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \text{Tr}\bigg( -\boldsymbol{y}^T dW\boldsymbol x + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \Big(\boldsymbol{1}^T \big(\exp(W\boldsymbol x)\odot (dW\boldsymbol x)\big)\Big) \bigg)\\<br/>
&amp;= \text{Tr}\bigg( -\boldsymbol{y}^T dW\boldsymbol x + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \Big(\big(\boldsymbol{1}\odot\exp(W\boldsymbol x)\big)^T (dW\boldsymbol x)\Big) \bigg)\\<br/>
&amp;= \text{Tr}\bigg( -\boldsymbol{y}^T dW\boldsymbol x + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \Big(\exp(W\boldsymbol x)^T (dW\boldsymbol x)\Big) \bigg)\\<br/>
&amp;= \text{Tr}\bigg( -\boldsymbol x \big( \boldsymbol y^T + \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \exp(W\boldsymbol x)^T  \big) dW\bigg)<br/>
\end{align*}<br/>
\]</p>

<p>最后一步是利用迹的可交换性得到，此时按照微分和导数的关系：<br/>
\[<br/>
dl = \text{Tr}(\frac{\partial l^T}{\partial W} dW)^T<br/>
\]</p>

<p>所以有<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial l}{\partial W} &amp;= \bigg( -\boldsymbol x \big( \boldsymbol y^T - \big(\boldsymbol{1}^T \exp(W\boldsymbol x)\big)^{-1} \odot \exp(W\boldsymbol x)^T  \big)\bigg)^T\\<br/>
&amp;= \bigg( -\boldsymbol x \big( \boldsymbol y^T - \text{softmax}(W\boldsymbol x)^T  \big)\bigg)^T\\<br/>
&amp;= \big(\text{softmax}(W\boldsymbol x) - \boldsymbol y\big) \boldsymbol x^T<br/>
\end{align*}<br/>
\]</p>

<p>解2：定义 \(\boldsymbol{a} = W\boldsymbol{x}\)，则 \(l = -\boldsymbol{y}^T\log\text{softmax}(\boldsymbol{a})\) ，先同上求出 \(\frac{\partial l}{\partial \boldsymbol{a}} = \text{softmax}(\boldsymbol{a})-\boldsymbol{y}\)，再利用复合法则：\(dl = \text{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}}^Td\boldsymbol{a}\right) = \text{tr}\left(\frac{\partial l}{\partial \boldsymbol{a}}^TdW \boldsymbol{x}\right) = \text{tr}\left(\boldsymbol{x}\frac{\partial l}{\partial \boldsymbol{a}}^TdW\right)\)，得到\(\frac{\partial l}{\partial W}= \frac{\partial l}{\partial\boldsymbol{a}}\boldsymbol{x}^T\)。</p>

<p>例7【二层神经网络】：\(l = -\boldsymbol{y}^T\log\text{softmax}(W_2\sigma(W_1\boldsymbol{x}))\) ，求 \(\frac{\partial l}{\partial W_1}和\frac{\partial l}{\partial W_2}\)。其中 \(\boldsymbol{y}\) 是除一个元素为1外其它元素为0的的 \(m\times 1\) 列向量，\(W_2\) 是 \(m\times p\) 矩阵，\(W_1\) 是 \(p \times n\) 矩阵，\(\boldsymbol{x}\) 是 \(n\times 1\) 列向量，l是标量；\(\log\) 表示自然对数，\(\text{softmax}(\boldsymbol{a}) = \frac{\exp(\boldsymbol{a})}{\boldsymbol{1}^T\exp(\boldsymbol{a})}\) 。同上，\(\sigma\) 是逐元素 \(\text{sigmoid}\) 函数 \(\sigma(a) = \frac{1}{1+\exp(-a)}\)。</p>

<p>解：定义 \(\boldsymbol{a}_1=W_1\boldsymbol{x}\)，\(\boldsymbol{h}_1 = \sigma(\boldsymbol{a}_1)\)，\(\boldsymbol{a}_2 = W_2 \boldsymbol{h}_1\)，则 \(l =-\boldsymbol{y}^T\log\text{softmax}(\boldsymbol{a}_2)\)。上例中已经求出 \(\frac{\partial l}{\partial \boldsymbol{a}_2} = \text{softmax}(\boldsymbol{a}_2)-\boldsymbol{y}\)。使用复合法则 <br/>
\[<br/>
dl = \text{Tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^Td\boldsymbol{a}_2\right) = \text{Tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^TdW_2 \boldsymbol{h}_1\right) + \underbrace{ \text{Tr}\left(\frac{\partial l}{\partial \boldsymbol{a}_2}^TW_2 d\boldsymbol{h}_1\right)}_{dl_2}<br/>
\]</p>

<p>使用矩阵乘法交换的迹技巧从第一项得到 <br/>
\[<br/>
\frac{\partial l}{\partial W_2}= \frac{\partial l}{\partial\boldsymbol{a}_2}\boldsymbol{h}_1^T<br/>
\]</p>

<blockquote>
<p>因为 \(\boldsymbol a_2 = W_2 \boldsymbol h_1\)，所以 \(d\boldsymbol a_2 = (dW_2) \boldsymbol h_1\)，所以 \(dl = \text{Tr}(\frac{l}{\partial \boldsymbol a_2}^T d\boldsymbol a_2)\) ，将 \(d\boldsymbol a_2\) 代入可得 <br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \text{Tr}\Big(\frac{l}{\partial \boldsymbol a_2}^T (dW_2)\boldsymbol h_1\Big)^T\\<br/>
&amp;= \text{Tr}\Big(\boldsymbol h_1 \frac{l}{\partial \boldsymbol a_2}^T dW_2\Big) \\<br/>
\end{align*}<br/>
\]</p>

<p>对比微分和导数的关系 \(dl = \text{Tr}(\frac{l}{\boldsymbol W_2}^T dW_2)\)，可得<br/>
\[<br/>
\frac{l}{\boldsymbol W_2} = \Big(\boldsymbol h_1 \frac{l}{\partial \boldsymbol a_2}^T \Big)^T = \frac{l}{\partial \boldsymbol a_2} \boldsymbol h_1^T<br/>
\]</p>
</blockquote>

<p>从第二项得到 \(\frac{\partial l}{\partial \boldsymbol{h}_1}= W_2^T\frac{\partial l}{\partial\boldsymbol{a}_2}\)。接下来对第二项继续使用复合法则来求 \(\frac{\partial l}{\partial \boldsymbol{a}_1}\)，并利用矩阵乘法和逐元素乘法交换的迹技巧：<br/>
\[<br/>
\begin{align*}<br/>
dl_2 = \text{Tr}\left(\frac{\partial l}{\partial\boldsymbol{h}_1}^Td\boldsymbol{h}_1\right) \\<br/>
&amp;= \text{Tr}\left(\frac{\partial l}{\partial\boldsymbol{h}_1}^T(\sigma&#39;(\boldsymbol{a}_1)\odot d\boldsymbol{a}_1)\right) \\<br/>
&amp;= \text{Tr}\left(\left(\frac{\partial l}{\partial\boldsymbol{h}_1}\odot \sigma&#39;(\boldsymbol{a}_1)\right)^Td\boldsymbol{a}_1\right)\\<br/>
\end{align*}<br/>
\]</p>

<p>得到 \(\frac{\partial l}{\partial \boldsymbol{a}_1}= \frac{\partial l}{\partial\boldsymbol{h}_1}\odot\sigma&#39;(\boldsymbol{a}_1)\)。为求 \(\frac{\partial l}{\partial W_1}\)，再用一次复合法则：<br/>
\[<br/>
\begin{align*}<br/>
dl_2 &amp;= \text{Tr}\left(\frac{\partial l}{\partial\boldsymbol{a}_1}^Td\boldsymbol{a}_1\right) \\<br/>
&amp;= \text{tr}\left(\frac{\partial l}{\partial\boldsymbol{a}_1}^TdW_1\boldsymbol{x}\right) \\<br/>
&amp;= \text{tr}\left(\boldsymbol{x}\frac{\partial l}{\partial\boldsymbol{a}_1}^TdW_1\right)<br/>
\end{align*}<br/>
\]</p>

<p>得到 \(\frac{\partial l}{\partial W_1}= \frac{\partial l}{\partial\boldsymbol{a}_1}\boldsymbol{x}^T\)。</p>

<p>推广：样本 \((\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_N,y_N)\)，\(l = -\sum_{i=1}^N \boldsymbol{y}_i^T\log\text{softmax}(W_2\sigma(W_1\boldsymbol{x}_i + \boldsymbol{b}_1) + \boldsymbol{b}_2)\)，其中 \(\boldsymbol{b}_1\) 是 \(p \times 1\) 列向量，\(\boldsymbol{b}_2\) 是 \(m\times 1\) 列向量，其余定义同上。</p>

<p>解1：定义 \(\boldsymbol{a}_{1,i} = W_1 \boldsymbol{x}_i + \boldsymbol{b}_1\)，\(\boldsymbol{h}_{1,i} = \sigma(\boldsymbol{a}_{1,i})\)，\(\boldsymbol{a}_{2,i} = W_2\boldsymbol{h}_{1,i} + \boldsymbol{b}_2\)，则 \(l = -\sum_{i=1}^N \boldsymbol{y}_i^T \log \text{softmax}(\boldsymbol{a}_{2,i})\)。先同上可求出 \(\frac{\partial l}{\partial \boldsymbol{a}_{2,i}} = \text{softmax}(\boldsymbol{a}_{2,i})-\boldsymbol{y}_i\) 。使用复合法则，<br/>
\[<br/>
\begin{align*}<br/>
dl &amp;= \text{tr}\left(\sum_{i=1}^N\frac{\partial l}{\partial \boldsymbol{a}_{2,i}}^T d \boldsymbol{a}_{2,i}\right) \\<br/>
&amp;= \text{tr}\left( \sum_{i=1}^N \frac{\partial l}{\partial \boldsymbol{a}_{2,i}}^T dW_2 \boldsymbol{h}_{1,i}\right) + \underbrace{\text{tr}\left( \sum_{i=1}^N \frac{\partial l}{\partial \boldsymbol{a}_{2,i}}^T W_2 d\boldsymbol{h}_{1,i}\right)}_{dl_2} + \text{tr}\left( \sum_{i=1}^N \frac{\partial l}{\partial \boldsymbol{a}_{2,i}}^T d \boldsymbol{b}_2\right)，<br/>
\end{align*}<br/>
\]</p>

<p>从第一项得到得到 \(\frac{\partial l}{\partial W_2}= \sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{2,i}}\boldsymbol{h}_{1,i}^T\)，从第二项得到 \(\frac{\partial l}{\partial \boldsymbol{h}_{1,i}}= W_2^T\frac{\partial l}{\partial\boldsymbol{a}_{2,i}}\)，从第三项得到到 \(\frac{\partial l}{\partial \boldsymbol{b}_2}= \sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{2,i}}\)。接下来对第二项继续使用复合法则，得到 \(\frac{\partial l}{\partial \boldsymbol{a}_{1,i}}= \frac{\partial l}{\partial\boldsymbol{h}_{1,i}}\odot\sigma&#39;(\boldsymbol{a}_{1,i})\)。为求 \(\frac{\partial l}{\partial W_1}, \frac{\partial l}{\partial \boldsymbol{b}_1}\)，再用一次复合法则：\(dl_2 = \text{tr}\left(\sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}^Td\boldsymbol{a}_{1,i}\right) = \text{tr}\left(\sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}^TdW_1\boldsymbol{x}_i\right) + \text{tr}\left(\sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}^Td\boldsymbol{b}_1\right)\)，得到 \(\frac{\partial l}{\partial W_1}= \sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}\boldsymbol{x}_i^T，\frac{\partial l}{\partial \boldsymbol{b}_1}= \sum_{i=1}^N \frac{\partial l}{\partial\boldsymbol{a}_{1,i}}\)。</p>

<p>解2：可以用矩阵来表示 \(N\) 个样本，以简化形式。定义 \(X = [\boldsymbol{x}_1, \cdots, \boldsymbol{x}_N]，A_1 = [\boldsymbol{a}_{1,1},\cdots,\boldsymbol{a}_{1,N}] =W_1 X + \boldsymbol{b}_1 \boldsymbol{1}^T，H_1 = [\boldsymbol{h}_{1,1}, \cdots, \boldsymbol{h}_{1,N}] = \sigma(A_1)，A_2 = [\boldsymbol{a}_{2,1},\cdots,\boldsymbol{a}_{2,N}] = W_2 H_1 + \boldsymbol{b}_2 \boldsymbol{1}^T\)，注意这里使用全1向量来扩展维度。先同上求出 \(\frac{\partial l}{\partial A_2} = [\text{softmax}(\boldsymbol{a}_{2,1})-\boldsymbol{y}_1, \cdots, \text{softmax}(\boldsymbol{a}_{2,N})-\boldsymbol{y}_N]\) 。使用复合法则，\(dl = \text{tr}\left(\frac{\partial l}{\partial A_2}^T d A_2\right) = \text{tr}\left( \frac{\partial l}{\partial A_2}^T dW_2 H_1 \right) + \underbrace{\text{tr}\left(\frac{\partial l}{\partial A_2}^T W_2 d H_1\right)}_{dl_2} + \text{tr}\left(\frac{\partial l}{\partial A_2}^T d \boldsymbol{b}_2 \boldsymbol{1}^T\right)\) ，从第一项得到 \(\frac{\partial l}{\partial W_2}= \frac{\partial l}{\partial A_2}H_1^T\)，从第二项得到 \(\frac{\partial l}{\partial H_1}= W_2^T\frac{\partial l}{\partial A_{2}}\)，从第三项得到到 \(\frac{\partial l}{\partial \boldsymbol{b}_2}= \frac{\partial l}{\partial A_2}\boldsymbol{1}\)。接下来对第二项继续使用复合法则，得到 \(\frac{\partial l}{\partial A_1}= \frac{\partial l}{\partial H_1}\odot\sigma&#39;(A_1)。为求\frac{\partial l}{\partial W_1}, \frac{\partial l}{\partial \boldsymbol{b}_1}\)，再用一次复合法则：\(dl_2 = \text{tr}\left(\frac{\partial l}{\partial A_1}^TdA_1\right) = \text{tr}\left(\frac{\partial l}{\partial A_1}^TdW_1X\right) + \text{tr}\left( \frac{\partial l}{\partial A_1}^Td\boldsymbol{b}_1 \boldsymbol{1}^T\right)\)，得到 \(\frac{\partial l}{\partial W_1}=  \frac{\partial l}{\partial A_1}X^T，\frac{\partial l}{\partial \boldsymbol{b}_1}= \frac{\partial l}{\partial A_1}\boldsymbol{1}\)。</p>

<h4 id="toc_3">矩阵对矩阵求导</h4>

<p>我们先定义向量 \(\mathbf f(p\times 1)\) 对向量 \(\mathbf x(m\times 1)\) 的导数 \(\frac{\partial \mathbf f}{\partial \mathbf x} = \left [\begin{array}{cccc} \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_1} \\ \frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_2} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_1}{\partial x_m} &amp; \frac{\partial f_2}{\partial x_m} &amp; \cdots &amp; \frac{\partial f_p}{\partial x_m} \\ \end{array} \right ]\)\((m\times p)\)，有 \(d\mathbf f = \frac{\partial \mathbf f}{\partial \mathbf x}^T d\mathbf x\)；再定义矩阵的向量化（按列优先）：\(\text{vec}(X) = [X_{11},\cdots,X_{m1},X_{12},\cdots,X_{m2},\cdots,X_{1n},\cdots,X_{mn}]^T\)\((mn\times 1)\)，并定义矩阵 \(F(p\times q)\) 对矩阵 \(X(m\times n)\) 的导数 \(\frac{\partial F}{\partial X} = \frac{\text{vec}(F)}{\text{vec}(X)}\)\((mn\times pq)\)。导数与微分有联系 \(\text{vec}(dF) = \frac{\partial F}{\partial X}^T \text{vec}(dX)\)。几点说明如下：</p>

<ol>
<li>按此定义，标量 \(f\) 对矩阵 \(X(m\times n)\) 的导数 \(\frac{\partial f}{\partial X}\) 是 \(mn\times 1\) 向量，与上面的定义不兼容，不过二者容易相互转换。为避免混淆，用记号 \(\nabla_X f\) 表示上篇定义的 \(m\times n\) 矩阵，则有 \(\frac{\partial f}{\partial X}=\mathrm{vec}(\nabla_X f)\)。虽然本篇的技术可以用于标量对矩阵求导这种特殊情况，但使用上篇中的技术更方便。</li>
<li>标量对矩阵的二阶导数，又称 Hessian 矩阵，定义为 \(\nabla^2_X f = \frac{\partial^2 f}{\partial X^2} = \frac{\partial \nabla_X f}{\partial X}\)\((mn\times mn)\)，是对称矩阵。对向量 \(\frac{\partial f}{\partial X}\) 或矩阵 \(\nabla_X f\) 求导都可以得到 Hessian 矩阵，但从矩阵 \(\nabla_X f\) 出发更方便。</li>
<li>\(\frac{\partial F}{\partial X} = \frac{\partial\mathrm{vec} (F)}{\partial X} = \frac{\partial F}{\partial \mathrm{vec}(X)} = \frac{\partial\mathrm{vec}(F)}{\partial \mathrm{vec}(X)}\)，求导时矩阵被向量化，弊端是这在一定程度破坏了矩阵的结构，会导致结果变得形式复杂；好处是多元微积分中关于梯度、Hessian矩阵的结论可以沿用过来，只需将矩阵向量化。例如优化问题中，牛顿法的更新 \(\Delta X\)，满足 \(\mathrm{vec}(\Delta X) = -(\nabla^2_X f)^{-1}\mathrm{vec}(\nabla_X f)\)。</li>
<li>在资料中，矩阵对矩阵的导数还有其它定义，比如 \(\frac{\partial F}{\partial X} = \left[\frac{\partial F_{kl}}{\partial X}\right](mp×nq)\)，它能兼容上篇中的标量对矩阵导数的定义，但微分与导数的联系（\(dF\) 等于 \(\frac{\partial F}{\partial X}\) 中每个 \(m\times n\) 子块分别与 \(dX\) 做内积）不够简明，不便于计算和应用。</li>
</ol>

<p>然后来建立运算法则。仍然要利用导数与微分的联系 \(\mathrm{vec}(dF) = \frac{\partial F}{\partial X}^T \mathrm{vec}(dX)\)，求微分的方法与上篇相同，而从微分得到导数需要一些向量化的技巧：</p>

<ol>
<li>线性：\(\mathrm{vec}(A+B) = \mathrm{vec}(A) + \mathrm{vec}(B)\)。</li>
<li>矩阵乘法：\(\mathrm{vec}(AXB) = (B^T \otimes A) \mathrm{vec}(X)\)，其中 \(\otimes\) 表示Kronecker积，\(A\)\((m\times n)\) 与 \(B\)\((p\times q)\) 的Kronecker积是 \(A\otimes B = [A_{ij}B]\)\((mp\times nq)\)。</li>
<li>转置：\(\mathrm{vec}(A^T) = K_{mn}\mathrm{vec}(A)\)，\(A\) 是 \(m\times n\) 矩阵，其中 \(K_{mn}\)\((mn\times mn)\) 是交换矩阵(commutation matrix)。</li>
<li>逐元素乘法：\(\mathrm{vec}(A\odot X) = \mathrm{diag}(A)\mathrm{vec}(X)\)，其中 \(\mathrm{diag}(A)\)\((mn\times mn)\) 是用 \(A\) 的元素（按列优先）排成的对角阵。</li>
</ol>

<p>观察一下可以断言，若矩阵函数 \(F\) 是矩阵 \(X\) 经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对 \(F\) 求微分，再做向量化并使用技巧将其它项交换至 \(\text{vec}(dX)\) 左侧，即能得到导数。</p>

<p>再谈一谈复合：假设已求得 \(\frac{\partial F}{\partial Y}\)，而 \(Y\) 是 \(X\) 的函数，如何求 \(\frac{\partial F}{\partial X}\) 呢？从导数与微分的联系入手，\(\mathrm{vec}(dF) = \frac{\partial F}{\partial Y}^T\mathrm{vec}(dY) = \frac{\partial F}{\partial Y}^T\frac{\partial Y}{\partial X}^T\mathrm{vec}(dX)\) ，可以推出链式法则 \(\frac{\partial F}{\partial X} = \frac{\partial Y}{\partial X}\frac{\partial F}{\partial Y}\)。</p>

<p>和标量对矩阵的导数相比，矩阵对矩阵的导数形式更加复杂，从不同角度出发常会得到形式不同的结果。有一些Kronecker积和交换矩阵相关的恒等式，可用来做等价变形：\((A\otimes B)^T = A^T \otimes B^T\)。\(\mathrm{vec}(\boldsymbol{ab}^T) = \boldsymbol{b}\otimes\boldsymbol{a}\)。\((A\otimes B)(C\otimes D) = (AC)\otimes (BD)\)。可以对 \(F = D^TB^TXAC\) 求导来证明，一方面，直接求导得到 \(\frac{\partial F}{\partial X} = (AC) \otimes (BD)\)；另一方面，引入 \(Y = B^T X A\)，有 \(\frac{\partial F}{\partial Y} = C \otimes D\), \(\frac{\partial Y}{\partial X} = A \otimes B\)，用链式法则得到 \(\frac{\partial F}{\partial X} = (A\otimes B)(C \otimes D)\)。\(K_{mn} = K_{nm}^T\), \(K_{mn}K_{nm} = I\)。\(K_{pm}(A\otimes B) K_{nq} = B\otimes A\)，\(A\) 是 \(m\times n\) 矩阵，\(B\) 是 \(p\times q\) 矩阵。可以对 \(AXB^T\) 做向量化来证明，一方面，\(\mathrm{vec}(AXB^T) = (B\otimes A)\mathrm{vec}(X)\)；另一方面，\(\mathrm{vec}(AXB^T) = K_{pm}\mathrm{vec}(BX^TA^T) = K_{pm}(A\otimes B)\mathrm{vec}(X^T) = K_{pm}(A\otimes B) K_{nq}\mathrm{vec}(X)\)。</p>

<p>接下来演示一些算例。</p>

<p>例1：\(F = AX\)，\(X\) 是 \(m\times n\) 矩阵，求 \(\frac{\partial F}{\partial X}\)。</p>

<p>解：先求微分：\(dF=AdX\)，再做向量化，使用矩阵乘法的技巧，注意在 \(dX\) 右侧添加单位阵：\(\mathrm{vec}(dF) = \mathrm{vec}(AdX) = (I_n\otimes A)\mathrm{vec}(dX)\)，对照导数与微分的联系得到 \(\frac{\partial F}{\partial X} = I_n\otimes A^T\)。</p>

<p>特例：如果 \(X\) 退化为向量，即 \(\boldsymbol{f} = A \boldsymbol{x}\)，则根据向量的导数与微分的关系 \(d\boldsymbol{f} = \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}^T d\boldsymbol{x}\)，得到 \(\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}} = A^T\)。</p>

<p>例2：\(f = \log |X|\) ，\(X\) 是 \(n\times n\) 矩阵，求 \(\nabla_X f\) 和 \(\nabla^2_X f\)。</p>

<p>解：使用上篇中的技术可求得 \(\nabla_X f = X^{-1T}\) 。为求 \(\nabla^2_X f\)，先求微分：\(d\nabla_X f = -(X^{-1}dXX^{-1})^T\)，再做向量化，使用转置和矩阵乘法的技巧 \(\mathrm{vec}(d\nabla_X f)= -K_{nn}\mathrm{vec}(X^{-1}dX X^{-1}) = -K_{nn}(X^{-1T}\otimes X^{-1})\mathrm{vec}(dX)\)，对照导数与微分的联系，得到 \(\nabla^2_X f = -K_{nn}(X^{-1T}\otimes X^{-1})\)，注意它是对称矩阵。在 \(X\) 是对称矩阵时，可简化为 \(\nabla^2_X f = -X^{-1}\otimes X^{-1}\)。</p>

<p>例3：\(F = A\exp(XB)\)，\(A\) 是 \(l\times m\) 矩阵，\(X\) 是 \(m\times n\) 矩阵，\(B\) 是 \(n\times p\) 矩阵，\(\exp\) 为逐元素函数，求\(\frac{\partial F}{\partial X}\)。</p>

<p>解：先求微分：\(dF = A(\exp(XB)\odot (dXB))\)，再做向量化，使用矩阵乘法的技巧：\(\mathrm{vec}(dF) = (I_p\otimes A)\mathrm{vec}(\exp(XB)\odot (dXB))\)，再用逐元素乘法的技巧：\(\mathrm{vec}(dF) = (I_p \otimes A) \mathrm{diag}(\exp(XB))\mathrm{vec}(dXB)\)，再用矩阵乘法的技巧：\(\mathrm{vec}(dF) = (I_p\otimes A)\mathrm{diag}(\exp(XB))(B^T\otimes I_m)\mathrm{vec}(dX)\)，对照导数与微分的联系得到 \(\frac{\partial F}{\partial X} = (B\otimes I_m)\mathrm{diag}(\exp(XB))(I_p\otimes A^T)\)。</p>

<p>例4【一元logistic回归】：\(l = -y \boldsymbol{x}^T \boldsymbol{w} + \log(1 + \exp(\boldsymbol{x}^T\boldsymbol{w}))\)，求 \(\nabla_\boldsymbol{w} l\) 和 \(\nabla^2_\boldsymbol{w} l\)。其中 \(y\) 是取值0或1的标量，\(\boldsymbol{x}\),\(\boldsymbol{w}\) 是 \(n\times 1\) 列向量。</p>

<p>解：使用上篇中的技术可求得 \(\nabla_\boldsymbol{w} l = \boldsymbol{x}(\sigma(\boldsymbol{x}^T\boldsymbol{w}) - y)\)，其中 \(\sigma(a) = \frac{\exp(a)}{1+\exp(a)}\) 为sigmoid函数。为求 \(\nabla^2_\boldsymbol{w} l\)，先求微分：\(d\nabla_\boldsymbol{w} l = \boldsymbol{x} \sigma&#39;(\boldsymbol{x}^T\boldsymbol{w})\boldsymbol{x}^T d\boldsymbol{w}\) ，其中 \(\sigma&#39;(a) = \frac{\exp(a)}{(1+\exp(a))^2}\) 为sigmoid函数的导数，对照导数与微分的联系，得到 \(\nabla_w^2 l = \boldsymbol{x}\sigma&#39;(\boldsymbol{x}^T\boldsymbol{w})\boldsymbol{x}^T\)。</p>

<p>推广：样本 \((\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_N,y_N)\)，\(l = \sum_{i=1}^N \left(-y_i \boldsymbol{x}_i^T\boldsymbol{w} + \log(1+\exp(\boldsymbol{x_i}^T\boldsymbol{w}))\right)\)，求 \(\nabla_w l\) 和 \(\nabla^2_w l\)。有两种方法，方法一：先对每个样本求导，然后相加；方法二：定义矩阵 \(X = \begin{bmatrix}\boldsymbol{x}_1^T \\ \vdots \\ \boldsymbol{x}_n^T \end{bmatrix}\)，向量 \(\boldsymbol{y} = \begin{bmatrix}y_1 \\ \vdots \\ y_n\end{bmatrix}\)，将 \(l\) 写成矩阵形式 \(l = -\boldsymbol{y}^T X\boldsymbol{w} + \boldsymbol{1}^T\log(\boldsymbol{1} + \exp(X\boldsymbol{w}))\)，进而可以求得 \(\nabla_\boldsymbol{w} l = X^T(\sigma(X\boldsymbol{w}) - \boldsymbol{y})，\nabla_w^2 l = X^T\text{diag}(\sigma&#39;(X\boldsymbol{w}))X\)。</p>

<p>例5【多元logistic回归】：\(l = -\boldsymbol{y}^T\log \text{softmax}(W\boldsymbol{x}) = -\boldsymbol{y}^TW\boldsymbol{x} + \log(\boldsymbol{1}^T\exp(W\boldsymbol{x}))\)，求 \(\nabla_W l\) 和 \(\nabla^2_W l\) 。其中其中 \(\boldsymbol{y}\) 是除一个元素为1外其它元素为0的 \(m\times 1\) 列向量，\(W\) 是 \(m\times n\) 矩阵，\(\boldsymbol{x}\) 是 \(n\times 1\) 列向量，l是标量。</p>

<p>解：上篇中已求得 \(\nabla_W l = (\text{softmax}(W\boldsymbol{x})-\boldsymbol{y})\boldsymbol{x}^T\)。为求 \(\nabla^2_W l\)，先求微分：定义 <br/>
\[<br/>
\begin{align*}<br/>
\boldsymbol{a} &amp;= W\boldsymbol{x}，d\text{softmax}(\boldsymbol{a}) \\<br/>
&amp;= \frac{\exp(\boldsymbol{a})\odot d\boldsymbol{a}}{\boldsymbol{1}^T\exp(\boldsymbol{a})} - \frac{\exp(\boldsymbol{a}) (\boldsymbol{1}^T(\exp(\boldsymbol{a})\odot d\boldsymbol{a}))}{(\boldsymbol{1}^T\exp(\boldsymbol{a}))^2}<br/>
\end{align*}<br/>
\]</p>

<p>这里需要化简去掉逐元素乘法，第一项中 \(\exp(\boldsymbol{a})\odot d\boldsymbol{a} = \text{diag}(\exp(\boldsymbol{a})) d\boldsymbol{a}\) ，第二项中 \(\boldsymbol{1}^T(\exp(\boldsymbol{a})\odot d\boldsymbol{a}) = \exp(\boldsymbol{a})^Td\boldsymbol{a}\)，故有 \(d\text{softmax}(\boldsymbol{a}) = D\text{softmax}(\boldsymbol{a})d\boldsymbol{a}\)，其中 \(D\text{softmax}(\boldsymbol{a}) = \frac{\text{diag}(\exp(\boldsymbol{a}))}{\boldsymbol{1}^T\exp(\boldsymbol{a})} - \frac{\exp(\boldsymbol{a})\exp(\boldsymbol{a})^T}{(\boldsymbol{1}^T\exp(\boldsymbol{a}))^2}\) ，代入有 \(d\nabla_W l =D \text{softmax}(\boldsymbol{a})d\boldsymbol{a}\boldsymbol{x}^T = D\text{softmax}(W\boldsymbol{x})dW \boldsymbol{x}\boldsymbol{x}^T\)，做向量化并使用矩阵乘法的技巧，得到 \(\nabla^2_W l = (\boldsymbol{x}\boldsymbol{x}^T) \otimes D\text{softmax}(W\boldsymbol{x})\)。</p>

<p>最后做个总结。我们发展了从整体出发的矩阵求导的技术，导数与微分的联系是计算的枢纽，标量对矩阵的导数与微分的联系是 \(df = \mathrm{tr}(\nabla_X^T f dX)\)，先对 \(f\) 求微分，再使用迹技巧可求得导数，特别地，标量对向量的导数与微分的联系是 \(df = \nabla^T_{\boldsymbol{x}}f d\boldsymbol{x}\)；矩阵对矩阵的导数与微分的联系是 \(\mathrm{vec}(dF) = \frac{\partial F}{\partial X}^T \mathrm{vec}(dX)\)，先对 \(F\) 求微分，再使用向量化的技巧可求得导数，特别地，向量对向量的导数与微分的联系是 \(d\boldsymbol{f} = \frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}^Td\boldsymbol{x}\)。</p>

<hr/>

<p><a href="https://zhuanlan.zhihu.com/p/27523007">矩阵求导浅析（一）</a><br/>
<a href="https://zhuanlan.zhihu.com/p/24709748">矩阵求导术（上）</a><br/>
<a href="https://zhuanlan.zhihu.com/p/24863977">矩阵求导术（下）</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15385007008840.html">次梯度 subgradient</a></h1>
			<p class="meta"><time datetime="2018-10-03T01:18:20+08:00" 
			pubdate data-updated="true">2018/10/3</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>次梯度方法是传统梯度下降算法的拓展，传统梯度下降算法是为了解决可导凸函数的问题，而次梯度方法主要是为了解决不可导梯度的问题。但是其算法收敛速度会相对较慢。</p>

<h3 id="toc_0">次梯度的定义</h3>

<p>次梯度是指对于函数 \(f\) 上的点 \(x\) 满足一下条件的 \(g\in\mathbb R^n\)： <br/>
\[<br/>
f(y) \geq f(x) + g^T(y-x)  <br/>
\]</p>

<p>即 </p>

<ol>
<li><p>若 \(f\) 是一个凸函数，若 \(f\) 在 \(x\) 处可导，则由一阶泰勒展开式：<br/>
\[ <br/>
f(y) \geq f(x) + \nabla  f(x)^T(y-x)  <br/>
\]</p></li>
<li><p>若 \(f\) 在 \(x\) 处不可导，则仍可得到一个下届： <br/>
\[<br/>
f(y) \geq f(x) + g^T(y-x) <br/>
\]</p>

<p>这个 \(g\) 就是 \(f\) 的子梯度。</p></li>
</ol>

<p>注意：虽然次梯度是针对不可导函数而设计的，但是可导函数也仍然可以使用，因此 \(f\) 是非凸函数也是可以的。</p>

<p>对于一个给定的点，可能不止一个这样的次梯度存在，而是一个次梯度的集合，这样的集合就叫做次微分（次导数），表示为 \(\partial f(x)\)。注意，如果微分存在的情况下，这样的次微分集合只包含一个元素，就是该点的梯度值，也就是蜕化为正常的梯度方式，所以说这是梯度的一种扩展。</p>

<h3 id="toc_1">次梯度的计算公式</h3>

<p>在点 \(x_0\) 的次导数的集合是一个非空闭区间 \([a,b]\) ，其中 \(a\) 和 \(b\) 是单侧极限<br/>
\[<br/>
a = lim_{x\rightarrow x_0^-} \frac{f(x) - f(x_0)}{x-x_0} \\<br/>
b = lim_{x\rightarrow x_0^+} \frac{f(x) - f(x_0)}{x-x_0} \\<br/>
\]</p>

<p>\(a\) 和 \(b\) 一定存在，且 \(a\le b\)。所有次导数的集合 \([a,b]\) 称为函数 \(f\) 在\(x_0\) 的次微分。</p>

<p>假设 \(f(x)=|x|\) 在 \(x=0\) 的次梯度为 \([-1, 1]\)。这是因为<br/>
\[<br/>
\begin{align*}<br/>
a &amp;= \lim_{x-&gt;0^-}\frac{|x|-0}{x}=\frac{-x}{x}=-1\\<br/>
b &amp;= \lim_{x-&gt;0^+}\frac{|x|-0}{x}=\frac{x}{x}=1\\<br/>
\end{align*}<br/>
\]</p>

<p>因此 \(f(x)\) 在 \(x=0\) 的次微分为 \([-1,1]\)。</p>

<h3 id="toc_2">次梯度的性质</h3>

<ol>
<li><strong>数乘不变性</strong>：\(\forall \alpha \ge 0, \partial(\alpha f)(x)=\alpha \partial f(x)\)</li>
<li><strong>加法不变性</strong>：\(\partial(f_1(x)+f_2(x)+...+f_m(x))=\partial f_1(x)+...+\partial f_m(x)\)</li>
<li><strong>放射特性</strong>：如果 \(f\) 是凸函数，那么 \(\partial f(Ax+b)=A^T\partial f(Ax+b)\)</li>
</ol>

<h3 id="toc_3">次梯度算法</h3>

<p>次梯度算法与梯度下降类似，仅仅是使用次梯度代替梯度，即： <br/>
\[<br/>
x^{(k)}=x^{(k-1)}-t_k  \cdot g^{(k-1)}, k=1,2,3... <br/>
\]</p>

<p>其中，\(g^{(k-1)} \in \partial f(x^{(k-1)})\) 为 \(f(x)\) 在 \(x\) 处的次梯度。 <br/>
与梯度下降算法不同的地方在于，次梯度算法并不是下降算法，每次对于参数的更新并不能保证代价函数是呈单调递减的趋势。</p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15345207395945.html">凸函数与严格凸函数、强凸函数</a></h1>
			<p class="meta"><time datetime="2018-08-17T23:45:39+08:00" 
			pubdate data-updated="true">2018/8/17</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>凸函数由于良好的性质，局部最优是全局最优，而有广泛的应用，这里简单做一个介绍。</p>

<h4 id="toc_0">凸函数</h4>

<p>设 \(f(x)\) 为定义 n 维欧氏空间中某个凸集 S 上的函数，若对于任何实数 \(\alpha\)（\(0\alpha 1\)）以及 \(S\) 中的任意不同两点 \(x\) 和 \(y\) ，均有<br/>
\[<br/>
\begin{equation}<br/>
f(\alpha x + (1-\alpha)y) \le \alpha f(x) + (1-\alpha)f(y)\label{fax}<br/>
\end{equation}<br/>
\]</p>

<p>则称 \(f(x)\) 为定义在凸集 \(S\) 上的凸函数。</p>

<p>凸函数的定义也很好理解，任意两点的连线必然在函数的上方，如下是一个典型的凸函数</p>

<div align="center">
    <img width="350" src="media/15345207395945/15379718476129.jpg" />
</div>

<h4 id="toc_1">严格凸函数</h4>

<p>如果将式 ( \ref{fax} ) 不等式中的 \(\le\) 改成 \(\lt\)，则称该函数为严格凸函数。</p>

<h4 id="toc_2">强凸函数</h4>

<p>强凸函数是指 \(\forall m \gt 0\)，\(f-\frac{m}{2} ||x||_2^2\) 也是凸的，其含义就是该凸函数的 “凸性” 比二次函数还要强，即使减去一个二次函数也是凸函数。</p>

<p>相比凸函数，强凸函数可以保证其梯度更大</p>

<div align="center">
    <img width="550" src="media/15345207395945/15380722342531.jpg" />
</div>

<p>如上的左边凸函数在使用梯度下降法收敛到最低点的时候，梯度很慢，甚至难以收敛；但是右图强凸函数可以保证梯度很大（右图下方为二次函数）。</p>

<h4 id="toc_3">凸函数的性质</h4>

<ol>
<li><p><strong>一阶特性（First-order characterization）</strong>：<br/>
\[<br/>
f(y) \ge f(x) + \nabla f(x)(y-x)<br/>
\]</p></li>
<li><p><strong>二阶特性（Second-order characterization）</strong>：<br/>
\[<br/>
\nabla^2 f(x) \ge 0<br/>
\]</p>

<p>这里的 \(\ge\) 表示 Hessian 矩阵是半正定的。</p></li>
<li><p><strong>Jensen不等式</strong>：<br/>
\[<br/>
f(\mathbb E(x)) \le \mathbb E(f(x))<br/>
\]</p></li>
</ol>

<p>其中，一阶特性或二阶特性是一个函数为凸函数的充要条件，通常用来证明一个函数是凸函数。</p>

<hr/>

<p><a href="http://wulc.me/2017/05/20/%E5%87%B8%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/">凸优化总结</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15342500383830.html">矩阵条件数与病态矩阵</a></h1>
			<p class="meta"><time datetime="2018-08-14T20:33:58+08:00" 
			pubdate data-updated="true">2018/8/14</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>矩阵A的条件数等于A的范数与A的逆的范数的乘积，即 \(\text{cond}(A)=‖A‖·‖A^{-1}‖\)，对应矩阵的3种范数，相应地可以定义3种条件数。</p>

<ul>
<li><p>\(\infty\)-条件数：<br/>
\[<br/>
\text{cond}(A) = ||A||\cdot ||A^{-1}||<br/>
\]</p></li>
<li><p>1-条件数<br/>
\[<br/>
\text{cond}(A) = ||A||_1\cdot ||A^{-1}||_1<br/>
\]</p></li>
<li><p>2-条件数<br/>
\[<br/>
\text{cond}(A) = ||A||_2\cdot ||A^{-1}||_2 = \sqrt{\frac{\lambda_1}{\lambda_n}}<br/>
\]</p></li>
</ul>

<p>其中 \(\lambda_1\) 是 \(A^HA\) 的最大特征值，\(\lambda_n\) 是 \(A^HA\) 的最小特征值。</p>

<blockquote>
<p>向量范数的计算，对向量 \(x\)</p>

<ol>
<li>\(||x||_1 = |x_1| + |x_2| + ... + |x_n|\)</li>
<li>\(||x||_2 = \sqrt{|x_1|^2 + |x_2|^2 + ... + |x_n|^2}\)</li>
<li>\(||x||_\infty = \max_{1\le i \le n}(|x_i|)\)</li>
<li>\(||x||_{-\infty} = \min_{1\le i \le n} (|x_i|)\)</li>
</ol>

<p>例如向量 \(z = [-5,6,8,10]\)，则 \(||z||_1 = 29\)，\(||z||_2 = 15\)，\(||z||_\infty = 10\)，\(||z||_{-\infty} = 5\)</p>

<p>矩阵范数的计算，对矩阵 \(A\)</p>

<ol>
<li>矩阵的一阶范数（列范数）：矩阵的每一列上的元素绝对值先求和，再从中取个最大的
\[
|A|_1 = \max_{1\le j \le n}\sum_{i=1}^n |a_{ij}|
\]</li>
<li>矩阵的二阶范数：矩阵 \(A^TA\) 的最大特征值开平方根
\[
|A|_2 = \sqrt{\lambda_{max}(A^T A)}
\]</li>
<li>矩阵的无穷范数（行范数）：矩阵的每一行上的元素绝对值先求和，再从中取个最大的
\[
|A|_1 = \max_{1\le i \le n}\sum_{j=1}^n |a_{ij}|
\]</li>
</ol>

<p>例如矩阵 \(A = \left [ \begin{array}{ccc} -1&amp;2&amp;-3\\ 4&amp;-6&amp;6\\\end{array}\right ]\)，则 \(||A||_1 = 9\)，\(||A||_2 = 10.0623\)，\(||A||_{\infty} = 16\)</p>

<p>另外常用的范数</p>

<ol>
<li><strong>矩阵的L0范数</strong>：矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏，上述矩阵A最终结果就是：6</li>
<li><strong>矩阵的L1范数</strong>：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵A最终结果就是：22</li>
<li><strong>矩阵的F范数</strong>：矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数，它的有点在它是一个凸函数，可以求导求解，易于计算，上述矩阵A最终结果就是：10.0995</li>
<li><strong>矩阵的L21范数</strong>：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数，上述矩阵A最终结果就是：17.1559</li>
</ol>
</blockquote>

<h4 id="toc_0">矩阵条件数的性质</h4>

<ol>
<li><p>\(\text{cond}(A) \ge 1\)</p>

<p>证明：\(\text{cond}(A) = ||A^{-1}||\cdot ||A|| \ge ||A^{-1}A = ||I|| = 1\)</p></li>
<li><p>\(\text{cond}(A) = \text{cond}(A^{-1})\)</p>

<p>证明：\(\text{cond}(A^{-1}) = ||A^{-1}||\cdot ||(A^{-1})^{-1}|| = ||A^{-1}||\cdot ||A|| = \text{cond}(A)\)</p></li>
<li><p>\(\text{cond}(\alpha A) = \text{cond}(A),\alpha \ne 0,\alpha \in \mathbb R\)</p>

<p>证明：\(\text{cond}(\alpha A) = ||\alpha A||\cdot ||(\alpha A)^{-1}|| = |\alpha|\cdot ||A|| \cdot \frac{1}{|\alpha|}\cdot ||A^{-1}|| \\<br/>
= ||A||\cdot ||A^{-1}|| = \text{cond}(A)\)</p></li>
<li><p>如果 \(U\) 是酉矩阵，则<br/>
\[<br/>
\text{cond}_2(U) = 1\\<br/>
\text{cond}_2(UA) = \text{cond}_2(AU) = \text{cond}_2(A)\\<br/>
\]</p></li>
<li><p>A，B可逆\(\text{cond}(AB) \le \text{cond}(A) \cdot \text{cond}(B)\)</p>

<p>证明：\(\text{cond}(AB) = ||AB||\cdot ||(AB)^{-1}|| \le ||A||\cdot ||B||\cdot ||A^{-1}|| \cdot ||B^{-1}|| \\<br/>
= \text{cond}(A)\cdot \text{cond}(B)<br/>
\)</p></li>
</ol>

<h4 id="toc_1">条件数的应用</h4>

<ol>
<li>若条件数 \(\text{cond}(A)\) 较小，就称 \(A\) 关于求逆矩阵或解线性方程组为<strong>良态的</strong>或<strong>好条件的</strong>。</li>
<li><p>若条件数 \(\text{cond}(A)\) 较大，就称 \(A\) 关于求逆矩阵或解线性方程组为<strong>病态的</strong>或<strong>坏条件的</strong>。</p>

<p>注意：\(\text{cond}(A)\) 多大 \(A\) 算病态，通常没有具体的定量标准。</p></li>
</ol>

<p>现有线性系统：\(Ax = b\)，解方程<br/>
\[<br/>
\left[\begin{array}{cc}400&amp;-201\\-800&amp;401\\\end{array}\right ] \left [ \begin{array}{c}x_1\\x_2\\\end{array}\right ] = \left [ \begin{array}{c}200\\-200\\\end{array}\right ]<br/>
\]</p>

<p>很容易得到解为：\(x_1 = -100\), \(x_2 = -200\)。如果在样本采集时存在一个微小的误差，比如，将 \(A\) 矩阵的系数 400 改变成 401：<br/>
\[<br/>
\left[\begin{array}{cc}401&amp;-201\\-800&amp;401\\\end{array}\right ] \left [ \begin{array}{c}x_1\\x_2\\\end{array}\right ] = \left [ \begin{array}{c}200\\-200\\\end{array}\right ]<br/>
\]</p>

<p>则得到一个截然不同的解：\(x_1 = 40000, x_2 = 79800\)。</p>

<p>当解集 x 对 A 和 b 的系数高度敏感，那么这样的方程组就是病态的 (ill-conditioned)。</p>

<h4 id="toc_2">病态的由来</h4>

<p>一个极端的例子，当A奇异时，条件数为无穷，这时即使不改变b，x也可以改变。奇异的本质原因在于矩阵有0特征值，\(x\) 在对应特征向量的方向上运动不改变 \(Ax\) 的值。如果一个特征值比其它特征值在数量级上小很多，\(x\) 在对应特征向量方向上很大的移动才能产生 \(b\) 微小的变化，这就解释了为什么这个矩阵为什么会有大的条件数，事实上，正规阵在二范数下的条件数就可以表示成 abs (最大特征值/最小特征值)。</p>

<h4 id="toc_3">\(L^2\) 范数解救病态</h4>

<p>L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题。因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解为：<br/>
\[<br/>
\hat w = (X^TX)X^Ty<br/>
\]</p>

<p>然而，如果当我们的样本 \(X\) 的数目比每个样本的维度还要小的时候，矩阵 \(X^TX\) 将会不是满秩的，即 \(\text{det}(X^TX)=0\)，则 \(X^TX\) 会变得不可逆，所以 \(\hat{w}\) 就没办法直接计算出来了。</p>

<p>但如果加上L2正则项，就变成了下面这种情况，就可以直接求逆了：<br/>
\[<br/>
\hat w = (X^TX + \alpha I)X^T y<br/>
\]</p>

<p>病态矩阵解集的不稳定性是由于解集空间包含了自由度过大的方向，解决这个问题的关键就是将这些方向去掉。其中单位矩阵I的维度为参数向量的维度。自由度太大会造成过拟合泛化能力极差，同时系统又极不稳定。当我们加入正则项之后，A就变成了满秩矩阵，便可以直接求逆了。</p>

<hr/>

<p><a href="https://blog.csdn.net/Michael__Corleone/article/details/75213123">向量和矩阵的各种范数比较（1范数、2范数、无穷范数等等）</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15335638026316.html">多元函数极值与Hessian矩阵的关系</a></h1>
			<p class="meta"><time datetime="2018-08-06T21:56:42+08:00" 
			pubdate data-updated="true">2018/8/6</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>海塞矩阵（Hessian Matrix），又译作海森矩阵，是一个多元函数的二阶偏导数构成的方阵。在机器学习和图像处理（例如SIFT和SURF特征检测）中，经常可以遇到它。</p>

<h3 id="toc_0">一元函数极值问题</h3>

<p>如果我们需要求一个一元函数的极值，比如 \(f(x) = x^2\)，我们必然想到要求函数的一阶导数，并令结果等于0：<br/>
\[<br/>
f&#39;(x) = 2x<br/>
\]</p>

<p>当时一阶导数为0，不总是能求出极值。我们知道在 \(x_0\) 点的一阶导数值在几何意义上表示函数在 \(x_0\) 点的切线。下图中，可以清晰说明导数为0，不一定就是极限值。</p>

<div align="center">
    <img width="350" src="media/15335638026316/15335656524655.jpg" />
</div>

<p>在B点导数为0，该点却不是极值点，而是驻点。</p>

<p>将一元函数 \(f(x)\) 在 \(x_0\) 处用二阶近似得<br/>
\[<br/>
f(x) = f(x_0) + f&#39;(x_0)(x-x_0) + \frac 1 2 f&#39;&#39;(x_0)(x-x_0)^2<br/>
\]</p>

<p>当 \(f&#39;(x_0)=0\) 时，如果二阶导数 \(f&#39;&#39;(x_0) \gt 0\)，可得在 \(x_0\) 旁边任意点有 \(f(x) \gt f(x_0)\)，那么 \(x_0\) 就是该局部极小值。如果二阶导数 \(f&#39;&#39;(x_0) \lt 0\)，可得在 \(x_0\) 旁边任意点有 \(f(x) \lt f(x_0)\)，\(x_0\) 是该局部极大值。如果 \(f&#39;&#39;(x_0)=0\)，则结果仍然是不确定的，我们就不得不再通过其他方式来确定函数的极值性。</p>

<h3 id="toc_1">多元函数极值问题</h3>

<p>类似的，如果在多元函数中求极值点，可以通过一阶导数等于0 和二阶导数来判断。假设 \(f\) 是一个二元函数<br/>
\[<br/>
f=(x,y)<br/>
\]</p>

<p>\(f\) 的一阶导数<br/>
\[<br/>
\begin{align*}<br/>
\frac{\partial f}{\partial x} &amp;= 0\\<br/>
\frac{\partial f}{\partial y} &amp;= 0\\<br/>
\end{align*}<br/>
\]</p>

<p>\(f\) 的二阶导数有9个，如果用矩阵形式表示就是<br/>
\[<br/>
H = \left [\begin{array}\\<br/>
\frac{\partial f}{\partial x\partial x}&amp;\frac{\partial f}{\partial x\partial y}\\<br/>
\frac{\partial f}{\partial y\partial x}&amp;\frac{\partial f}{\partial y\partial y}\\<br/>
\end{array} \right ]<br/>
\]</p>

<p>如果将多元函数在 \((x_0,y_0)\) 处二阶近似得<br/>
\[<br/>
f(x,y) = f(x_0,y_0) + [x-x_0,y-y_0]\left [\begin{array}\\\frac{\partial f}{\partial x}\\\frac{\partial}{\partial y}\\\end{array}\right ] + \frac 1 2 [x-x_0,y-y_0] \left [\begin{array}\\<br/>
\frac{\partial f}{\partial x\partial x}&amp;\frac{\partial f}{\partial x\partial y}\\<br/>
\frac{\partial f}{\partial y\partial x}&amp;\frac{\partial f}{\partial y\partial y}\\<br/>
\end{array} \right ] \left[ \begin{array}\\x-x_0\\y-y_0\\\end{array}\right ]<br/>
\]</p>

<p>令 \(\triangle x = [x-x_0,y-y_0]\)，上式简化表示即<br/>
\[<br/>
f(x,y) = f(x_0,y_0) + \triangle x \nabla f + \triangle x H \triangle x^T<br/>
\]</p>

<p>二元函数 \(f(x,y)\) 一阶导数为0，即 \(\triangle x \nabla f = 0\)，所以<br/>
\[<br/>
f(x,y) = f(x_0,y_0) + \triangle x H \triangle x^T<br/>
\]</p>

<p>如果 \(\triangle x H \triangle x^T \gt 0\)，则 \(f(x,y) \gt f(x_0,y_0)\) 即 \((x_0,y_0)\) 是函数局部极小值。因为 \(H\) 是实对称矩阵，当 \(\triangle x H \triangle x^T \gt 0\) 时，可以说 \(H\) 是正定矩阵。所以我们可以得出结论</p>

<ul>
<li>当 \(H\) 是正定矩阵时，\((x_0,y_0)\) 是函数局部极小值；</li>
<li>当 \(H\) 是负定矩阵时，\((x_0,y_0)\) 是函数局部极大值；</li>
<li>当 \(H\) 是不定矩阵时，\((x_0,y_0)\) 不是函数极值点；</li>
</ul>

<hr/>

<p><a href="https://blog.csdn.net/baimafujinji/article/details/51167852">Hessian矩阵与多元函数极值</a><br/>
<a href="https://blog.csdn.net/chduan_10/article/details/78075112">怎么理解二阶偏导与凸函数的Hessian矩阵是半正定的</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15225424238056.html">数学期望与条件期望</a></h1>
			<p class="meta"><time datetime="2018-04-01T08:27:03+08:00" 
			pubdate data-updated="true">2018/4/1</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>在概率论和统计学中，数学期望(Expectation)（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和，是最基本的数学特征之一。它反映随机变量平均取值的大小。</p>

<p>需要注意的是，期望值并不一定等同于常识中的“期望”——“期望值”也许与每一个结果都不相等。期望值是该变量输出值的平均数。期望值并不一定包含于变量的输出值集合里。</p>

<p>大数定律规定，随着重复次数接近无穷大，数值的算术平均值几乎肯定地收敛于期望值。</p>

<h3 id="toc_0">离散型数学期望</h3>

<p>离散型随机变量 \(X\) 的取值为 \(X_1,X_2,...,X_n\)，\(P(X_1),P(X_2),...,P(X_n)\) 为 \(X\) 对应取值的概率，则：<br/>
\[<br/>
\mathbb E(X) = X_1\cdot P(X_1) + X_2 \cdot P(X_2) + ... + X_n\cdot P(X_n) = \sum_{i=1}^n X_i P(X_i)<br/>
\]</p>

<p>若随机变量 \(Y\) 符合函数 \(Y=g(x)\)，则有：<br/>
\[<br/>
\mathbb E(Y) = \mathbb E[g(x)] = \sum_x g(x) f(x)<br/>
\]</p>

<h3 id="toc_1">连续型数学期望</h3>

<p>设连续性随机变量 \(X\) 的概率密度函数为 \(f(x)\)，若积分绝对收敛，则随机变量 \(X\) 的数学期望为：<br/>
\[<br/>
\mathbb E(x) = \int_{-\infty}^{\infty} x f(x) \mathbf dx<br/>
\]</p>

<p><strong>定理</strong>：若随机变量 \(Y\) 符合函数 \(Y=g(x)\)，且 \(\int_{-\infty}^{\infty} g(x)f(x) dx\) 绝对收敛，则有：<br/>
\[<br/>
\mathbb E(Y) = \mathbb E[g(x)] = \int_{-\infty}^{\infty} g(x)f(x) \mathbf dx<br/>
\]</p>

<p>该定理的意义在于：我们求 \(Y\) 时不需要算出 \(Y\) 的分布律或者概率密度，只要利用 \(X\) 的分布律或概率密度即可。</p>

<p>上述定理还可以推广到两个或以上随机变量的函数情况。<br/>
设 \(Z\) 是随机变量 \(X\)、\(Y\) 的函数（\(g\) 是连续函数），\(Z\) 是一个一维随机变量，二维随机变量（\(X\)，\(Y\)）的概率密度为 \(f(x,y)\)，则有：<br/>
\[<br/>
\mathbb E(Z) = \mathbb E[g(X,Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y)f(x,y) \mathbf dx\mathbf dy<br/>
\]</p>

<h3 id="toc_2">性质</h3>

<ol>
<li>\(\mathbb E(C) = C\)</li>
<li>\(\mathbb E(CX) = C\mathbb E(X)\)</li>
<li>\(\mathbb E(X+Y) = \mathbb E(X) + \mathbb E(Y)\)</li>
<li>当 \(X\) 与 \(Y\) 相互独立时，\(\mathbb E(X,Y) = \mathbb E(X)\mathbb E(Y)\)</li>
</ol>

<h3 id="toc_3">条件期望 Conditional Exceptation</h3>

<p>定义在给定 \(Y\) 下随机变量 \(X\) 的条件期望为：<br/>
\[<br/>
\mathbb E(X|Y) = \left \{\begin{array}\\<br/>
\sum_{x} x f_{X|Y}(x|Y)\quad&amp;\text{if } X \text{ is discrete}\\[10pt]<br/>
\int_{\infty}^{\infty} xf_{X|Y}(x|Y) \mathbf dx\quad&amp;\text{if }X \text{ is continuous}\\[10pt]<br/>
\end{array} \right .<br/>
\]</p>

<p>注意因为 \(f_{X|Y}(x|Y)\) 中在参数中包含随机变量 \(Y\)，所以条件期望也是个随机变量。我们可以定义在特定 \(Y\) 值的 \(X\) 的条件期望，如果 \(Y\) 是连续型随机变量：<br/>
\[<br/>
\mathbb E[X|Y=y] = \left \{\begin{array}\\<br/>
\sum_{x} x f_{X|Y}(x|y)\quad&amp;\text{if } X \text{ is discrete}\\[10pt]<br/>
\int_{\infty}^{\infty} xf_{X|Y}(x|y) \mathbf dx\quad&amp;\text{if }X \text{ is continuous}\\[10pt]<br/>
\end{array} \right .<br/>
\] </p>

<p>其中：<br/>
\[<br/>
f_{X}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}<br/>
\]</p>

<p>如果 \(Y\) 是离散型随机变量，条件期望为：<br/>
\[<br/>
\mathbb E[X|Y=y] = \left \{\begin{array}\\<br/>
\sum_{x} x f_{X}(x|Y=y)\quad&amp;\text{if } X \text{ is discrete}\\[10pt]<br/>
\int_{\infty}^{\infty} xf_{X}(x|Y=y) \mathbf dx\quad&amp;\text{if }X \text{ is continuous}\\[10pt]<br/>
\end{array} \right .<br/>
\] </p>

<p>其中<br/>
\[<br/>
f_{X}(x|Y=y) = \frac{f_{X,Y}(x,y)}{P(Y=y)}<br/>
\]</p>

<h4 id="toc_4">条件期望性质</h4>

<ol>
<li><p><strong>线性</strong>：假设 \(X\)，\(Y\) 是连续随机变量：<br/>
\[<br/>
\mathbb E[\alpha X+ \beta Z|Y] = \alpha \mathbb E(X|Y) + \beta\mathbb E(Z|Y)<br/>
\]</p>

<p>其中 \(\alpha\) 和 \(\beta\) 是常量。</p></li>
<li><p><strong>独立性</strong>：如果 \(X\) 与 \(Y\) 独立，则有：<br/>
\[<br/>
\mathbb E(X|Y) = \mathbb E(X)<br/>
\]</p>

<p>证明：假设 \(X\)，\(Y\) 都是连续随机变量，<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E(X|Y) &amp;= \int_{-\infty}^{\infty} x f_{X|Y}(x|y) \mathbf dx\\<br/>
&amp;= \int_{-\infty}^{\infty} x \frac{f_{X,Y}(x,y)}{f_Y(y)} \mathbf dx\\<br/>
&amp;= \int_{-\infty}^{\infty} x \frac{f_{X}(x) f_{Y}(y)}{f_Y(y)} \mathbf dx\\<br/>
&amp;= \int_{-\infty}^{\infty} x f_{X}(x) \mathbf dx\\<br/>
&amp;= \mathbb E(X)<br/>
\end{align*}<br/>
\]</p></li>
<li><p><strong>稳定性</strong>：如果 \(Y\) 是一个随机变量，<br/>
\[<br/>
\mathbb E[Xg(Y)|Y] = g(Y)\mathbb E(X|Y)<br/>
\]</p>

<p>特例：\(\mathbb E[g(Y) | Y] = g(Y)\)</p>

<p>证明：假设 \(X\) 和 \(Y\) 是连续随机变量，<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E[Xg(Y)|Y=y] &amp;= \int_{-\infty}^{\infty} x g(y)f_{X|Y}(x|y) \mathbf dx\\<br/>
&amp;= g(y)\int_{-\infty}^{\infty} x f_{X|Y}(x|y) \mathbf dx\\<br/>
&amp;= g(y) \mathbb E[X|Y=y]<br/>
\end{align*}<br/>
\]</p></li>
<li><p><strong>迭代期望法则</strong>：英文 Law of Iterated Expectations，LIE，如果 \(Y\) 是一个随机变量，期望是 \(\mathbb E(Y)\)，\(X\) 是相同概率空间的随机变量，有：<br/>
\[<br/>
\mathbb E[\mathbb E(Y|X)] = \mathbb E(Y)<br/>
\]</p>

<p>也就是给定 \(X\) 下的 \(Y\) 的条件期望值的期望等于 \(Y\) 的期望。</p>

<p>证明：假设 \(X\) 和 \(Y\) 是连续型随机变量，令 \(g(x) = \mathbb E(Y|X=x)\)，<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E[g(X)] &amp;= \int_{-\infty}^{-\infty} g(x) f_X(x) \mathbf dx = \int_{-\infty}^{-\infty} \mathbb E(Y|X=x) f_X(x) \mathbf dx\\<br/>
&amp;= \int_{-\infty}^{-\infty} \bigg( \int_{-\infty}^{-\infty} y f_{Y|X}(y|x) \mathbf dy \bigg) f_X(x) \mathbf dx\\<br/>
&amp;= \int_{-\infty}^{-\infty} \bigg( \int_{-\infty}^{-\infty} y \frac{f_{Y,X}(y,x)}{f_X(x)} \mathbf dy \bigg) f_X(x) \mathbf dx\\<br/>
&amp;= \int_{-\infty}^{-\infty}\int_{-\infty}^{-\infty} y f_{X,Y}(x,y) \mathbf dy\mathbf dx\\<br/>
&amp;= \int_{-\infty}^{-\infty} y\bigg(\int_{-\infty}^{-\infty} f_{X,Y}(x,y) \mathbf dx\bigg) \mathbf dy\\<br/>
&amp;= \int_{-\infty}^{-\infty} y f_Y(y) \mathbf dy\\<br/>
&amp;= \mathbb E(Y)<br/>
\end{align*}<br/>
\]</p>

<blockquote>
<p>\[<br/>
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) dx<br/>
\]</p>
</blockquote>

<p>再看另外一个：<br/>
\[<br/>
\mathbb E[\mathbb E[g(X,Y) | Y]] = \mathbb E[g(X,Y)]<br/>
\]</p>

<p>证明：假设 \(X\) 和 \(Y\) 是随机变量，<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E[\mathbb E[g(X,Y) | Y]] &amp;= \int_{-\infty}^{\infty} \mathbb E[g(X,Y)|Y=y] f_Y(y) \mathbf dy\\<br/>
&amp;= \int_{-\infty}^{\infty} \bigg(\int_{-\infty}^{\infty} g(x,y) f_{X|Y}(x|y) \mathbf dx \bigg) f_Y(y) \mathbf dy\\<br/>
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f_{X|Y}(x|y) f_Y(y) \mathbf dx \mathbf dy\\<br/>
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f_{X,Y}(x,y) \mathbf dx \mathbf dy\\<br/>
&amp;= \mathbb E[g(X,Y)]<br/>
\end{align*}<br/>
\]</p></li>
<li><p><strong>Tower property</strong>：对于随机变量 \(X\)、\(Y\) 和 \(Z\)：<br/>
\[<br/>
\mathbb E[\mathbb E(X|Y,Z)|Y] = \mathbb E(X|Y)<br/>
\]</p>

<p>证明：假设 \(X\)、\(Y\) 和 \(Z\) 是连续型随机变量，令 \(g(y,z) = \mathbb E(X|Y,Z)\) ，<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E[\mathbb E(X|Y,Z)|Y=y] &amp;= \int_{-\infty}^{\infty} \mathbb E(X|Y=y,Z=z) f_{Z|Y}(z|y) \mathbf dz\\<br/>
&amp;= \int_{-\infty}^{\infty} \bigg[\int_{-\infty}^{\infty} x f_{X|Y,Z}(x|y,z) \mathbf dx \bigg] f_{Z|Y}(z|y) \mathbf dz\\<br/>
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f_{X|Y,Z}(x|y,z) f_{Z|Y}(z|y) \mathbf dx \mathbf dz\\<br/>
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \frac{f_{X,Y,Z}(x,y,z)}{f_{Y,Z}(y,z)}\frac{f_{Z,Y}(z,y)}{f_{Y}(y)} \mathbf dx \mathbf dz\\<br/>
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \frac{f_{X,Y,Z}(x,y,z)}{f_{Y}(y)} \mathbf dx \mathbf dz\\<br/>
&amp;= \int_{-\infty}^{\infty} x \frac{\int_{-\infty}^{\infty} f_{X,Y,Z}(x,y,z) \mathbf dz}{f_{Y}(y)} \mathbf dx\\<br/>
&amp;= \int_{-\infty}^{\infty} x \frac{f_{X,Y}(x,y) }{f_{Y}(y)} \mathbf dx\\<br/>
&amp;= \int_{-\infty}^{\infty} x {f_{X|Y}(x|y)} \mathbf dx\\<br/>
&amp;= \mathbb E(X|Y=y)\\<br/>
\end{align*}<br/>
\]</p></li>
</ol>

<h4 id="toc_5">期望的一般形式</h4>

<p>普通期望：<br/>
\[<br/>
\mathbb E[X] = \left\{ \begin{array}\\<br/>
\int_{\mathcal X} x dF_X(x) = \int_{\mathcal X} x f_X(x) dx\quad&amp;\text{if }X\text{ is continuous}\\[10pt]<br/>
\int_{\Omega} X(\omega) dP(\omega)\quad&amp;\text{if }X\text{ is discrete}\\[10pt]<br/>
\end{array} \right .<br/>
\]</p>

<p>条件期望：<br/>
\[<br/>
\mathbb E[g(X|Y)|Y=y] = \left\{\begin{array}\\<br/>
\int_{\mathcal X} g(x,y) dF_{X|Y}(x|y) = \int_{\mathcal X} g(x,y) f_{X|Y}(x|y) dx\quad&amp;\text{if }X\text{ is continuous}\\[10pt]<br/>
\int_{\Omega} g(X(\omega),y) dP(\omega|Y=y)\quad&amp; \text{if }X\text{ is discrete}\\[10pt]<br/>
\end{array} \right.<br/>
\]</p>

<hr/>

<p><a href="https://en.wikipedia.org/wiki/Conditional_expectation">Conditional expectation</a><br/>
<a href="http://math.arizona.edu/%7Etgk/464_07/cond_exp.pdf">Conditional expectation</a><br/>
<a href="https://nptel.ac.in/courses/111103022/module5/lec2.pdf">Conditional expectation</a><br/>
<a href="https://people.maths.bris.ac.uk/%7Emb13434/cont_cond.pdf">Continuous conditional distributions</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15225083436209.html">Mcdiarmid 不等式</a></h1>
			<p class="meta"><time datetime="2018-03-31T22:59:03+08:00" 
			pubdate data-updated="true">2018/3/31</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>假设 \(Z_1,Z_2,...,Z_m\) 是集合 \(\mathcal Z\) 中的独立随机变量。进一步，假设 \(f:\mathcal Z_m\rightarrow \mathbb R\) 是关于 \(Z_1,Z_2,...,Z_n\) 的函数，对于所有的 \(i\) ，都有 \(z_1,z_2,...,z_n,z&#39;_i\in \mathcal X\) 满足：<br/>
\[<br/>
\sup_{z_1,...,z_n,z&#39;_i}|f(z_1,z_2,...,z_i,...,z_n) - f(z_1,z_2,...,z&#39;_i,...,z_n)| \le c_i<br/>
\]</p>

<p>对于所有的 \(\epsilon \gt 0\) 有：<br/>
\[<br/>
\begin{equation}<br/>
P\bigg[\bigg|f(Z_1,...,Z_n) - \mathbb E\Big(f(Z_1,...,Z_n)\Big)\bigg| \ge \epsilon\bigg] \le 2\exp\bigg(\frac{-2\epsilon^2}{\sum_{i=1}^n c_i^2 } \bigg)\label{pbb}<br/>
\end{equation}<br/>
\]</p>

<h3 id="toc_0">证明</h3>

<p>令 \(Y = f(Z_1,...,Z_n)\) 和 \(\mu = \mathbb E(f(Z_1,...,Z_n))\) ，有：<br/>
\[<br/>
P(|Y-\mu| \ge \epsilon) = P(Y-\mu \ge \epsilon) + P(Y-\mu \le -\epsilon)<br/>
\]</p>

<p>我们现在来求不等式右边第一项的边界，第二项可以用相似的方法。</p>

<p>令 \(V_i = \mathbb E(Y|Z_1,...,Z_i) - \mathbb E(Y|Z_1,...,Z_{i-1})\) ，有：<br/>
\[<br/>
\begin{align*}<br/>
\sum_{i=1}^n V_i &amp;= V_1 + V_2 + ... + V_n\\<br/>
&amp;=  \mathbb E(Y|Z_1) - \mathbb E(Y) + \mathbb E(Y|Z_1,Z_2) - \mathbb E(Y|Z_1) + ... + \mathbb E(Y|Z_1,...,Z_n) - \mathbb E(Y|Z_1,...,Z_{n-1})\\[6pt]<br/>
&amp;= \mathbb E(Y|Z_1,...,Z_n) - \mathbb E(Y)\\[6pt]<br/>
&amp;= \mathbb E(f(Z_1,...,Z_n)|Z_1,...,Z_n) - \mathbb E(f(Z_1,...,Z_n)\\[6pt]<br/>
&amp;= f(Z_1,...,Z_n) - \mathbb E(f(Z_1,...,Z_n))\\<br/>
\end{align*}<br/>
\]</p>

<p>要证明的原式 \ref{pbb} 等价于证明下式：<br/>
\[<br/>
\begin{align}<br/>
P\bigg[f(Z_1,...,Z_n) - \mathbb E\Big(f(Z_1,...,Z_n)\Big) \ge \epsilon\bigg] &amp;= P\bigg(\sum_{i=1}^n V_i \ge \epsilon\bigg) \label{pbbf1}\\<br/>
&amp;= P\Big(e^{t\sum_{i=1}^n V_i} \ge e^{t\epsilon}\Big) \label{pbbf2}\\<br/>
&amp;= e^{-t\epsilon} \mathbb E\Big(e^{t\sum_{i=1}^n V_i}\Big) \label{pbbf3}\\<br/>
&amp;= e^{-t\epsilon} \mathbb E\Big(\mathbb E\big[ e^{t\sum_{i=1}^n V_i} \big |Z_1,...,Z_{n-1}\big]\Big) \label{pbbf4}\\<br/>
&amp;= e^{-t\epsilon} \mathbb E\Big(\mathbb E\big[ e^{t\sum_{i=1}^{n-1} V_i + tV_n} \big |Z_1,...,Z_{n-1}\big]\Big) \label{pbbf5}\\<br/>
&amp;= e^{-t\epsilon} \mathbb E\Big(\mathbb E\big[ e^{t\sum_{i=1}^{n-1} V_i} e^{tV_n} \big |Z_1,...,Z_{n-1}\big]\Big) \label{pbbf6}\\<br/>
&amp;= e^{-t\epsilon} \mathbb E\Big(e^{t\sum_{i=1}^{n-1} V_i} \mathbb E \big[e^{tV_n} \big |Z_1,...,Z_{n-1}\big]\Big) \label{pbbf7}\\<br/>
\end{align}<br/>
\]</p>

<p>上式中 \ref{pbbf2} 到 \ref{pbbf3} 是利用了马尔可夫不等式得到。从 \ref{pbbf3} 到 \ref{pbbf4} 式应用了条件期望的 LIE 的性质。式 \ref{pbbf6} 中可以将 \(e^{t\sum_{i=1}^{n-1} V_i}\) 看成关于 \(Z_1,Z_2,...,Z_{n-1}\) 的函数，将 \(e^{tV_n}\) 看成关于 \(V_1,...,V_n\) 的函数，利用条件期望的稳定性可得。 </p>

<p>现在需要证明的式中 \(\mathbb E[e^{tV_n} \big |Z_1,...,Z_{n-1}\big]\) 和霍夫丁不等式形式一样，考虑到：<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E[V_i|Z_1,...,Z_{i-1}] &amp;= \mathbb E\Big[\mathbb E(Y|Z_1,...,Z_i) - \mathbb E(Y|Z_1,...,Z_{i-1})\Big|Z_1,...,Z_{i-1}\Big]\\<br/>
&amp;= \mathbb E\Big[\mathbb E(Y|Z_1,...,Z_i)\Big|Z_1,...,Z_{i-1}\Big] - \mathbb E\Big[\mathbb E(Y|Z_1,...,Z_{i-1}) \Big |Z_1,...,Z_{i-1} \Big]\\<br/>
&amp;= \mathbb E(Y|Z_1,...,Z_{i-1}) - \mathbb E(Y|Z_1,...,Z_{i-1}) \\<br/>
&amp;= 0<br/>
\end{align*}<br/>
\]</p>

<p>为了使用霍夫丁引理还需要知道 \((V_i|Z_1,...,Z_{i-1})\) 的上下界，已知：<br/>
\[<br/>
V_i|Z_1,...,Z_{i-1} = \Big[\mathbb E(Y|Z_1,...,Z_i) - \mathbb E(Y|Z_1,...,Z_{i-1})\Big]\Big| Z_1,...,Z_{i-1}<br/>
\]</p>

<p>令：<br/>
\[<br/>
\begin{align*}<br/>
U_i &amp;= \sup_u \mathbb E\big[Y|Z_1,...,Z_{i-1},u \big]-\mathbb E\big[Y|Z_1,...,Z_{i-1}\big]\\<br/>
L_i &amp;= \inf_l  \mathbb E\Big[Y|Z_1,...,Z_{i-1},l \big]-\mathbb E\big[Y|Z_1,...,Z_{i-1}\big]\\<br/>
\end{align*}<br/>
\]</p>

<p>所以有：<br/>
\[<br/>
L_i \le V_i|Z_1,...,Z_{i-1} \le U_i<br/>
\]</p>

<p>现在：<br/>
\[<br/>
\begin{align}<br/>
U_i - L_i &amp;= \sup_{u} \mathbb E[f(Z_1,...,Z_n)|Z_1,...,Z_{i-1},u] - \inf_{l} \mathbb E[f(Z_1,...,Z_n)|Z_1,...,Z_{i-1},l] \nonumber\\<br/>
&amp;= \sup_{u} \int f(Z_1,...Z_{i-1},u,z_{i+1},...,z_n) \mathbf dP(z_{i+1},...,z_n|Z_1,...Z_{i-1},u) - \inf_{l} \int f(Z_1,...,Z_{i-1},l,z_{i+1},...,z_n)\mathbf dP(z_{i+1},...,z_n|Z_1,...Z_{i-1},l)\nonumber\\<br/>
&amp;= \sup_{u} \int f(Z_1,...Z_{i-1},u,z_{i+1},...,z_n) \mathbf d\frac{P(Z_1,...Z_{i-1},u,z_{i+1},...,z_n)}{P(Z_1,...,Z_{i-1},u)}  - \inf_{l} \int f(Z_1,...,Z_{i-1},l,z_{i+1},...,z_n)\mathbf d\frac{P(Z_1,...,Z_{i-1},l,z_{i+1},...,z_n)}{P(Z_1,...,Z_{i-1},l)}\nonumber\\<br/>
&amp;= \sup_{u} \int f(Z_1,...Z_{i-1},u,z_{i+1},...,z_n) \mathbf d\frac{P(Z_1,...Z_{i-1},u)\cdot P(z_{i+1},...,z_n)}{P(Z_1,...,Z_{i-1},u)}  - \inf_{l} \int f(Z_1,...,Z_{i-1},l,z_{i+1},...,z_n)\mathbf d\frac{P(Z_1,...,Z_{i-1},l)\cdot P(z_{i+1},...,z_n)}{P(Z_1,...,Z_{i-1},l)}\nonumber\\<br/>
&amp;= \sup_{u} \int f(Z_1,...Z_{i-1},u,z_{i+1},...,z_n) \mathbf d P(z_{i+1},...,z_n) - \inf_{l} \int f(Z_1,...,Z_{i-1},l,z_{i+1},...,z_n)\mathbf d P(z_{i+1},...,z_n)\nonumber\\<br/>
&amp;\le \sup_{u,l} \bigg [\int f(Z_1,...Z_{i-1},u,z_{i+1},...,z_n)\mathbf d P(z_{i+1},...,z_n) - \int f(Z_1,...,Z_{i-1},l,z_{i+1},...,z_n) \mathbf d P(z_{i+1},...,z_n)\bigg ]\nonumber\\<br/>
&amp;=  \sup_{u,l} \int \bigg [f(Z_1,...Z_{i-1},u,z_{i+1},...,z_n) - f(Z_1,...,Z_{i-1},l,z_{i+1},...,z_n) \bigg ]\mathbf d P(z_{i+1},...,z_n)\nonumber\\<br/>
&amp;\le  \int \sup_{u,l} \Big[f(Z_1,...Z_{i-1},u,z_{i+1},...,z_n) - f(Z_1,...,Z_{i-1},l,z_{i+1},...,z_n) \Big]\mathbf d P(z_{i+1},...,z_n)\nonumber\\<br/>
&amp;\le \int c_i \mathbf d P(z_{i+1},...,z_n)\nonumber\\<br/>
&amp;= c_i\nonumber\\<br/>
\end{align}<br/>
\]</p>

<p>上式倒数第四行因为 sup 是下凸函数，使用 Jensen不等式可以交换 sup 与积分顺序。</p>

<blockquote>
<p>上确界函数的凸性：假设 \(X\) 是任意随机变量，对于任意的 \(f\) 都有<br/>
\[<br/>
\sup_{y\in Y} \int f(X,y) \le \int \sup_{y\in Y} f(X,y)<br/>
\]</p>

<p>证明很简单，因为<br/>
\[<br/>
\begin{align*}<br/>
&amp;\because\quad f(X,y) \le \sup_{y\in Y} f(X,y)\\<br/>
&amp;\therefore\quad \int f(X,y) \le \int \sup_{y\in Y} f(X,y)\\<br/>
&amp;\therefore\quad \sup_{y\in Y} \int f(X,y) \le \int \sup_{y\in Y} f(X,y)<br/>
\end{align*}<br/>
\]</p>
</blockquote>

<p>若令 \(a = L_i\)，\(b = a + c_i\)，则 \(a \le V_i|Z_1,...,Z_{i-1} \le b\)，应用霍夫丁引理：<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E\big[e^{tV_n} \big |Z_1,...,Z_{n-1}\big] &amp;\le \exp\Big(\frac{t^2(b-a)^2}{8} \Big)\\<br/>
&amp;= \exp\Big(\frac{t^2(a+c_n-a)^2}{8} \Big)\\<br/>
&amp;= \exp\Big(\frac{t^2 c_n^2}{8} \Big)\\<br/>
\end{align*}<br/>
\]</p>

<p>代入 \ref{pbbf7} 式可得：<br/>
\[<br/>
\begin{align*}<br/>
e^{-t\epsilon} \mathbb E\Big(e^{t\sum_{i=1}^{n-1} V_i} \mathbb E \big[e^{tV_n} \big |Z_1,...,Z_{n-1}\big]\Big) &amp;\le e^{-t\epsilon} \mathbb E\Big(e^{t\sum_{i=1}^{n-1} V_i} \exp\Big(\frac{t^2 c_n^2}{8} \Big)\Big)\\<br/>
&amp;= e^{-t\epsilon} \exp\Big(\frac{t^2 c_n^2}{8} \Big)\mathbb E\Big(e^{t\sum_{i=1}^{n-1} V_i}\Big)\\<br/>
&amp;= e^{-t\epsilon} \exp\Big(\sum_{n-1}^n \frac{t^2 c_n^2}{8} \Big)\mathbb E\Big(e^{t\sum_{i=1}^{n-2} V_i}\Big)\\<br/>
&amp;= \cdots \\<br/>
&amp;= e^{-t\epsilon} \exp\Big(\sum_{i=1}^n \frac{t^2 c_i^2}{8} \Big)\\<br/>
&amp;= \exp\Big(\sum_{i=1}^n \frac{t^2 c_i^2}{8} -t\epsilon\Big)<br/>
\end{align*}<br/>
\]</p>

<p>所以：<br/>
\[<br/>
P\bigg[f(Z_1,...,Z_n) - \mathbb E\Big(f(Z_1,...,Z_n)\Big) \ge \epsilon\bigg] \le \exp\Big(\sum_{i=1}^n \frac{t^2 c_i^2}{8} -t\epsilon\Big)<br/>
\]</p>

<p>取<br/>
\[<br/>
t= \frac{4\epsilon}{\sum_{i=1}^n c_i^2}<br/>
\]</p>

<p>时，<br/>
\[<br/>
P\bigg[f(Z_1,...,Z_n) - \mathbb E\Big(f(Z_1,...,Z_n)\Big) \ge \epsilon\bigg] \le \exp\Big(-\frac{2\epsilon^2}{\sum_{i=1}^n c_i^2}\Big)<br/>
\]</p>

<p>原不等式为：<br/>
\[<br/>
P\bigg[\bigg|f(Z_1,...,Z_n) - \mathbb E\Big(f(Z_1,...,Z_n)\Big)\bigg| \ge \epsilon\bigg] \le 2\exp\Big(-\frac{2\epsilon^2}{\sum_{i=1}^n c_i^2}\Big)<br/>
\]</p>

<hr/>

<p><a href="https://cs.nyu.edu/%7Erostami/ml/2007/ashish-mcdiarmid.pdf">ashish-mcdiarmid</a><br/>
<a href="http://www.stat.cmu.edu/%7Elarry/=sml/Concentration.pdf">Concentration of Measure</a><br/>
<a href="https://stats.stackexchange.com/questions/21362/understanding-proof-of-mcdiarmids-inequality">understanding proof of mcdiarmids inequality</a><br/>
<a href="http://web.eecs.umich.edu/%7Ecscott/past_courses/eecs598w14/notes/09_bounded_difference.pdf">The Bounded Difference Inequality</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15200089476142.html">正态分布的累积分布函数 Cumulative distribution function of Normal distribution</a></h1>
			<p class="meta"><time datetime="2018-03-03T00:42:27+08:00" 
			pubdate data-updated="true">2018/3/3</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>标准正态分布（均值为0，方差为1）<br/>
\[<br/>
\mathcal N(0,1) = \frac 1 {\sqrt{2\pi}} \exp(-\frac{x^2}{2})<br/>
\]</p>

<p>的累积分布函数常用大写希腊字母 \(\Phi\) 表示：<br/>
\[<br/>
\Phi(x) = P(X \le x) = \frac 1 {\sqrt{2\pi}} \int^x_{-\infty}\exp(-\frac{t^2}{2}) \mathbb dt<br/>
\]</p>

<p>这个积分不能使用基础函数表示，被认为是一个特殊函数。我们先来看另一个定义误差函数。</p>

<h3 id="toc_0">误差函数</h3>

<p>在数学中，误差函数（经常被称为高斯误差函数）是一个特殊函数（非基础），它是 sigmoid 形状，常出现在概率学、统计学中。定义为：<br/>
\[<br/>
\begin{align*}<br/>
\mathbf{Erf}(x) &amp;= \frac{1}{\sqrt{\pi}} \int_{-x}^x \exp(-{t^2}) \mathbb dt\\<br/>
&amp;= \frac{2}{\sqrt{\pi}} \int_0^x \exp(-{t^2}) \mathbb dt\\<br/>
\end{align*}<br/>
\]</p>

<p>在统计学中，对于非负 \(x\) ，误差函数的解释是：对于通常分布在均值为 0，方差为 \(\frac 1 2\) 的随机变量 \(Y\)，\(\mathbf{Erf}(x)\) 描述为 \(Y\) 落在范围 \([-x,x]\) 之间的概率，所以：<br/>
\[<br/>
\mathbf{Erf}(x) = \int_{-x}^{x} \frac{1}{\sqrt{2\pi}\cdot \sqrt{1/2}} \exp(-\frac{t^2}{2\cdot \frac 1 2}) \mathbb dt= \frac{1}{\sqrt{\pi}} \int_{-x}^x \exp(-{t^2}) \mathbb dt<br/>
\]</p>

<h3 id="toc_1">累积分布函数与误差函数</h3>

<p>比较正态分布的累积分布函数与误差函数，可以发现非常相像<br/>
\[<br/>
\begin{align}<br/>
\Phi(x) &amp;= P(X \le x) = \frac 1 {\sqrt{2\pi}} \int^x_{-\infty}\exp(-\frac{t^2}{2}) \mathbb dt\label{px}\\<br/>
\mathbf{Erf}(x) &amp;= \frac{2}{\sqrt{\pi}} \int_{0}^x \exp(-{t^2}) \mathbb dt\label{erfx}\\<br/>
\end{align}<br/>
\]</p>

<p>对于误差函数 \(\mathbf{Erf}(x)\)，令 \(t^2 = {s^2}/{2}\)，也就是 \(t = {s}/{\sqrt{2}}\)（ \(t\) 是非负数），由此 \(\mathbb dt = \mathbb d({s}/{\sqrt{2}})\)，令 \(t=0\) 和 \(t=x\) 得 \(s=0\) 和 \(s = \sqrt{2}x\)，代入 \ref{erfx} 式可得：<br/>
\[<br/>
\begin{align*}<br/>
\mathbf{Erf}(x) &amp;= \frac{2}{\sqrt{\pi}} \int_{0}^{\sqrt{2}x} \exp(-\frac{s^2}{2}) \mathbb d(s/\sqrt{2})\\<br/>
&amp;= \frac{2}{\sqrt{\pi}} \int_{0}^{\sqrt{2}x} \frac{1}{\sqrt{2}} \exp(-\frac{s^2}{2}) \mathbb ds\\<br/>
&amp;= \frac{2}{\sqrt{2\pi}} \int_{0}^{\sqrt{2}x}\exp(-\frac{t^2}{2}) \mathbb dt\\<br/>
&amp;= 2\bigg(\frac{1}{\sqrt{2\pi}} \int_{0}^{\sqrt{2}x}\exp(-\frac{t^2}{2}) \mathbb dt \bigg)\\<br/>
&amp;= 2\bigg(\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\sqrt{2}x}\exp(-\frac{t^2}{2}) \mathbb dt  - \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{0}\exp(-\frac{t^2}{2}) \mathbb dt\bigg)\\<br/>
&amp;= 2\bigg(\Phi(\sqrt{2}x)- \Phi(0)\bigg)\\<br/>
&amp;= 2\bigg(\Phi(\sqrt{2}x)- \frac 1 2\bigg)\\<br/>
&amp;= 2\Phi(\sqrt{2}x) - 1<br/>
\end{align*}<br/>
\]</p>

<p>现在很容易用误差函数来表示高斯分布的CDF：<br/>
\[<br/>
\Phi(\sqrt{2}x) = \frac 1 2 \bigg[{ \mathbf{Erf}(x) + 1}\bigg]\\<br/>
\Rightarrow \Phi(x) = \frac 12 \bigg[{\mathbf{Erf}\bigg(\frac{x}{\sqrt{2}}\bigg) + 1}\bigg]<br/>
\]</p>

<p>对于一般均值为 \(\mu\)，方差为 \(\sigma\) 的正态分布 \(f(x)\)，其 CDF 为：<br/>
\[<br/>
{\displaystyle F(x)=\Phi \left({\frac {x-\mu }{\sigma }}\right)={\frac {1}{2}}\left[1+\mathbf {Erf} \left({\frac {x-\mu }{\sigma {\sqrt {2}}}}\right)\right]}<br/>
\] </p>

<p>那现在我们可以绘制出<font color="#006400"> <strong>正态分布</strong> </font>和对应的 <font color="#cc0000"><strong>CDF</strong></font> 图像：</p>

<div align="center">
    <img src="media/15200089476142/15350352677388.jpg" width="240" />
</div>

<p>高斯分布的 CDF 函数的反函数被称为反误差函数，为：<br/>
\[<br/>
\Phi^{-1}(p)=\sqrt2\;\mathbf{Erf}^{-1} \left(2p - 1 \right). \quad p \in (0,1)<br/>
\]</p>

<p>该分位数函数有时也被称为probit函数。probit函数已被证明没有初等原函数。</p>

<hr/>

<p><a href="https://en.wikipedia.org/wiki/Error_function">Error Function</a><br/>
<a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal Distribution</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15199171411644.html">GeoGebra 中英文指令对照表</a></h1>
			<p class="meta"><time datetime="2018-03-01T23:12:21+08:00" 
			pubdate data-updated="true">2018/3/1</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>\[<br/>
\renewcommand\arraystretch{2}<br/>
\begin{array}{|l|l|}\hline<br/>
\quad\text{英文指令}\quad&amp;\quad\text{中文指令}\quad\\\hline<br/>
\quad\text{ANOVA}\quad&amp;\quad\text{方差分析}\quad\\\hline<br/>
\quad\text{AffineRatio}\quad&amp;\quad\text{仿射比λ}\quad\\\hline<br/>
\quad\text{Angle}\quad&amp;\quad\text{角度}\quad\\\hline<br/>
\quad\text{AngularBisector}\quad&amp;\quad\text{角平分线}\quad\\\hline<br/>
\quad\text{Append}\quad&amp;\quad\text{添加}\quad\\\hline<br/>
\quad\text{ApplyMatrix}\quad&amp;\quad\text{应用矩阵}\quad\\\hline<br/>
\quad\text{Arc}\quad&amp;\quad\text{圆弧}\quad\\\hline<br/>
\quad\text{AreCollinear}\quad&amp;\quad\text{共线}\quad\\\hline<br/>
\quad\text{AreConcurrent}\quad&amp;\quad\text{共点}\quad\\\hline<br/>
\quad\text{AreConcyclic}\quad&amp;\quad\text{共圆}\quad\\\hline<br/>
\quad\text{AreCongruent}\quad&amp;\quad\text{全等}\quad\\\hline<br/>
\quad\text{AreEqual}\quad&amp;\quad\text{相等}\quad\\\hline<br/>
\quad\text{AreParallel}\quad&amp;\quad\text{平行}\quad\\\hline<br/>
\quad\text{ArePerpendicular}\quad&amp;\quad\text{垂直}\quad\\\hline<br/>
\quad\text{Area}\quad&amp;\quad\text{面积}\quad\\\hline<br/>
\quad\text{Asymptote}\quad&amp;\quad\text{渐近线}\quad\\\hline<br/>
\quad\text{AttachCopyToView}\quad&amp;\quad\text{附加副本}\quad\\\hline<br/>
\quad\text{Axes}\quad&amp;\quad\text{轴线}\quad\\\hline<br/>
\quad\text{AxisStepX}\quad&amp;\quad\text{x轴步长}\quad\\\hline<br/>
\quad\text{AxisStepY}\quad&amp;\quad\text{y轴步长}\quad\\\hline<br/>
\quad\text{BarChart}\quad&amp;\quad\text{条形图}\quad\\\hline<br/>
\quad\text{BarCode}\quad&amp;\quad\text{条形码}\quad\\\hline<br/>
\quad\text{Barycenter}\quad&amp;\quad\text{质心}\quad\\\hline<br/>
\quad\text{Bernoulli}\quad&amp;\quad\text{伯努利分布}\quad\\\hline<br/>
\quad\text{Binomial}\quad&amp;\quad\text{二项式系数}\quad\\\hline<br/>
\quad\text{BinomialDist}\quad&amp;\quad\text{二项分布}\quad\\\hline<br/>
\quad\text{Bottom}\quad&amp;\quad\text{下底}\quad\\\hline<br/>
\quad\text{BoxPlot}\quad&amp;\quad\text{箱线图}\quad\\\hline<br/>
\quad\text{Button}\quad&amp;\quad\text{按钮}\quad\\\hline<br/>
\quad\text{CFactor}\quad&amp;\quad\text{复数因式分解}\quad\\\hline<br/>
\quad\text{CIFactor}\quad&amp;\quad\text{复杂无理数因式分解}\quad\\\hline<br/>
\quad\text{CSolutions}\quad&amp;\quad\text{复数解集}\quad\\\hline<br/>
\quad\text{CSolve}\quad&amp;\quad\text{复数解}\quad\\\hline<br/>
\quad\text{Cauchy}\quad&amp;\quad\text{柯西分布}\quad\\\hline<br/>
\quad\text{Cell}\quad&amp;\quad\text{单元格}\quad\\\hline<br/>
\quad\text{CellRange}\quad&amp;\quad\text{单元格区域数字列表}\quad\\\hline<br/>
\quad\text{Center}\quad&amp;\quad\text{中心}\quad\\\hline<br/>
\quad\text{CenterView}\quad&amp;\quad\text{中心定位}\quad\\\hline<br/>
\quad\text{Centroid}\quad&amp;\quad\text{重心}\quad\\\hline<br/>
\quad\text{Checkbox}\quad&amp;\quad\text{复选框}\quad\\\hline<br/>
\quad\text{ChiSquared}\quad&amp;\quad\text{卡方分布}\quad\\\hline<br/>
\quad\text{ChiSquaredTest}\quad&amp;\quad\text{卡方检验}\quad\\\hline<br/>
\quad\text{Circle}\quad&amp;\quad\text{圆形}\quad\\\hline<br/>
\quad\text{CircleArc}\quad&amp;\quad\text{圆弧过圆心与两点}\quad\\\hline<br/>
\quad\text{CircleSector}\quad&amp;\quad\text{圆扇形}\quad\\\hline<br/>
\quad\text{CircumcircleArc}\quad&amp;\quad\text{圆弧过三点}\quad\\\hline<br/>
\quad\text{CircumcircleSector}\quad&amp;\quad\text{扇形过三点}\quad\\\hline<br/>
\quad\text{Circumference}\quad&amp;\quad\text{圆周长}\quad\\\hline<br/>
\quad\text{Classes}\quad&amp;\quad\text{组限列表}\quad\\\hline<br/>
\quad\text{ClosestPoint}\quad&amp;\quad\text{最近点}\quad\\\hline<br/>
\quad\text{ClosestPointRegion}\quad&amp;\quad\text{区域内最近点}\quad\\\hline<br/>
\quad\text{Coefficients}\quad&amp;\quad\text{系数列表}\quad\\\hline<br/>
\quad\text{Column}\quad&amp;\quad\text{列序}\quad\\\hline<br/>
\quad\text{ColumnName}\quad&amp;\quad\text{列名称}\quad\\\hline<br/>
\quad\text{Command}\quad&amp;\quad\text{指令}\quad\\\hline<br/>
\quad\text{CommonDenominator}\quad&amp;\quad\text{公分母}\quad\\\hline<br/>
\quad\text{CompetitionRank}\quad&amp;\quad\text{竞争排名}\quad\\\hline<br/>
\quad\text{CompleteSquare}\quad&amp;\quad\text{配方式}\quad\\\hline<br/>
\quad\text{ComplexRoot}\quad&amp;\quad\text{复数根}\quad\\\hline<br/>
\quad\text{Cone}\quad&amp;\quad\text{圆锥}\quad\\\hline<br/>
\quad\text{Conic}\quad&amp;\quad\text{圆锥曲线}\quad\\\hline<br/>
\quad\text{ConstructionStep}\quad&amp;\quad\text{作图步序}\quad\\\hline<br/>
\quad\text{ContingencyTable}\quad&amp;\quad\text{列联表}\quad\\\hline<br/>
\quad\text{ContinuedFraction}\quad&amp;\quad\text{连分式}\quad\\\hline<br/>
\quad\text{ContourPlot}\quad&amp;\quad\text{等高线}\quad\\\hline<br/>
\quad\text{ConvexHull}\quad&amp;\quad\text{凸包}\quad\\\hline<br/>
\quad\text{CopyFreeObject}\quad&amp;\quad\text{复制自由对象}\quad\\\hline<br/>
\quad\text{Corner}\quad&amp;\quad\text{角落}\quad\\\hline<br/>
\quad\text{CountIf}\quad&amp;\quad\text{条件计数}\quad\\\hline<br/>
\quad\text{Covariance}\quad&amp;\quad\text{协方差}\quad\\\hline<br/>
\quad\text{Cross}\quad&amp;\quad\text{叉积}\quad\\\hline<br/>
\quad\text{CrossRatio}\quad&amp;\quad\text{交比}\quad\\\hline<br/>
\quad\text{Cube}\quad&amp;\quad\text{正六面体}\quad\\\hline<br/>
\quad\text{Cubic}\quad&amp;\quad\text{三次曲线}\quad\\\hline<br/>
\quad\text{Curvature}\quad&amp;\quad\text{曲率}\quad\\\hline<br/>
\quad\text{CurvatureVector}\quad&amp;\quad\text{曲率向量}\quad\\\hline<br/>
\quad\text{CurveCartesian}\quad&amp;\quad\text{曲线}\quad\\\hline<br/>
\quad\text{Cylinder}\quad&amp;\quad\text{圆柱}\quad\\\hline<br/>
\quad\text{DataFunction}\quad&amp;\quad\text{数据函数}\quad\\\hline<br/>
\quad\text{Defined}\quad&amp;\quad\text{定义否}\quad\\\hline<br/>
\quad\text{Degree}\quad&amp;\quad\text{次数}\quad\\\hline<br/>
\quad\text{DelauneyTriangulation}\quad&amp;\quad\text{Delaunay三角网}\quad\\\hline<br/>
\quad\text{Delete}\quad&amp;\quad\text{删除}\quad\\\hline<br/>
\quad\text{Denominator}\quad&amp;\quad\text{分母}\quad\\\hline<br/>
\quad\text{DensityPlot}\quad&amp;\quad\text{密度图}\quad\\\hline<br/>
\quad\text{Derivative}\quad&amp;\quad\text{导数}\quad\\\hline<br/>
\quad\text{Determinant}\quad&amp;\quad\text{行列式}\quad\\\hline<br/>
\quad\text{Diameter}\quad&amp;\quad\text{共轭直径}\quad\\\hline<br/>
\quad\text{Difference}\quad&amp;\quad\text{差异}\quad\\\hline<br/>
\quad\text{Dilate}\quad&amp;\quad\text{位似}\quad\\\hline<br/>
\quad\text{Dimension}\quad&amp;\quad\text{维度}\quad\\\hline<br/>
\quad\text{Direction}\quad&amp;\quad\text{方向向量}\quad\\\hline<br/>
\quad\text{Directrix}\quad&amp;\quad\text{准线}\quad\\\hline<br/>
\quad\text{Distance}\quad&amp;\quad\text{距离}\quad\\\hline<br/>
\quad\text{Div}\quad&amp;\quad\text{整商}\quad\\\hline<br/>
\quad\text{Division}\quad&amp;\quad\text{除法}\quad\\\hline<br/>
\quad\text{Divisors}\quad&amp;\quad\text{因数个数}\quad\\\hline<br/>
\quad\text{DivisorsList}\quad&amp;\quad\text{因数}\quad\\\hline<br/>
\quad\text{DivisorsSum}\quad&amp;\quad\text{因数和}\quad\\\hline<br/>
\quad\text{Dodecahedron}\quad&amp;\quad\text{正十二面体}\quad\\\hline<br/>
\quad\text{Dot}\quad&amp;\quad\text{点积}\quad\\\hline<br/>
\quad\text{DotPlot}\quad&amp;\quad\text{点阵图}\quad\\\hline<br/>
\quad\text{DynamicCoordinates}\quad&amp;\quad\text{动态坐标}\quad\\\hline<br/>
\quad\text{Eccentricity}\quad&amp;\quad\text{离心率}\quad\\\hline<br/>
\quad\text{Element}\quad&amp;\quad\text{元素}\quad\\\hline<br/>
\quad\text{Eliminate}\quad&amp;\quad\text{消元}\quad\\\hline<br/>
\quad\text{Ellipse}\quad&amp;\quad\text{椭圆}\quad\\\hline<br/>
\quad\text{Ends}\quad&amp;\quad\text{两底}\quad\\\hline<br/>
\quad\text{Envelope}\quad&amp;\quad\text{包络}\quad\\\hline<br/>
\quad\text{Erlang}\quad&amp;\quad\text{爱尔朗分布}\quad\\\hline<br/>
\quad\text{Evaluate}\quad&amp;\quad\text{计算}\quad\\\hline<br/>
\quad\text{Excentricity}\quad&amp;\quad\text{半焦距}\quad\\\hline<br/>
\quad\text{Execute}\quad&amp;\quad\text{执行}\quad\\\hline<br/>
\quad\text{Expand}\quad&amp;\quad\text{展开}\quad\\\hline<br/>
\quad\text{Exponential}\quad&amp;\quad\text{指数分布}\quad\\\hline<br/>
\quad\text{Extremum}\quad&amp;\quad\text{极值点}\quad\\\hline<br/>
\quad\text{FDistribution}\quad&amp;\quad\text{F分布}\quad\\\hline<br/>
\quad\text{Factor}\quad&amp;\quad\text{因式分解}\quad\\\hline<br/>
\quad\text{Factors}\quad&amp;\quad\text{因式集簇}\quad\\\hline<br/>
\quad\text{FillCells}\quad&amp;\quad\text{填充单元格}\quad\\\hline<br/>
\quad\text{FillColumn}\quad&amp;\quad\text{填充列}\quad\\\hline<br/>
\quad\text{FillRow}\quad&amp;\quad\text{填充行}\quad\\\hline<br/>
\quad\text{First}\quad&amp;\quad\text{最前项}\quad\\\hline<br/>
\quad\text{FirstAxis}\quad&amp;\quad\text{长轴}\quad\\\hline<br/>
\quad\text{FirstAxisLength}\quad&amp;\quad\text{半长轴长}\quad\\\hline<br/>
\quad\text{Fit}\quad&amp;\quad\text{拟合曲线}\quad\\\hline<br/>
\quad\text{FitExp}\quad&amp;\quad\text{指数拟合}\quad\\\hline<br/>
\quad\text{FitGrowth}\quad&amp;\quad\text{生长曲线拟合}\quad\\\hline<br/>
\quad\text{FitImplicit}\quad&amp;\quad\text{隐函数拟合}\quad\\\hline<br/>
\quad\text{FitLineX}\quad&amp;\quad\text{拟合直线X}\quad\\\hline<br/>
\quad\text{FitLineY}\quad&amp;\quad\text{拟合直线Y}\quad\\\hline<br/>
\quad\text{FitLog}\quad&amp;\quad\text{对数拟合}\quad\\\hline<br/>
\quad\text{FitLogistic}\quad&amp;\quad\text{逻辑斯蒂曲线拟合}\quad\\\hline<br/>
\quad\text{FitPoly}\quad&amp;\quad\text{多项式拟合}\quad\\\hline<br/>
\quad\text{FitPow}\quad&amp;\quad\text{幂函数拟合}\quad\\\hline<br/>
\quad\text{FitSin}\quad&amp;\quad\text{正弦拟合}\quad\\\hline<br/>
\quad\text{Flatten}\quad&amp;\quad\text{扁平列表}\quad\\\hline<br/>
\quad\text{Focus}\quad&amp;\quad\text{焦点}\quad\\\hline<br/>
\quad\text{FractionText}\quad&amp;\quad\text{分数文本}\quad\\\hline<br/>
\quad\text{Frequency}\quad&amp;\quad\text{频数列表}\quad\\\hline<br/>
\quad\text{FrequencyPolygon}\quad&amp;\quad\text{频数多边形}\quad\\\hline<br/>
\quad\text{FrequencyTable}\quad&amp;\quad\text{频数表}\quad\\\hline<br/>
\quad\text{FromBase}\quad&amp;\quad\text{十进制形式}\quad\\\hline<br/>
\quad\text{Function}\quad&amp;\quad\text{函数}\quad\\\hline<br/>
\quad\text{FutureValue}\quad&amp;\quad\text{未来值}\quad\\\hline<br/>
\quad\text{GCD}\quad&amp;\quad\text{最大公约数}\quad\\\hline<br/>
\quad\text{Gamma}\quad&amp;\quad\text{伽玛分布}\quad\\\hline<br/>
\quad\text{GeometricMean}\quad&amp;\quad\text{几何平均数}\quad\\\hline<br/>
\quad\text{GetTime}\quad&amp;\quad\text{系统时间}\quad\\\hline<br/>
\quad\text{GroebnerDegRevLex}\quad&amp;\quad\text{分次反字典序Groebner基}\quad\\\hline<br/>
\quad\text{GroebnerLex}\quad&amp;\quad\text{字典序Groebner基}\quad\\\hline<br/>
\quad\text{GroebnerLexDeg}\quad&amp;\quad\text{分次字典序Groebner基}\quad\\\hline<br/>
\quad\text{HarmonicMean}\quad&amp;\quad\text{调和平均数}\quad\\\hline<br/>
\quad\text{Height}\quad&amp;\quad\text{高度}\quad\\\hline<br/>
\quad\text{HideLayer}\quad&amp;\quad\text{隐藏图层}\quad\\\hline<br/>
\quad\text{Histogram}\quad&amp;\quad\text{直方图}\quad\\\hline<br/>
\quad\text{HistogramRight}\quad&amp;\quad\text{直方图右}\quad\\\hline<br/>
\quad\text{HyperGeometric}\quad&amp;\quad\text{超几何分布}\quad\\\hline<br/>
\quad\text{Hyperbola}\quad&amp;\quad\text{双曲线}\quad\\\hline<br/>
\quad\text{IFactor}\quad&amp;\quad\text{无理数因式分解}\quad\\\hline<br/>
\quad\text{Icosahedron}\quad&amp;\quad\text{正二十面体}\quad\\\hline<br/>
\quad\text{Identity}\quad&amp;\quad\text{单位矩阵}\quad\\\hline<br/>
\quad\text{If}\quad&amp;\quad\text{如果}\quad\\\hline<br/>
\quad\text{ImplicitCurve}\quad&amp;\quad\text{隐式曲线}\quad\\\hline<br/>
\quad\text{ImplicitDerivative}\quad&amp;\quad\text{隐式微分}\quad\\\hline<br/>
\quad\text{Incircle}\quad&amp;\quad\text{内切圆}\quad\\\hline<br/>
\quad\text{IndexOf}\quad&amp;\quad\text{索引}\quad\\\hline<br/>
\quad\text{InfiniteCone}\quad&amp;\quad\text{无限长圆锥}\quad\\\hline<br/>
\quad\text{InfiniteCylinder}\quad&amp;\quad\text{无限长圆柱}\quad\\\hline<br/>
\quad\text{Insert}\quad&amp;\quad\text{插入}\quad\\\hline<br/>
\quad\text{Integral}\quad&amp;\quad\text{积分}\quad\\\hline<br/>
\quad\text{IntegralBetween}\quad&amp;\quad\text{区域积分}\quad\\\hline<br/>
\quad\text{Intersect}\quad&amp;\quad\text{交点}\quad\\\hline<br/>
\quad\text{IntersectConic}\quad&amp;\quad\text{相交曲线}\quad\\\hline<br/>
\quad\text{IntersectPath}\quad&amp;\quad\text{相交路径}\quad\\\hline<br/>
\quad\text{Intersection}\quad&amp;\quad\text{交集}\quad\\\hline<br/>
\quad\text{InverseBinomial}\quad&amp;\quad\text{逆二项分布}\quad\\\hline<br/>
\quad\text{InverseCauchy}\quad&amp;\quad\text{逆柯西分布}\quad\\\hline<br/>
\quad\text{InverseChiSquared}\quad&amp;\quad\text{逆卡方分布}\quad\\\hline<br/>
\quad\text{InverseExponential}\quad&amp;\quad\text{逆指数分布}\quad\\\hline<br/>
\quad\text{InverseFDistribution}\quad&amp;\quad\text{逆F分布}\quad\\\hline<br/>
\quad\text{InverseGamma}\quad&amp;\quad\text{逆伽玛分布}\quad\\\hline<br/>
\quad\text{InverseHyperGeometric}\quad&amp;\quad\text{逆超几何分布}\quad\\\hline<br/>
\quad\text{InverseLaplace}\quad&amp;\quad\text{拉普拉斯逆变换}\quad\\\hline<br/>
\quad\text{InverseLogNormal}\quad&amp;\quad\text{逆对数正态分布}\quad\\\hline<br/>
\quad\text{InverseLogistic}\quad&amp;\quad\text{逆逻辑分布}\quad\\\hline<br/>
\quad\text{InverseNormal}\quad&amp;\quad\text{逆正态分布}\quad\\\hline<br/>
\quad\text{InversePascal}\quad&amp;\quad\text{逆帕斯卡分布}\quad\\\hline<br/>
\quad\text{InversePoisson}\quad&amp;\quad\text{逆泊松分布}\quad\\\hline<br/>
\quad\text{InverseTDistribution}\quad&amp;\quad\text{逆T分布}\quad\\\hline<br/>
\quad\text{InverseWeibull}\quad&amp;\quad\text{逆威布尔分布}\quad\\\hline<br/>
\quad\text{InverseZipf}\quad&amp;\quad\text{逆齐普夫分布}\quad\\\hline<br/>
\quad\text{Invert}\quad&amp;\quad\text{逆反}\quad\\\hline<br/>
\quad\text{IsInRegion}\quad&amp;\quad\text{在区域内}\quad\\\hline<br/>
\quad\text{IsInteger}\quad&amp;\quad\text{整数}\quad\\\hline<br/>
\quad\text{IsPrime}\quad&amp;\quad\text{质数}\quad\\\hline<br/>
\quad\text{Iteration}\quad&amp;\quad\text{迭代}\quad\\\hline<br/>
\quad\text{IterationList}\quad&amp;\quad\text{迭代列表}\quad\\\hline<br/>
\quad\text{Join}\quad&amp;\quad\text{合并}\quad\\\hline<br/>
\quad\text{KeepIf}\quad&amp;\quad\text{条件子列}\quad\\\hline<br/>
\quad\text{LCM}\quad&amp;\quad\text{最小公倍数}\quad\\\hline<br/>
\quad\text{LaTeX}\quad&amp;\quad\text{公式文本}\quad\\\hline<br/>
\quad\text{Laplace}\quad&amp;\quad\text{拉普拉斯变换}\quad\\\hline<br/>
\quad\text{Last}\quad&amp;\quad\text{最后项}\quad\\\hline<br/>
\quad\text{LeftSide}\quad&amp;\quad\text{左边}\quad\\\hline<br/>
\quad\text{LeftSum}\quad&amp;\quad\text{左和}\quad\\\hline<br/>
\quad\text{Length}\quad&amp;\quad\text{长度}\quad\\\hline<br/>
\quad\text{LetterToUnicode}\quad&amp;\quad\text{字母到统一码}\quad\\\hline<br/>
\quad\text{Limit}\quad&amp;\quad\text{极限}\quad\\\hline<br/>
\quad\text{LimitAbove}\quad&amp;\quad\text{右极限}\quad\\\hline<br/>
\quad\text{LimitBelow}\quad&amp;\quad\text{左极限}\quad\\\hline<br/>
\quad\text{Line}\quad&amp;\quad\text{直线}\quad\\\hline<br/>
\quad\text{LineBisector}\quad&amp;\quad\text{中垂线}\quad\\\hline<br/>
\quad\text{Locus}\quad&amp;\quad\text{轨迹}\quad\\\hline<br/>
\quad\text{LocusEquation}\quad&amp;\quad\text{轨迹方程}\quad\\\hline<br/>
\quad\text{LogNormal}\quad&amp;\quad\text{对数正态分布}\quad\\\hline<br/>
\quad\text{Logistic}\quad&amp;\quad\text{逻辑分布}\quad\\\hline<br/>
\quad\text{LowerSum}\quad&amp;\quad\text{下和}\quad\\\hline<br/>
\quad\text{MatrixPlot}\quad&amp;\quad\text{矩阵图}\quad\\\hline<br/>
\quad\text{MatrixRank}\quad&amp;\quad\text{矩阵的秩}\quad\\\hline<br/>
\quad\text{Max}\quad&amp;\quad\text{最大值}\quad\\\hline<br/>
\quad\text{Maximize}\quad&amp;\quad\text{最大值点}\quad\\\hline<br/>
\quad\text{Mean}\quad&amp;\quad\text{平均数}\quad\\\hline<br/>
\quad\text{MeanX}\quad&amp;\quad\text{横坐标平均数}\quad\\\hline<br/>
\quad\text{MeanY}\quad&amp;\quad\text{纵坐标平均数}\quad\\\hline<br/>
\quad\text{Median}\quad&amp;\quad\text{中位数}\quad\\\hline<br/>
\quad\text{Midpoint}\quad&amp;\quad\text{中点}\quad\\\hline<br/>
\quad\text{Min}\quad&amp;\quad\text{最小值}\quad\\\hline<br/>
\quad\text{Minimize}\quad&amp;\quad\text{最小值点}\quad\\\hline<br/>
\quad\text{MinimumSpanningTree}\quad&amp;\quad\text{最小生成树}\quad\\\hline<br/>
\quad\text{Mirror}\quad&amp;\quad\text{对称}\quad\\\hline<br/>
\quad\text{MixedNumber}\quad&amp;\quad\text{带分数}\quad\\\hline<br/>
\quad\text{Mod}\quad&amp;\quad\text{余式}\quad\\\hline<br/>
\quad\text{Mode}\quad&amp;\quad\text{众数}\quad\\\hline<br/>
\quad\text{NDerivative}\quad&amp;\quad\text{数值导数}\quad\\\hline<br/>
\quad\text{NIntegral}\quad&amp;\quad\text{定积分}\quad\\\hline<br/>
\quad\text{NSolutions}\quad&amp;\quad\text{近似解集}\quad\\\hline<br/>
\quad\text{NSolve}\quad&amp;\quad\text{近似解}\quad\\\hline<br/>
\quad\text{NSolveODE}\quad&amp;\quad\text{解常微分方程组}\quad\\\hline<br/>
\quad\text{Name}\quad&amp;\quad\text{名称}\quad\\\hline<br/>
\quad\text{Net}\quad&amp;\quad\text{展开图}\quad\\\hline<br/>
\quad\text{NextPrime}\quad&amp;\quad\text{后一质数}\quad\\\hline<br/>
\quad\text{Normal}\quad&amp;\quad\text{正态分布}\quad\\\hline<br/>
\quad\text{NormalQuantilePlot}\quad&amp;\quad\text{正态分位数图}\quad\\\hline<br/>
\quad\text{Normalize}\quad&amp;\quad\text{归一化}\quad\\\hline<br/>
\quad\text{Numerator}\quad&amp;\quad\text{分子}\quad\\\hline<br/>
\quad\text{Numeric}\quad&amp;\quad\text{近似数}\quad\\\hline<br/>
\quad\text{Object}\quad&amp;\quad\text{对象}\quad\\\hline<br/>
\quad\text{Octahedron}\quad&amp;\quad\text{正八面体}\quad\\\hline<br/>
\quad\text{Ordinal}\quad&amp;\quad\text{序数}\quad\\\hline<br/>
\quad\text{OrdinalRank}\quad&amp;\quad\text{序数列表}\quad\\\hline<br/>
\quad\text{OrthogonalLine}\quad&amp;\quad\text{垂线}\quad\\\hline<br/>
\quad\text{OrthogonalPlane}\quad&amp;\quad\text{垂面}\quad\\\hline<br/>
\quad\text{OrthogonalVector}\quad&amp;\quad\text{法向量}\quad\\\hline<br/>
\quad\text{OsculatingCircle}\quad&amp;\quad\text{密切圆}\quad\\\hline<br/>
\quad\text{PMCC}\quad&amp;\quad\text{相关系数}\quad\\\hline<br/>
\quad\text{Pan}\quad&amp;\quad\text{平移视图}\quad\\\hline<br/>
\quad\text{Parabola}\quad&amp;\quad\text{抛物线}\quad\\\hline<br/>
\quad\text{Parameter}\quad&amp;\quad\text{焦参数}\quad\\\hline<br/>
\quad\text{ParametricDerivative}\quad&amp;\quad\text{参数导数}\quad\\\hline<br/>
\quad\text{ParseToFunction}\quad&amp;\quad\text{解析为函数}\quad\\\hline<br/>
\quad\text{ParseToNumber}\quad&amp;\quad\text{解析为数}\quad\\\hline<br/>
\quad\text{PartialFractions}\quad&amp;\quad\text{分项分式}\quad\\\hline<br/>
\quad\text{Pascal}\quad&amp;\quad\text{帕斯卡分布}\quad\\\hline<br/>
\quad\text{PathParameter}\quad&amp;\quad\text{路径值}\quad\\\hline<br/>
\quad\text{Payment}\quad&amp;\quad\text{每期付款额}\quad\\\hline<br/>
\quad\text{Percentile}\quad&amp;\quad\text{百分位数}\quad\\\hline<br/>
\quad\text{Perimeter}\quad&amp;\quad\text{周长}\quad\\\hline<br/>
\quad\text{Periods}\quad&amp;\quad\text{期数}\quad\\\hline<br/>
\quad\text{Plane}\quad&amp;\quad\text{平面}\quad\\\hline<br/>
\quad\text{PlaneBisector}\quad&amp;\quad\text{中垂面}\quad\\\hline<br/>
\quad\text{PlaySound}\quad&amp;\quad\text{播放声音}\quad\\\hline<br/>
\quad\text{Point}\quad&amp;\quad\text{描点}\quad\\\hline<br/>
\quad\text{PointIn}\quad&amp;\quad\text{内点}\quad\\\hline<br/>
\quad\text{PointList}\quad&amp;\quad\text{点列}\quad\\\hline<br/>
\quad\text{Poisson}\quad&amp;\quad\text{泊松分布}\quad\\\hline<br/>
\quad\text{Polar}\quad&amp;\quad\text{极线}\quad\\\hline<br/>
\quad\text{PolyLine}\quad&amp;\quad\text{折线}\quad\\\hline<br/>
\quad\text{Polygon}\quad&amp;\quad\text{多边形}\quad\\\hline<br/>
\quad\text{Polynomial}\quad&amp;\quad\text{多项式函数}\quad\\\hline<br/>
\quad\text{PresentValue}\quad&amp;\quad\text{现值}\quad\\\hline<br/>
\quad\text{PreviousPrime}\quad&amp;\quad\text{前一质数}\quad\\\hline<br/>
\quad\text{PrimeFactors}\quad&amp;\quad\text{质因数}\quad\\\hline<br/>
\quad\text{Prism}\quad&amp;\quad\text{棱柱}\quad\\\hline<br/>
\quad\text{Product}\quad&amp;\quad\text{乘积}\quad\\\hline<br/>
\quad\text{Prove}\quad&amp;\quad\text{证明}\quad\\\hline<br/>
\quad\text{ProveDetails}\quad&amp;\quad\text{证明过程}\quad\\\hline<br/>
\quad\text{Pyramid}\quad&amp;\quad\text{棱锥}\quad\\\hline<br/>
\quad\text{Q1}\quad&amp;\quad\text{第一四分位数}\quad\\\hline<br/>
\quad\text{Q3}\quad&amp;\quad\text{第三四分位数}\quad\\\hline<br/>
\quad\text{QuadricSide}\quad&amp;\quad\text{侧面}\quad\\\hline<br/>
\quad\text{RSquare}\quad&amp;\quad\text{可决系数R方}\quad\\\hline<br/>
\quad\text{Radius}\quad&amp;\quad\text{半径}\quad\\\hline<br/>
\quad\text{Random}\quad&amp;\quad\text{区间随机数}\quad\\\hline<br/>
\quad\text{RandomBinomial}\quad&amp;\quad\text{随机二项分布数}\quad\\\hline<br/>
\quad\text{RandomDiscrete}\quad&amp;\quad\text{离散随机数}\quad\\\hline<br/>
\quad\text{RandomElement}\quad&amp;\quad\text{随机元素}\quad\\\hline<br/>
\quad\text{RandomNormal}\quad&amp;\quad\text{正态分布随机数}\quad\\\hline<br/>
\quad\text{RandomPointIn}\quad&amp;\quad\text{随机内点}\quad\\\hline<br/>
\quad\text{RandomPoisson}\quad&amp;\quad\text{泊松分布随机数}\quad\\\hline<br/>
\quad\text{RandomPolynomial}\quad&amp;\quad\text{随机多项式}\quad\\\hline<br/>
\quad\text{RandomUniform}\quad&amp;\quad\text{均匀分布随机数}\quad\\\hline<br/>
\quad\text{Rate}\quad&amp;\quad\text{利率}\quad\\\hline<br/>
\quad\text{Rationalize}\quad&amp;\quad\text{有理化}\quad\\\hline<br/>
\quad\text{Ray}\quad&amp;\quad\text{射线}\quad\\\hline<br/>
\quad\text{RectangleSum}\quad&amp;\quad\text{矩形和}\quad\\\hline<br/>
\quad\text{ReducedRowEchelonForm}\quad&amp;\quad\text{简化行梯阵式}\quad\\\hline<br/>
\quad\text{Relation}\quad&amp;\quad\text{关系}\quad\\\hline<br/>
\quad\text{Remove}\quad&amp;\quad\text{移除}\quad\\\hline<br/>
\quad\text{RemoveUndefined}\quad&amp;\quad\text{移除未定义对象}\quad\\\hline<br/>
\quad\text{Rename}\quad&amp;\quad\text{重命名}\quad\\\hline<br/>
\quad\text{Repeat}\quad&amp;\quad\text{重复}\quad\\\hline<br/>
\quad\text{ResidualPlot}\quad&amp;\quad\text{残差图}\quad\\\hline<br/>
\quad\text{Reverse}\quad&amp;\quad\text{逆序排列}\quad\\\hline<br/>
\quad\text{RightSide}\quad&amp;\quad\text{右边}\quad\\\hline<br/>
\quad\text{RigidPolygon}\quad&amp;\quad\text{刚体多边形}\quad\\\hline<br/>
\quad\text{Root}\quad&amp;\quad\text{零点}\quad\\\hline<br/>
\quad\text{RootList}\quad&amp;\quad\text{零值点列}\quad\\\hline<br/>
\quad\text{RootMeanSquare}\quad&amp;\quad\text{均方根}\quad\\\hline<br/>
\quad\text{Roots}\quad&amp;\quad\text{零值点}\quad\\\hline<br/>
\quad\text{Rotate}\quad&amp;\quad\text{旋转}\quad\\\hline<br/>
\quad\text{RotateText}\quad&amp;\quad\text{旋转文本}\quad\\\hline<br/>
\quad\text{Row}\quad&amp;\quad\text{行序}\quad\\\hline<br/>
\quad\text{RunClickScript}\quad&amp;\quad\text{运行单击脚本}\quad\\\hline<br/>
\quad\text{RunUpdateScript}\quad&amp;\quad\text{运行更新脚本}\quad\\\hline<br/>
\quad\text{SD}\quad&amp;\quad\text{标准差}\quad\\\hline<br/>
\quad\text{SDX}\quad&amp;\quad\text{横坐标标准差}\quad\\\hline<br/>
\quad\text{SDY}\quad&amp;\quad\text{纵坐标标准差}\quad\\\hline<br/>
\quad\text{SVD}\quad&amp;\quad\text{奇异值分解}\quad\\\hline<br/>
\quad\text{Sample}\quad&amp;\quad\text{样本}\quad\\\hline<br/>
\quad\text{SampleSD}\quad&amp;\quad\text{样本标准差}\quad\\\hline<br/>
\quad\text{SampleSDX}\quad&amp;\quad\text{样本横坐标标准差}\quad\\\hline<br/>
\quad\text{SampleSDY}\quad&amp;\quad\text{样本纵坐标标准差}\quad\\\hline<br/>
\quad\text{SampleVariance}\quad&amp;\quad\text{样本方差}\quad\\\hline<br/>
\quad\text{ScientificText}\quad&amp;\quad\text{科学记数法}\quad\\\hline<br/>
\quad\text{SecondAxis}\quad&amp;\quad\text{短轴}\quad\\\hline<br/>
\quad\text{SecondAxisLength}\quad&amp;\quad\text{半短轴长}\quad\\\hline<br/>
\quad\text{Sector}\quad&amp;\quad\text{扇形}\quad\\\hline<br/>
\quad\text{Segment}\quad&amp;\quad\text{线段}\quad\\\hline<br/>
\quad\text{SelectObjects}\quad&amp;\quad\text{选择}\quad\\\hline<br/>
\quad\text{SelectedElement}\quad&amp;\quad\text{选定的元素}\quad\\\hline<br/>
\quad\text{SelectedIndex}\quad&amp;\quad\text{选定的索引}\quad\\\hline<br/>
\quad\text{Semicircle}\quad&amp;\quad\text{半圆}\quad\\\hline<br/>
\quad\text{Sequence}\quad&amp;\quad\text{序列}\quad\\\hline<br/>
\quad\text{SetActiveView}\quad&amp;\quad\text{活动视图}\quad\\\hline<br/>
\quad\text{SetAxesRatio}\quad&amp;\quad\text{坐标轴比例}\quad\\\hline<br/>
\quad\text{SetBackgroundColor}\quad&amp;\quad\text{背景色}\quad\\\hline<br/>
\quad\text{SetCaption}\quad&amp;\quad\text{标题}\quad\\\hline<br/>
\quad\text{SetColor}\quad&amp;\quad\text{颜色}\quad\\\hline<br/>
\quad\text{SetConditionToShowObject}\quad&amp;\quad\text{显示条件}\quad\\\hline<br/>
\quad\text{SetConstructionStep}\quad&amp;\quad\text{作图步骤}\quad\\\hline<br/>
\quad\text{SetCoords}\quad&amp;\quad\text{坐标}\quad\\\hline<br/>
\quad\text{SetDynamicColor}\quad&amp;\quad\text{动态颜色}\quad\\\hline<br/>
\quad\text{SetFilling}\quad&amp;\quad\text{填充}\quad\\\hline<br/>
\quad\text{SetFixed}\quad&amp;\quad\text{固定}\quad\\\hline<br/>
\quad\text{SetLabelMode}\quad&amp;\quad\text{标签模式}\quad\\\hline<br/>
\quad\text{SetLayer}\quad&amp;\quad\text{图层}\quad\\\hline<br/>
\quad\text{SetLineStyle}\quad&amp;\quad\text{线型}\quad\\\hline<br/>
\quad\text{SetLineThickness}\quad&amp;\quad\text{线径}\quad\\\hline<br/>
\quad\text{SetPerspective}\quad&amp;\quad\text{格局}\quad\\\hline<br/>
\quad\text{SetPointSize}\quad&amp;\quad\text{点径}\quad\\\hline<br/>
\quad\text{SetPointStyle}\quad&amp;\quad\text{点型}\quad\\\hline<br/>
\quad\text{SetSeed}\quad&amp;\quad\text{设定种子}\quad\\\hline<br/>
\quad\text{SetSpinSpeed}\quad&amp;\quad\text{转速}\quad\\\hline<br/>
\quad\text{SetTooltipMode}\quad&amp;\quad\text{工具提示模式}\quad\\\hline<br/>
\quad\text{SetTrace}\quad&amp;\quad\text{跟踪}\quad\\\hline<br/>
\quad\text{SetValue}\quad&amp;\quad\text{赋值}\quad\\\hline<br/>
\quad\text{SetViewDirection}\quad&amp;\quad\text{指定视向}\quad\\\hline<br/>
\quad\text{SetVisibleInView}\quad&amp;\quad\text{显示对象}\quad\\\hline<br/>
\quad\text{Shear}\quad&amp;\quad\text{切变}\quad\\\hline<br/>
\quad\text{ShortestDistance}\quad&amp;\quad\text{最短距离}\quad\\\hline<br/>
\quad\text{ShowAxes}\quad&amp;\quad\text{显示坐标轴}\quad\\\hline<br/>
\quad\text{ShowGrid}\quad&amp;\quad\text{显示网格}\quad\\\hline<br/>
\quad\text{ShowLabel}\quad&amp;\quad\text{显示标签}\quad\\\hline<br/>
\quad\text{ShowLayer}\quad&amp;\quad\text{显示图层}\quad\\\hline<br/>
\quad\text{Shuffle}\quad&amp;\quad\text{随机排列}\quad\\\hline<br/>
\quad\text{SigmaXX}\quad&amp;\quad\text{横坐标平方和}\quad\\\hline<br/>
\quad\text{SigmaXY}\quad&amp;\quad\text{横纵坐标乘积和}\quad\\\hline<br/>
\quad\text{SigmaYY}\quad&amp;\quad\text{纵坐标平方和}\quad\\\hline<br/>
\quad\text{Simplify}\quad&amp;\quad\text{化简}\quad\\\hline<br/>
\quad\text{Slider}\quad&amp;\quad\text{滑动条}\quad\\\hline<br/>
\quad\text{Slope}\quad&amp;\quad\text{斜率}\quad\\\hline<br/>
\quad\text{SlopeField}\quad&amp;\quad\text{斜率场}\quad\\\hline<br/>
\quad\text{SlowPlot}\quad&amp;\quad\text{缓慢绘制}\quad\\\hline<br/>
\quad\text{Solutions}\quad&amp;\quad\text{解集}\quad\\\hline<br/>
\quad\text{Solve}\quad&amp;\quad\text{精确解}\quad\\\hline<br/>
\quad\text{SolveCubic}\quad&amp;\quad\text{解三次多项式}\quad\\\hline<br/>
\quad\text{SolveODE}\quad&amp;\quad\text{解常微分方程}\quad\\\hline<br/>
\quad\text{SolveQuartic}\quad&amp;\quad\text{解四次多项式}\quad\\\hline<br/>
\quad\text{Sort}\quad&amp;\quad\text{升序排列}\quad\\\hline<br/>
\quad\text{Spearman}\quad&amp;\quad\text{秩相关系数}\quad\\\hline<br/>
\quad\text{Sphere}\quad&amp;\quad\text{球面}\quad\\\hline<br/>
\quad\text{Spline}\quad&amp;\quad\text{样条曲线}\quad\\\hline<br/>
\quad\text{StartAnimation}\quad&amp;\quad\text{启动动画}\quad\\\hline<br/>
\quad\text{StartLogging}\quad&amp;\quad\text{启动日志}\quad\\\hline<br/>
\quad\text{StartRecord}\quad&amp;\quad\text{开始记录}\quad\\\hline<br/>
\quad\text{StemPlot}\quad&amp;\quad\text{茎叶图}\quad\\\hline<br/>
\quad\text{StepGraph}\quad&amp;\quad\text{阶梯图}\quad\\\hline<br/>
\quad\text{StickGraph}\quad&amp;\quad\text{棒图}\quad\\\hline<br/>
\quad\text{StopLogging}\quad&amp;\quad\text{停止日志}\quad\\\hline<br/>
\quad\text{Stretch}\quad&amp;\quad\text{伸缩}\quad\\\hline<br/>
\quad\text{Substitute}\quad&amp;\quad\text{代入}\quad\\\hline<br/>
\quad\text{Sum}\quad&amp;\quad\text{总和}\quad\\\hline<br/>
\quad\text{SumSquaredErrors}\quad&amp;\quad\text{误差平方和}\quad\\\hline<br/>
\quad\text{SurdText}\quad&amp;\quad\text{根式文本}\quad\\\hline<br/>
\quad\text{Surface}\quad&amp;\quad\text{曲面}\quad\\\hline<br/>
\quad\text{TDistribution}\quad&amp;\quad\text{T分布}\quad\\\hline<br/>
\quad\text{TMean2Estimate}\quad&amp;\quad\text{双样本均值T估计}\quad\\\hline<br/>
\quad\text{TMeanEstimate}\quad&amp;\quad\text{单均值T估计}\quad\\\hline<br/>
\quad\text{TTest}\quad&amp;\quad\text{T检验}\quad\\\hline<br/>
\quad\text{TTest2}\quad&amp;\quad\text{双样本T检验}\quad\\\hline<br/>
\quad\text{TTestPaired}\quad&amp;\quad\text{配对T检验}\quad\\\hline<br/>
\quad\text{TableText}\quad&amp;\quad\text{表格文本}\quad\\\hline<br/>
\quad\text{Take}\quad&amp;\quad\text{提取}\quad\\\hline<br/>
\quad\text{Tangent}\quad&amp;\quad\text{切线}\quad\\\hline<br/>
\quad\text{TaylorSeries}\quad&amp;\quad\text{泰勒公式}\quad\\\hline<br/>
\quad\text{Tetrahedron}\quad&amp;\quad\text{正四面体}\quad\\\hline<br/>
\quad\text{Text}\quad&amp;\quad\text{文本}\quad\\\hline<br/>
\quad\text{TextToUnicode}\quad&amp;\quad\text{文本到统一码}\quad\\\hline<br/>
\quad\text{Textfield}\quad&amp;\quad\text{输入框}\quad\\\hline<br/>
\quad\text{TiedRank}\quad&amp;\quad\text{平秩列表}\quad\\\hline<br/>
\quad\text{ToBase}\quad&amp;\quad\text{进制形式}\quad\\\hline<br/>
\quad\text{ToComplex}\quad&amp;\quad\text{复数形式}\quad\\\hline<br/>
\quad\text{ToExponential}\quad&amp;\quad\text{指数形式}\quad\\\hline<br/>
\quad\text{ToPoint}\quad&amp;\quad\text{点形式}\quad\\\hline<br/>
\quad\text{ToPolar}\quad&amp;\quad\text{极坐标形式}\quad\\\hline<br/>
\quad\text{ToolImage}\quad&amp;\quad\text{工具图像}\quad\\\hline<br/>
\quad\text{Top}\quad&amp;\quad\text{上底}\quad\\\hline<br/>
\quad\text{Translate}\quad&amp;\quad\text{平移}\quad\\\hline<br/>
\quad\text{Transpose}\quad&amp;\quad\text{转置}\quad\\\hline<br/>
\quad\text{TrapezoidalSum}\quad&amp;\quad\text{梯形和}\quad\\\hline<br/>
\quad\text{TravelingSalesman}\quad&amp;\quad\text{旅行商问题}\quad\\\hline<br/>
\quad\text{TriangleCenter}\quad&amp;\quad\text{三角形中心}\quad\\\hline<br/>
\quad\text{TriangleCurve}\quad&amp;\quad\text{三角曲线}\quad\\\hline<br/>
\quad\text{Triangular}\quad&amp;\quad\text{三角形分布}\quad\\\hline<br/>
\quad\text{TrigCombine}\quad&amp;\quad\text{三角函数积化和差}\quad\\\hline<br/>
\quad\text{TrigExpand}\quad&amp;\quad\text{三角函数和差化积}\quad\\\hline<br/>
\quad\text{TrigSimplify}\quad&amp;\quad\text{三角函数化简}\quad\\\hline<br/>
\quad\text{Trilinear}\quad&amp;\quad\text{三线坐标点}\quad\\\hline<br/>
\quad\text{TurningPoint}\quad&amp;\quad\text{拐点}\quad\\\hline<br/>
\quad\text{Turtle}\quad&amp;\quad\text{海龟}\quad\\\hline<br/>
\quad\text{TurtleBack}\quad&amp;\quad\text{后退}\quad\\\hline<br/>
\quad\text{TurtleDown}\quad&amp;\quad\text{落笔}\quad\\\hline<br/>
\quad\text{TurtleForward}\quad&amp;\quad\text{前进}\quad\\\hline<br/>
\quad\text{TurtleLeft}\quad&amp;\quad\text{左转}\quad\\\hline<br/>
\quad\text{TurtleRight}\quad&amp;\quad\text{右转}\quad\\\hline<br/>
\quad\text{TurtleUp}\quad&amp;\quad\text{抬笔}\quad\\\hline<br/>
\quad\text{UnicodeToLetter}\quad&amp;\quad\text{统一码到字母}\quad\\\hline<br/>
\quad\text{UnicodeToText}\quad&amp;\quad\text{统一码到文本}\quad\\\hline<br/>
\quad\text{Uniform}\quad&amp;\quad\text{均匀分布}\quad\\\hline<br/>
\quad\text{Union}\quad&amp;\quad\text{并集}\quad\\\hline<br/>
\quad\text{Unique}\quad&amp;\quad\text{互异}\quad\\\hline<br/>
\quad\text{UnitOrthogonalVector}\quad&amp;\quad\text{单位法向量}\quad\\\hline<br/>
\quad\text{UnitVector}\quad&amp;\quad\text{单位向量}\quad\\\hline<br/>
\quad\text{UpdateConstruction}\quad&amp;\quad\text{更新作图}\quad\\\hline<br/>
\quad\text{UpperSum}\quad&amp;\quad\text{上和}\quad\\\hline<br/>
\quad\text{Variance}\quad&amp;\quad\text{方差}\quad\\\hline<br/>
\quad\text{Vector}\quad&amp;\quad\text{向量}\quad\\\hline<br/>
\quad\text{Vertex}\quad&amp;\quad\text{顶点}\quad\\\hline<br/>
\quad\text{VerticalText}\quad&amp;\quad\text{竖排文本}\quad\\\hline<br/>
\quad\text{Volume}\quad&amp;\quad\text{体积}\quad\\\hline<br/>
\quad\text{Voronoi}\quad&amp;\quad\text{Voronoi图}\quad\\\hline<br/>
\quad\text{Weibull}\quad&amp;\quad\text{威布尔分布}\quad\\\hline<br/>
\quad\text{ZMean2Estimate}\quad&amp;\quad\text{双样本均值Z估计}\quad\\\hline<br/>
\quad\text{ZMean2Test}\quad&amp;\quad\text{双样本均值Z检验}\quad\\\hline<br/>
\quad\text{ZMeanEstimate}\quad&amp;\quad\text{单均值Z估计}\quad\\\hline<br/>
\quad\text{ZMeanTest}\quad&amp;\quad\text{单均值Z检验}\quad\\\hline<br/>
\quad\text{ZProportion2Estimate}\quad&amp;\quad\text{双样本比例Z估计}\quad\\\hline<br/>
\quad\text{ZProportion2Test}\quad&amp;\quad\text{双样本比例Z检验}\quad\\\hline<br/>
\quad\text{ZProportionEstimate}\quad&amp;\quad\text{单比例Z估计}\quad\\\hline<br/>
\quad\text{ZProportionTest}\quad&amp;\quad\text{单比例Z检验}\quad\\\hline<br/>
\quad\text{Zip}\quad&amp;\quad\text{映射}\quad\\\hline<br/>
\quad\text{Zipf}\quad&amp;\quad\text{齐普夫分布}\quad\\\hline<br/>
\quad\text{ZoomIn}\quad&amp;\quad\text{放大}\quad\\\hline<br/>
\quad\text{ZoomOut}\quad&amp;\quad\text{缩小}\quad\\\hline<br/>
\quad\text{nPr}\quad&amp;\quad\text{排列数}\quad\\\hline<br/>
\end{array}<br/>
\]</p>

<hr/>

<p><a href="http://jiaoshiqun.cn/geogebra/zhiling">GeoGebra 中英文指令对照表</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15198236385746.html">极坐标计算二重积分</a></h1>
			<p class="meta"><time datetime="2018-02-28T21:13:58+08:00" 
			pubdate data-updated="true">2018/2/28</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>在平面里取一个顶点 \(O\)，叫做极点。引一条射线 \(OX\)，叫做极轴，这样便建立了极坐标系。对于平面里任意一点M</p>

<div align="center">
    <img src="media/15198236385746/15349443539184.jpg" width="230" />
</div>

<p>其中 \(|OM|=r\)，\(\angle XOM = \theta\)，这样 \(M\) 点的极坐标便是 \((r,\theta)\)。以 \(O\) 点为原点，\(OX\) 为 \(X\) 轴，过 \(O\) 点垂直 \(X\) 轴，作射线 \(OY\) 为 \(Y\) 轴，这样可以将极坐标转为平面坐标，如下图：</p>

<div align="center">
    <img src="media/15198236385746/15349451158772.jpg" width="280" />
</div>

<p>图中显然 \(|OA| = |OM| \sin\theta = r\sin\theta\)，\(|OB| = |OM|\cos\theta = r\cos\theta\)，所以 \(M\) 点的平面坐标为 \((r\cos\theta,r\sin\theta)\)。同理如果给出了点 \(N\) 的平面坐标为 \((x,y)\)，那么：<br/>
\[<br/>
\tan\theta = \frac{y}{x}\\<br/>
r^2 = x^2+y^2<br/>
\]</p>

<p>通过这样可以进行转换平面坐标转极坐标。</p>

<h3 id="toc_0">极坐标下的面积</h3>

<p>如下图的圆中：</p>

<div align="center">
    <img src='media/15198236385746/15349482178114.jpg' width="200" />
</div>

<p>在扇形夹角和半径已知的情况下，很容易求出扇形的面积：<br/>
\[<br/>
\Delta \mathbf A = \pi r^2 \frac{\Delta \theta}{2\pi} = \frac 1 2 r^2\Delta \theta\\<br/>
\]</p>

<p>现在来看一个无规则形状：</p>

<div align="center">
    <img src="media/15198236385746/15349533671011.jpg" width="230" />
</div>

<p>我们可以利用黎曼和的知识对其进行切分，形成一个个小扇形，这个小扇形如果角度足够小可以将其当做半径为 \(r\) 圆形里面的一个扇形：<br/>
\[<br/>
\Delta \mathbf A \approx \pi r^2 \frac{\Delta \theta}{2\pi} = \frac 1 2 r^2\Delta \theta\\<br/>
\mathbb d\mathbf A = \frac 1 2 r^2\mathbb d \theta\\<br/>
\]</p>

<p>曲线内的任意扇形面积：<br/>
\[<br/>
\mathbf A_{part} = \int_{\theta_2}^{\theta_1} \frac 1 2 r^2\mathbb d \theta<br/>
\]</p>

<p>整个面积：<br/>
\[<br/>
\mathbf A = \int_{-{\pi}/{2}}^{{\pi}/{2}} \frac 1 2 r^2\mathbb d \theta<br/>
\]</p>

<h3 id="toc_1">二重积分的极坐标转换</h3>

<p>二重积分的被积函数为 \(f(x,y)\)，令 \(x=r\cos\theta\)，\(y=r\sin\theta\)，所以被积函数可以转换为 \(f(r\cos\theta,r\sin\theta)\)，现在来看极坐标下的积分元素 \(d\sigma\) 的表示方法。设积分区域 \(D\) 为平面有界区域, 并且从原点发出的射线与 \(D\) 的边界线交点不多于两个, 则区域 \(D\) 被分割情形见下图. </p>

<div align="center">
    <img width="280" src="media/15198236385746/15349528135200.jpg" />
</div>

<p>图中分割的其中一小块的面积为：<br/>
\[<br/>
\begin{align*}<br/>
\triangle \sigma &amp;= \frac 1 2 (r+\triangle r)^2 \triangle \theta - \frac 1 2 r^2 \triangle \theta \\<br/>
&amp;= r \triangle r \triangle \theta + \frac 1 2 \triangle r^2\triangle \theta<br/>
\end{align*}<br/>
\]</p>

<p>去掉高阶无穷小 \(\frac 1 2 \triangle r^2\triangle \theta\)，可得：<br/>
\[<br/>
\triangle \sigma \approx r\triangle r\triangle \theta<br/>
\]</p>

<p>故：<br/>
\[<br/>
\mathbb d\sigma = r\mathbb d r\mathbb d\theta<br/>
\]</p>

<p>于是，二重积分<br/>
\[<br/>
\iint\limits_D f(x,y) \mathbb dx \mathbb dy = \iint\limits_D f(r\cos\theta,r\sin\theta) r\mathbb dr\mathbb d\theta<br/>
\]</p>

<h4 id="toc_2">极坐标下极坐标转化为累次积分的三种形式</h4>

<p>如下区域特征：<br/>
    <div align="center"><br/>
        <img src="media/15198236385746/15349545736898.jpg" width="250" /><br/>
    </div></p>

<p>其中：<br/>
    \[<br/>
    D = \left \{ \begin{array}\\<br/>
    \alpha\le\theta\le\beta\\<br/>
    \varphi_1(\theta)\le r\le \varphi_2(\theta)<br/>
    \end{array} \right .<br/>
    \]</p>

<p>现在区间 \([\alpha,\beta]\) 上任意取定一个 \(\theta\) 值，\(D\) 上的极径 \(r\) 从 \(\varphi_1(\theta)\) 变到 \(\varphi_2(\theta)\) 。又 \(\theta\) 是在 \([\alpha,\beta]\) 上任意取定的，所以 \(\theta\) 的变化范围是 \([\alpha,\beta]\) 。这样就可看出，极坐标系中的二重积分的公式为：<br/>
\[<br/>
\begin{align*}<br/>
\iint\limits_D f(x,y) \mathbb dx \mathbb dy &amp;= \iint\limits_D f(r\cos\theta,r\sin\theta) r\mathbb dr\mathbb d\theta\\<br/>
&amp;= \int_{\alpha}^{\beta}\Big[\int_{\varphi_1(\theta)}^{\varphi_2(\theta)} f(r\cos\theta,r\sin\theta)r\mathbb dr\Big]\mathbb d\theta\\<br/>
&amp;= \int_{\alpha}^{\beta} \mathbb d\theta \int_{\varphi_1(\theta)}^{\varphi_2(\theta)} f(r\cos\theta,r\sin\theta)r\mathbb dr\\<br/>
\end{align*}<br/>
\]</p>

<p>再看两个上图的特例情况：</p>

<ol>
<li><p>\(0\le r \le \varphi(\theta)\)，\(\alpha\le \theta\le\beta\)：<br/>
<div align="center"><br/>
    <img src="media/15198236385746/15349556750495.jpg" width="220" /><br/>
</div></p>

<p>\[<br/>
\begin{align*}<br/>
\iint\limits_D f(x,y) \mathbb dx \mathbb dy = \int_{\alpha}^{\beta} \mathbb d\theta \int_0^{\varphi(\theta)} f(r\cos\theta,r\sin\theta)r\mathbb dr\\<br/>
\end{align*}<br/>
\]</p></li>
<li><p>\(0\le r \le \varphi(\theta)\)，\(0\le\theta\le 2\pi\)：<br/>
<div align="center"><br/>
    <img src="media/15198236385746/15349558131970.jpg" width="220" /><br/>
</div></p>

<p>\[<br/>
\begin{align*}<br/>
\iint\limits_D f(x,y) \mathbb dx \mathbb dy = \int_{0}^{2\pi} \mathbb d\theta \int_0^{\varphi(\theta)} f(r\cos\theta,r\sin\theta)r\mathbb dr\\<br/>
\end{align*}<br/>
\]</p></li>
</ol>

<hr/>

<p><a href="https://wenku.baidu.com/view/d2c27c0510661ed9ac51f325.html">极坐标下二重积分的计算</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15127423559946.html">霍夫丁不等式 Hoeffding's inequality</a></h1>
			<p class="meta"><time datetime="2017-12-08T22:12:35+08:00" 
			pubdate data-updated="true">2017/12/8</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>在概率论中，Hoeffding不等式提供了有界独立随机变量之和偏离其预期值超过一定数量的概率的上界。 1963年Wassily Hoeffding证明了Hoeffding不等式。</p>

<h4 id="toc_0">霍夫丁不等式的定义</h4>

<p>设有两两独立的一系列随机变量 \(X_1,X_2,...,X_i,...,X_n\) 。假设对所有的 \(X_{i}\) 都是几乎有界的变量，即满足：<br/>
\[<br/>
P(X_i \in [a_i,b_i]) = 1<br/>
\]</p>

<p>那么这n个随机变量的经验平均数：<br/>
\[<br/>
\overline X = \frac{X_1+X_2+...+X_n}{n}<br/>
\]</p>

<p>霍夫丁不等式定义如下，对于任意的 \(t&gt;0\)，霍夫丁不等式理论一：<br/>
\[<br/>
{{\displaystyle {\begin{aligned}\operatorname {P} ({\overline {X}}-\mathbb E [{\overline {X}}]\geq t)\leq e^{-2nt^{2}}\end{aligned}}}}\\<br/>
\]</p>

<p>霍夫丁不等式理论二：<br/>
\[<br/>
{P ({\overline {X}}-\mathbb E [{\overline {X}}]\geq t)\leq \exp \left(-{\frac {2t^{2}n^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),\!}\\<br/>
{P (|{\overline {X}}-\mathbb E [{\overline {X}}]|\geq t)\leq 2\exp \left(-{\frac {2t^{2}n^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),\!}<br/>
\]</p>

<p>在证明霍夫丁不等式之前，先来看一看霍夫丁引理：</p>

<h4 id="toc_1">霍夫丁引理 Hoeffding&#39;s Lemma</h4>

<p>如果 \(X\) 是一个期望为 0 的实数随机变量，即 \(\mathbb E(X)=0\) ，当 \(a \le X \le b\) ，对于任意一个 \(\lambda \in R\) ，都有：<br/>
\[<br/>
\mathbb E(e^{\lambda X}) \le \exp(\frac{\lambda^2(b-a)^2}{8})<br/>
\]</p>

<p>注意因为假设随机变量 \(X\) 有一个 0 的期望值，在定理中 \(a\) 和 \(b\) 必满足 \(a \le 0 \le b\)。</p>

<h5 id="toc_2">引理证明</h5>

<p>因为 \(e^{\lambda x}\) 是一个关于 \(x\) 的下凸函数，由琴生不等式知，对于 \(a\le x \le b\)：<br/>
\[<br/>
e^{\lambda X} = \exp(\frac{\lambda ab-\lambda aX - \lambda ba + \lambda bX}{b-a}) =  \exp(\frac{\lambda a(b-X)}{b-a} + \frac{\lambda b(X-a)}{b-a}) \le \frac{b-X}{b-a}e^{\lambda a} + \frac{X-a}{b-a} e^{\lambda b}<br/>
\]</p>

<p>所以：<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E[e^{\lambda X}] &amp;\le \mathbb E[\frac{b-X}{b-a} e^{\lambda a} + \frac{X-a}{b-a} e^{\lambda b}]\\<br/>
&amp;= \frac{b-\mathbb E[X]}{b-a} e^{\lambda a} + \frac{\mathbb E[X]-a}{b-a} e^{\lambda b}\\<br/>
&amp;= \frac{b}{b-a} e^{\lambda a} + \frac{-a}{b-a} e^{\lambda b}\\<br/>
&amp;= -\frac{a}{b-a} e^{\lambda a} [-\frac b a + e^{\lambda (b - a)}]\\<br/>
&amp;= -\frac{a}{b-a} e^{\lambda a} [-\frac{b-a+a}{a} + e^{\lambda (b - a)}]\\<br/>
&amp;= -\frac{a}{b-a} e^{\lambda a} [-\frac{b-a}{a} -1 + e^{\lambda (b - a)}]\\<br/>
\end{align*}<br/>
\]</p>

<p>令 \(\theta = - \frac a {b-a} &gt; 0\)：<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E[e^{\lambda X}] &amp;\le -\frac{a}{b-a} e^{\lambda a} [-\frac{b-a}{a} -1 + e^{\lambda (b - a)}]\\<br/>
&amp;= \theta e^{-\lambda (b-a)\theta}[\frac 1 \theta - 1 + e^{\lambda(b-a)}]\\<br/>
&amp;= e^{-\lambda (b-a)\theta}(1 - \theta + \theta e^{\lambda(b-a)})<br/>
\end{align*}<br/>
\]</p>

<p>令 \(u = \lambda (b-a)\) ，则<br/>
 \[<br/>
 \begin{equation}<br/>
 \mathbb E[e^{\lambda X}] \le e^{-\theta u}(1-\theta + \theta e^u)\label{elx}\\<br/>
 \end{equation}<br/>
 \]</p>

<p>考虑到：<br/>
\[<br/>
\begin{align*}<br/>
1-\theta + \theta e^u &amp;= \theta(\frac 1 \theta + 1 + e^u)\\<br/>
&amp;= \theta(-\frac b a + e^{u})\\<br/>
&amp; &gt; 0\quad \because \theta &gt; 0\text{ and } a \lt 0\text{ and } b \ge 0\\<br/>
\end{align*}<br/>
\] </p>

<p>所以式 \ref{elx} 可以写成：<br/>
\[<br/>
\mathbb E[e^{\lambda X}] \le e^{-\theta u}(1-\theta + \theta e^u) = \exp[-\theta u + \log (1-\theta + \theta e^u)]<br/>
\]</p>

<p>定义：<br/>
\[<br/>
\left \{ \begin{array} \\<br/>
\phi:\mathbb R \rightarrow \mathbb R\\<br/>
\phi(u) = -\theta u + \log(1-\theta + \theta e^u)<br/>
\end{array} \right .<br/>
\]</p>

<p>即得：<br/>
\[<br/>
\begin{equation}<br/>
\mathbb E[e^{\lambda X}] \le e^{\phi(u)} \label{eep}<br/>
\end{equation}<br/>
\]</p>

<p>运用泰勒中值定理（同济大学高数上p139），对于每一个实数 \(u\) 都存在一个 \(v\) 在 0 到 \(\mu\) 之间，有：<br/>
\[<br/>
\phi(u) = \phi(0) + \phi&#39;(0)u + \frac{\phi&#39;&#39;(v)}{2!}u^2<br/>
\]</p>

<p>注意到<br/>
\[<br/>
\begin{align*}<br/>
\phi(0) &amp;= 0 + \log(1-\theta + \theta) = 0\\<br/>
\phi&#39;(0) &amp;= -\theta + \frac{\theta e^u}{1-\theta+\theta e^u}\bigg |_{u=0} = 0\\<br/>
\phi&#39;&#39;(v) &amp;= \frac{\theta e^u(1-\theta+\theta e^u) - (\theta e^u)^2}{(1-\theta+\theta e^u)^2}\bigg|_{u=v}\\<br/>
&amp;= \frac{\theta e^u(1-\theta)}{(1-\theta+\theta e^u)^2}\bigg|_{u=v}\\<br/>
&amp;= \frac{\theta e^v(1-\theta)}{(1-\theta+\theta e^v)^2}\\<br/>
&amp;= \frac{\theta e^v}{1-\theta+\theta e^v} \quad \frac{1-\theta}{1-\theta+\theta e^v}\\<br/>
&amp;= t(1-t)<br/>
\end{align*}<br/>
\]</p>

<p>其中<br/>
\[<br/>
\begin{align*}<br/>
t &amp;= \frac{\theta e^v}{1-\theta+\theta e^v}\\<br/>
&amp;= \frac{\theta e^v}{1 + \theta(e^v-1)}\\<br/>
&amp;&gt; 0<br/>
\end{align*}<br/>
\]</p>

<p>所以：<br/>
\[<br/>
\phi&#39;&#39;(v) = t(1-t) = -t^2 + t = -(t^2 - t + \frac 1 4)+\frac 1 4 = -(t-\frac 1 2)^2+\frac 1 4 \le \frac 1 4<br/>
\]</p>

<p>因此：<br/>
\[<br/>
\phi(x) \le \phi(0) + \phi&#39;(0)u + \frac{\phi&#39;&#39;(v)}{2!}u^2 = 0 + 0 + \frac 1 2 u^2 \cdot \frac 1 4 = \frac{u^2}{8} = \frac{\lambda^2(b-a)^2}{8}<br/>
\]</p>

<p>代入式 \ref{eep} 得：<br/>
\[<br/>
\mathbb E[e^{\lambda X}] \le \exp(\frac{\lambda^2(b-a)^2}{8})<br/>
\]</p>

<p>得证。</p>

<h4 id="toc_3">霍夫丁不等式证明</h4>

<p>令 \(S_n = X_1 + X_2 + ... + X_n\)，即 \(\overline X = \frac{S_n}{n}\) ，则霍夫丁不等式可以写成：<br/>
\[<br/>
\begin{align*}<br/>
&amp; {P ({\overline {X}}-{\mathbb E} [{\overline {X}}]\geq o)\leq \exp \left(-{\frac {2o^{2}n^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),\!}\\<br/>
\Rightarrow &amp; {P (\frac{S_n}{n}-{\mathbb E} [\frac{S_n}{n}]\geq o)\leq \exp \left(-{\frac {2o^{2}n^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),\!}\\<br/>
\Rightarrow &amp; {P (\frac{S_n}{n}- \frac{\mathbb E[S_n]}{n}\geq o)\leq \exp \left(-{\frac {2o^{2}n^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),\!}\\<br/>
\Rightarrow &amp; {P ({S_n}- {\mathbb E[S_n]}\geq no)\leq \exp \left(-{\frac {2o^{2}n^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),\!}\\<br/>
\end{align*}<br/>
\]</p>

<p>令 \(t = no\) 便可以得到：<br/>
\[<br/>
{P ({S_n}- {\mathbb E[S_n]}\geq t)\leq \exp \left(-{\frac {2t^{2}}{\sum _{i=1}^{n}(b_{i}-a_{i})^{2}}}\right),\!}\\<br/>
\]</p>

<p>对于 \(s,t &gt; 0\)<br/>
\[<br/>
\begin{align*}<br/>
P ({S_n}- {\mathbb E[S_n]}\geq t) = P(e^{s(S_n-\mathbb E[S_n])} \ge e^{st})<br/>
\end{align*}<br/>
\]</p>

<p>令 \(X = e^{s(S_n-\mathbb E[S_n])} \)，\(\alpha = e^{st}\)，应用马尔可夫不等式：<br/>
\[<br/>
\begin{align}<br/>
P ({S_n}- {\mathbb E[S_n]}\geq t) &amp;= P(e^{s(S_n-\mathbb E[S_n])} \ge e^{st})\nonumber\\<br/>
&amp;\le \frac{\mathbb E[e^{s(S_n-\mathbb E[S_n])}]}{e^{st}}\nonumber\\<br/>
&amp;= e^{-st} \mathbb E[\exp(s(S_n-\mathbb E[S_n]))]\nonumber\\<br/>
&amp;= e^{-st} \mathbb E[\exp[s(\sum_{i=1}^n X_i -\mathbb E[\sum_{i=1}^n X_i])]]\nonumber\\<br/>
&amp;= e^{-st} \mathbb E[\exp[s(\sum_{i=1}^n(X_i -\mathbb E[X_i]))]]\nonumber\\<br/>
&amp;= e^{-st} \mathbb E[\prod_{i=1}^n \exp[s(X_i -\mathbb E[X_i])]]\nonumber\\<br/>
&amp;= e^{-st} \prod_{i=1}^n \mathbb E[\exp[s(X_i -\mathbb E[X_i])]]\label{spe}<br/>
\end{align}<br/>
\]</p>

<p>当 \(\lambda = s\) ，\(x = X_i -\mathbb E[X_i]\) 代入霍夫丁引理，可得：<br/>
\[<br/>
\mathbb E[\exp[s(X_i -\mathbb E[X_i])]] \le \exp(\frac{s^2(b_i-a_i)^2}{8})<br/>
\]</p>

<p>将上式代入式 \ref{spe} 得：<br/>
\[<br/>
\begin{align*}<br/>
P({S_n}- {\mathbb E[S_n]}\geq t) &amp;= e^{-st} \prod_{i=1}^n \mathbb E[\exp[s(X_i -\mathbb E[X_i])]]\\<br/>
&amp;\le e^{-st} \prod_{i=1}^n \exp({\frac{s^2(b_i-a_i)^2}{8}})\\<br/>
&amp;= \exp(-st + \frac 1 8 s^2 \sum_{i=1}^n (b_i-a_i)^2)<br/>
\end{align*}<br/>
\]</p>

<p>为了得到上界，我们要找到右边关于 \(s\) 的不等式的最小值，定义：<br/>
\[<br/>
\left \{ \begin{array}\\<br/>
&amp;g:\mathbb R+ \rightarrow \mathbb R\\<br/>
&amp;g(s):-st + \frac 1 8 s^2 \sum_{i=1}^n (b_i-a_i)^2\\<br/>
\end{array}\right .<br/>
\]</p>

<p>对 \(g(s)\) 求导，并令其等于0，即 \(g&#39;(s) = 0\)：<br/>
\[<br/>
g&#39;(s) = -t + \frac 1 4 s \sum_{i=1}^n (b_i-a_i)^2 = 0 \\<br/>
\Rightarrow s = \frac{4t}{\sum_{i=1}^n (b_i-a_i)^2}<br/>
\]</p>

<p>因此：<br/>
\[<br/>
\begin{align*}<br/>
P({S_n}- {\mathbb E[S_n]}\geq t) &amp;\le \exp[-st + \frac 1 8 s^2 \sum_{i=1}^n (b_i-a_i)^2]\\<br/>
&amp;= \exp[\frac{-4t^2}{\sum_{i=1}^n (b_i-a_i)^2} + \frac 1 8 (\frac{4t}{\sum_{i=1}^n (b_i-a_i)^2})^2 \sum_{i=1}^n (b_i-a_i)^2]\\<br/>
&amp;= \exp[\frac{-4t^2}{\sum_{i=1}^n (b_i-a_i)^2} + \frac{2t^2}{\sum_{i=1}^n (b_i-a_i)^2}]\\<br/>
&amp;= \exp[\frac{-2t^2}{\sum_{i=1}^n (b_i-a_i)^2}]\\<br/>
\end{align*}<br/>
\]</p>

<p>得证。</p>

<hr/>

<p><a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffiding inequality</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15124873066800.html">切比雪夫不等式 chebyshev's inequality</a></h1>
			<p class="meta"><time datetime="2017-12-05T23:21:46+08:00" 
			pubdate data-updated="true">2017/12/5</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>切比雪夫不等式是以俄罗斯数学家 Pafnuty Chebyshev 命名的，它的数学形式如下：<br/>
\[<br/>
P(|X-\mu|\ge \epsilon) \le \frac{\sigma^2}{\epsilon^2}<br/>
\]</p>

<p>其中 \(\mu\) 是期望，\(\sigma\) 是标准差。切比雪夫不等式的一个扩展形式：<br/>
\[<br/>
P(|X-\mu| \lt \epsilon ) \le 1 - \frac{\sigma^2}{\epsilon^2}<br/>
\]</p>

<h4 id="toc_0">切比雪夫不等式证明</h4>

<p>这里提供两种方式证明切比雪夫不等式</p>

<h5 id="toc_1">直接证明</h5>

<p>对于每一个事件 \(A\) ，假设 \(I_A\) 为 \(A\) 的指示函数，定义为：<br/>
\[<br/>
I_A = \left \{ \begin{array}\\ 1 \quad &amp; x \in A\\ 0 \quad &amp; x \notin A\\\end{array}\right .<br/>
\]</p>

<p>所以：<br/>
\[<br/>
\begin{align*}<br/>
P(|X-\mu| \ge \epsilon) &amp;= \mathbb E(I_{|X-\mu| \ge \epsilon})\\<br/>
&amp;= \mathbb E\Big(I_{[({X-\mu})/{\epsilon}]^2 \ge 1}\Big)\\<br/>
&amp;\le \mathbb E\Big[(\frac{X-\mu}{\epsilon})^2\Big]\\<br/>
&amp;= \frac{\mathbb E[(X-\mu)^2]}{\epsilon^2}\\<br/>
&amp;= \frac {\sigma^2}{\epsilon^2}<br/>
\end{align*}<br/>
\]</p>

<h5 id="toc_2">通过马尔可夫不等式证明</h5>

<p>由马尔可夫不等式可知：<br/>
\[<br/>
P(|Y|\ge \alpha) \le \frac{\mathbb E(|Y|)}{\alpha}<br/>
\]</p>

<p>令 \(Y=(X-\mu)^2\) ，得：<br/>
\[<br/>
P((X-\mu)^2\ge \alpha) \le \frac{((X-\mu)^2)}{\alpha} = \frac {\sigma^2}{\alpha}<br/>
\]</p>

<p>令 \(\alpha = \epsilon^2\)：<br/>
\[<br/>
P((X-\mu)^2\ge \epsilon^2) \le \frac{\sigma^2}{\epsilon^2}<br/>
\]</p>

<p>在 \(\epsilon&gt;0\) 的情况下，也就等价于：<br/>
\[<br/>
P(|X-\mu| \ge \epsilon) \le \frac{\sigma^2}{\epsilon^2}<br/>
\]</p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15121456213558.html">马尔可夫不等式 Markov's inequality</a></h1>
			<p class="meta"><time datetime="2017-12-02T00:27:01+08:00" 
			pubdate data-updated="true">2017/12/2</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>在概率论中，马尔可夫不等式给定了一个随机变量的非负函数大于等于一个正常数的概率的上界，它是以俄罗斯数学家 Andrey Markov命名的，尽管它最早出现在 Chebyshev(马尔可夫的老师) 的工作中，有时候它也被称为第一切比雪夫不等式。</p>

<h3 id="toc_0">马尔可夫不等式定义</h3>

<p>如果 \(X\) 是一个非负的随机变量，\(\alpha &gt; 0\)，\(X\) 不小于 \(\alpha\) 的概率不大于 \(X\) 的期望除以 \(\alpha\)：<br/>
\[<br/>
P(X\ge \alpha) \le \frac{\mathbb E(X)}{\alpha}<br/>
\]</p>

<p>令 \(\tilde\alpha=\frac{\alpha}{\mathbb E(X)}\)，所以前面的不等式可以写成：<br/>
\[<br/>
P(X\ge \tilde\alpha \mathbb E(X)) \le \frac 1 {\tilde\alpha}<br/>
\]</p>

<p>在测度理论中，马尔可夫不等式表明如果 \((X,\Sigma,\mu)\) 是一个测度空间，\(f\) 是一个可测量的扩展的实数函数，\(\epsilon \ge 0\)，那么：<br/>
\[<br/>
\mu(\{x\in X:|f(x)|\ge \epsilon\}) \le \frac 1 \epsilon \int_X |f|d\mu<br/>
\]</p>

<p>如果 \(\phi\) 是关于非负实数的单调递增非负函数，\(X\) 是一个随机变量，\(\alpha \ge 0\)，\(\phi(\alpha)&gt;0\)，有：<br/>
\[<br/>
P(|X| \ge \alpha) \le \frac{\mathbb E(\phi(|X|))}{\phi(\alpha)}<br/>
\]</p>

<p>一个直接的推论，若 \(X\) 是一个非负实数：<br/>
\[<br/>
P(X \ge \alpha) \le \frac{\mathbb E(X^n)}{\alpha^n}<br/>
\]</p>

<h3 id="toc_1">马尔可夫不等式证明</h3>

<h4 id="toc_2">概率论</h4>

<p>从期望的定义可以知道：<br/>
\[<br/>
\mathbb E(X) = \int_{-\infty}^{\infty} x f(x) dx<br/>
\]</p>

<p>因为 \(X\) 是一个非负的随机变量，因此：<br/>
\[<br/>
\mathbb E(X) = \int_{-\infty}^{\infty} x f(x) dx = \int_{0}^{\infty} x f(x) dx<br/>
\]</p>

<p>所以我们可以推导出：<br/>
\[<br/>
\begin{align*}<br/>
\mathbb E(X) &amp;= \int_{0}^{\infty} x f(x) dx = \int_0^\alpha x f(x) dx + \int_\alpha^{\infty} x f(x)\\<br/>
&amp;\ge \int_\alpha^{\infty} x f(x) dx \\<br/>
&amp;\ge \int_\alpha^{\infty} \alpha f(x) dx \\<br/>
&amp;= \alpha \int_\alpha^{\infty} f(x) dx \\<br/>
&amp;= \alpha P(x \ge \alpha)<br/>
\end{align*}<br/>
\]</p>

<p>所以可得到：<br/>
\[<br/>
P(X \ge \alpha) \le \frac{\mathbb E(X)}{\alpha}<br/>
\]</p>

<h4 id="toc_3">测度理论</h4>

<p>我们可以假设函数 \(f\) 是非负的，因为只有它的绝对值进入等式。现在考虑 \(X\) 上的实数函数：<br/>
\[<br/>
s(x) = \left \{ \begin{array}\\<br/>
\epsilon \quad &amp; \text{if }f(x)\text{ }\ge \epsilon\\<br/>
0 \quad &amp; \text{if }f(x)\text{ }\lt \epsilon\\<br/>
\end{array} \right.<br/>
\]</p>

<p>然后有 \(0\le s(x) \le f(x)\)，通过勒贝格积分可得：<br/>
\[<br/>
\int_X f(x) d\mu \ge \int_X s(x) d\mu = \epsilon \mu(\{x \in X:f(x) \ge \epsilon \})<br/>
\]</p>

<p>因为 \(\epsilon \gt 0\) ，两边除以 \(\epsilon\) ，可得：<br/>
\[<br/>
\mu(\{x \in X:f(x) \ge \epsilon \}) \le \frac 1 \epsilon \int_X f(x) d\mu<br/>
\]</p>

<hr/>

<p><a href="https://en.wikipedia.org/wiki/Markov%27s_inequality">Markov&#39; inequality In Wikipad</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15029730205260.html">不等式约束问题</a></h1>
			<p class="meta"><time datetime="2017-08-17T20:30:20+08:00" 
			pubdate data-updated="true">2017/8/17</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>拉格朗日乘子法求解不等式约束问题：</p>

<h4 id="toc_0">原始问题</h4>

<p>假设函数：<br/>
\[<br/>
\begin{align*}<br/>
\min\quad &amp;f(x) \nonumber\\<br/>
s.t.\quad &amp;h_j(x) = 0,\quad j=1,2,...,p\\<br/>
s.t.\quad &amp;c_i(x) \le 0,\quad i=1,2,...,q<br/>
\end{align*}<br/>
\]</p>

<p>为了解决问题，定义拉格朗日函数：<br/>
\[<br/>
L(x,\alpha,\beta)=f(x)+\sum_{i=1} \alpha_i c_i(x）+ \sum_{j=1} \beta_j h_j(x)<br/>
\]</p>

<p>这里\(\alpha_i\)和\(\beta_j\)是拉格朗日乘子，其中\(\alpha_i\ge0\)、\(\beta_j \ne 0\)<br/>
\[<br/>
\begin{align}<br/>
&amp;\because h_j(x)=0 \Rightarrow \sum_{j=1} \beta_j h_j(x) = 0 \nonumber\\<br/>
&amp;\therefore L(x,\alpha,\beta) = f(x)+\sum_{i=1} \alpha_i c_i(x) \nonumber\\<br/>
&amp;\because \left.<br/>
             \begin{array}{lcl}<br/>
             \alpha_i \ge 0 \nonumber\\<br/>
             c_i(x) \le 0 <br/>
             \end{array}  <br/>
        \right\} \Rightarrow \sum_{i=1} \alpha_i c_i(x) \le 0 \nonumber\\<br/>
\label{max_L}<br/>
&amp;\therefore \max_{\alpha_i,\beta_j:\alpha_i \ge 0} L(x,\alpha,\beta) = f(x) \\<br/>
\end{align}<br/>
\]</p>

<p>易知，当\(L(x,\alpha,\beta)\)函数取最大值的时候，\(\alpha_i=0\,\)或\( c_i(x) = 0\,\)，也就是\(\alpha_i c_i(x) = 0\,\)。</p>

<p>考虑关于\(x\)的函数<br/>
\[<br/>
\begin{equation}<br/>
\label{theta_p}<br/>
    \theta_P(x)=\max_{\alpha,\beta:\alpha\ge0} L(x,\alpha\beta)<br/>
\end{equation}<br/>
\]</p>

<p>那么现在求解原始问题\(\,\min f(x)\)，等价于求解\(\,\min_x\max_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta)\,\)，即\(\min_x \theta_P(x)\)，这样原始问题就变成了拉格朗日函数的极小极大问题，公式如下：<br/>
\[<br/>
\label{min_f_x}<br/>
\begin{equation}<br/>
\min_x f(x) = \min_x\max_{\alpha,\beta:\alpha_i \ge 0} L(x,\alpha,\beta) = \min_x \theta_P(x)<br/>
\end{equation}<br/>
\]</p>

<p>这个方程先是求解含有\(\alpha\)和\(\beta\)的极大值，它直接求导后并不容易得出结果，所以这里我们定义一个它的对偶函数（交换极小极大位置）来方便计算。</p>

<h4 id="toc_1">对偶问题</h4>

<p>定义<br/>
\[<br/>
\begin{equation}<br/>
\label{theta_d}<br/>
\theta_D(\alpha,\beta) = \min_x L(x,\alpha,\beta) \\<br/>
\end{equation}<br/>
\]</p>

<p>那么原始问题的对偶问题就是极大化\(\theta_D(\alpha,\beta)\)，这被称为拉格朗日函数的极大极小化问题。</p>

<p>展开拉格朗日极大极小问题：<br/>
\[<br/>
\begin{align}<br/>
\max_{\alpha,\beta} \theta_D(\alpha,\beta) &amp;= \max_{\alpha,\beta} \min_x L(x,\alpha,\beta) \nonumber\\<br/>
&amp;= \max_{\alpha,\beta}[\min_x(f(x)+\sum_{i=1}^N \alpha_i c_i(x）+ \sum_{j=1}^N \beta_j h_j(x))] \nonumber\\<br/>
&amp;= \max_{\alpha,\beta}(\min_x f(x)+\min_x\sum_{i=1}^N \alpha_i c_i(x)) \nonumber\\<br/>
&amp;= \max_{\alpha,\beta}\min_x f(x)+\max_{\alpha,\beta}\min_x \sum_{i=1}^N \alpha_i c_i(x) \nonumber\\<br/>
\label{max_theta_d_m}<br/>
&amp;= \min_x f(x)+\max_{\alpha,\beta}\min_x \sum_{i=1}^N \alpha_i c_i(x) \\<br/>
\end{align} <br/>
\]</p>

<p>在上面推导公式中，由于\(f(x)\)与\(\alpha\)和\(\beta\)无关，所有\(\max_{\alpha,\beta}\min_x f(x)=\min_x f(x)\)</p>

<h3 id="toc_2">对偶问题与原始问题的关系</h3>

<p>设原始问题的最优值为\(p^*\)，对偶问题的最优解为\(d^*\)，则有：<br/>
\[<br/>
\begin{align*}<br/>
p^* &amp;= \min_x f(x) = \min_x \max_{\alpha,\beta:\alpha_i \ge 0}L(x,\alpha,\beta) = \min_x\theta_P(x) \\<br/>
d^* &amp;= \max_{\alpha,\beta} \theta_D(\alpha,\beta)<br/>
\end{align*}<br/>
\]结合公式（\ref{theta_p}）和（\ref{theta_d}）可知：<br/>
\[<br/>
\begin{align*}<br/>
&amp;\because \min_x L(x,\alpha,\beta) \le L(x,\alpha,\beta) \le \max_{\alpha,\beta}L(x,\alpha,\beta) \\<br/>
&amp;\Rightarrow \theta_D(\alpha,\beta) \le \theta_P(x) \\<br/>
&amp;\Rightarrow \max_{\alpha,\beta}\theta_D(\alpha,\beta) \le \min_x \theta_P(x)<br/>
\end{align*}<br/>
\]</p>

<p>即<br/>
\[<br/>
\begin{equation}<br/>
\label{d_p}<br/>
d^* = \max_{\alpha,\beta}\theta_D(\alpha,\beta) \le \min_x \theta_P(x) = p^*<br/>
\end{equation}<br/>
\]</p>

<p>可以得知在某些条件下，原始问题和对偶问题的最优值会相等，即\(d^* = p^*\)，而这里某些条件就是下面讨论的KKT条件。</p>

<h4 id="toc_3">KKT条件</h4>

<p>结合公式（\ref{max_theta_d_m}）继续推导公式（\ref{d_p}）：<br/>
\[<br/>
\begin{align*}<br/>
d^* &amp;= \max_{\alpha,\beta}\theta_D(\alpha,\beta) \\<br/>
&amp;= \min_x f(x)+\max_{\alpha,\beta}\min_x \sum_{i=1}^N \alpha_i c_i(x) \\<br/>
&amp;= p^* + \max_{\alpha,\beta}\min_x \sum_{i=1}^N \alpha_i c_i(x)<br/>
\end{align*}<br/>
\]</p>

<p>在上面推导过程中，假设\(x^*\)是原始问题的可行解，并且\(d^* = p^*\)，所以有<br/>
\[<br/>
\begin{equation}<br/>
\alpha_i c_i(x^*) = 0 <br/>
\end{equation}<br/>
\]</p>

<p>因此：<br/>
\[<br/>
\begin{align*}<br/>
d^* &amp;= \max_{\alpha,\beta}\theta_D(\alpha,\beta) \\<br/>
&amp;= \min_x f(x)+\max_{\alpha,\beta}\min_x \sum_{i=1}^N \alpha_i c_i(x) \\<br/>
&amp;= \min_x f(x) \\<br/>
\end{align*}<br/>
\]</p>

<p>结合公式（\ref{theta_d}）和上面结果，可知：<br/>
\[<br/>
\begin{equation}<br/>
\label{max_d_m}<br/>
\max_{\alpha,\beta}\theta_D(x) = \max_{\alpha,\beta}\min_x L(x,\alpha,\beta) = \min_x f(x) = f(x^*)<br/>
\end{equation}<br/>
\]</p>

<p>将\(x^*\)代入公式（\ref{max_L}）得：<br/>
\[<br/>
\begin{equation}<br/>
\label{f_m_L}<br/>
f(x^*) = \max_{\alpha,\beta}L(x^*,\alpha,\beta)<br/>
\end{equation}<br/>
\]</p>

<p>由上面公式公式（\ref{max_d_m}）和（\ref{f_m_L}）可知：<br/>
\[<br/>
\min_x L(x,\alpha,\beta) = L(x^*,\alpha,\beta)<br/>
\]</p>

<p>所以\(x^*\)是\(L(x,\alpha,\beta)\)的极值，可以得出结论：</p>

<p>\[<br/>
\begin{equation}<br/>
\nabla_xL(x^*,\alpha,\beta) = 0<br/>
\end{equation}<br/>
\]</p>

<p>所以KKT条件为：<br/>
\[<br/>
\begin{equation*}<br/>
\nabla_xL(x^*,\alpha,\beta) = 0 \\<br/>
h_j(x^*) = 0:j=1;2;...;N \\<br/>
c_i(x^*) \le 0:i=1;2;...;N \\<br/>
\alpha_ic_i(x^*) = 0:i=1;2;...;N \\<br/>
\alpha_i \ge 0:i=1;2;...;N \\<br/>
\beta_j \ne 0:j=1;2;...;N \\<br/>
\end{equation*}<br/>
\]</p>

<p>上面KKT条件中，第一个称为极值条件，第二个第三个是拉格朗日原约束条件，第四个是互补松弛条件，第五个第六个是拉格朗日系数约束。</p>

<p>推导说明，在满足KKT条件的情况下，对原始问题的求解可以转换为对偶问题\(\max_{\alpha,\beta}\min_x L(x,\alpha,\beta)\)的求解。</p>


		</div>

		

	</article>
  
	<div class="pagination">
	
<a href="archives.html">Blog Archives</a>
	 
	    
	</div>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98.html"><strong>最短路径问题&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98.html"><strong>聚类问题&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%85%B6%E4%BB%96%E7%AE%97%E6%B3%95.html"><strong>其他算法&nbsp;(6)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95.html"><strong>基础算法&nbsp;(23)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="SVM.html">SVM&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="SNE.html">SNE&nbsp;(1)</a>&nbsp;&nbsp;
	        
	        	<a href="EM.html">EM&nbsp;(5)</a>&nbsp;&nbsp;
	        
	        	<a href="%E5%86%B3%E7%AD%96%E6%A0%91.html">决策树&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="HMM.html">HMM&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0.html">集成学习&nbsp;(8)</a>&nbsp;&nbsp;
	        
	        	<a href="%E9%99%8D%E7%BB%B4.html">降维&nbsp;(3)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html"><strong>数学基础&nbsp;(14)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="Python%E5%AD%A6%E4%B9%A0.html"><strong>Python学习&nbsp;(2)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><strong>神经网络&nbsp;(15)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0.html"><strong>增强学习&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15454660806753.html">深度学习中的正则化-Dropout方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15446218642343.html">图像相似度方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15436296136092.html">蒙特卡罗树搜搜 MCTS</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15424711438602.html">人工神经网络-GAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15418610530072.html">人工神经网络-SOM自组织系统</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    

<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>